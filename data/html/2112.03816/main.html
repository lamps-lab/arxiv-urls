<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops</title>
<!--Generated on Thu Dec  5 01:36:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Autonomous Navigation,  Robotics,  Artificial Intelligence,  Precision Agriculture.
">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Simone Cerrato,
Vittorio Mazzia,
Francesco Salvetti,
Mauro Martini,
Simone Angarano,
Alessandro Navone,
Marcello Chiaberge
</span><span class="ltx_author_notes">All the authors are with the Department of Electronics and Telecommunications (DET), Politecnico di Torino, Torino, TO 10129, email: name.surname@polito.itThis work has been developed with the contribution of the Politecnico di Torino Interdepartmental Centre for Service Robotics (PIC4SeR).</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
    
<p class="ltx_p">Expensive sensors and inefficient algorithmic pipelines significantly affect the overall cost of autonomous machines. However, affordable robotic solutions are essential to practical usage, and their financial impact constitutes a fundamental requirement to employ service robotics in most fields of application. Among all, researchers in the precision agriculture domain strive to devise robust and cost-effective autonomous platforms in order to provide genuinely large-scale competitive solutions.
In this article, we present a complete algorithmic pipeline for row-based crops autonomous navigation, specifically designed to cope with low-range sensors and seasonal variations. Firstly, we build on a robust data-driven methodology to generate a viable path for the autonomous machine, covering the full extension of the crop with only the occupancy grid map information of the field. Moreover, our solution leverages on latest advancement of deep learning optimization techniques and synthetic generation of data to provide an affordable solution that efficiently tackles the well-known Global Navigation Satellite System unreliability and degradation due to vegetation growing inside rows. Extensive experimentation and simulations against computer-generated environments and real-world crops demonstrated the robustness and intrinsic generalizability to different factors of variations of our methodology that opens the possibility of highly affordable and fully autonomous machines.</p>
  
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Autonomous Navigation, Robotics, Artificial Intelligence, Precision Agriculture.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Agriculture 3.0 and 4.0 have gradually introduced autonomous machines and interconnected sensors into several agricultural processes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="DeepWay: a deep learning waypoint estimator for global path generation" class="ltx_ref">35</a>, <a href="#bib.bib2" title="Waypoint generation in row-based crops with deep learning and contrastive clustering" class="ltx_ref">43</a>]</cite>, trying to introduce robust and cost-effective novel solutions into the overall production chain. For instance, precision agriculture has progressively innovated tools for automatic harvesting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="Robots in agriculture: state of art and practical experiences" class="ltx_ref">41</a>]</cite>, vegetative assessment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="Assessment of rice leaf blast severity using hyperspectral imaging during late vegetative growth" class="ltx_ref">51</a>]</cite>, crops yield estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="Yield estimation in cotton using uav-based multi-sensor imagery" class="ltx_ref">16</a>]</cite>, smart and sustainable pesticide spraying robotic system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="Design and development of intelligent pesticide spraying system for agricultural robot" class="ltx_ref">13</a>]</cite> and many others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="Refining satellite imagery by using uav imagery for vineyard environment: a cnn based approach" class="ltx_ref">25</a>, <a href="#bib.bib22" title="A compilation of uav applications for precision agriculture" class="ltx_ref">40</a>]</cite>. Indeed, the pervasiveness of precision agriculture techniques has such a huge impact on certain fields of application that the adoption of them has become increasingly essential to achieve high product quality standards <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="Vineyard detection from unmanned aerial systems images" class="ltx_ref">9</a>]</cite>. Moreover, the introduction of robots in agriculture will increasingly have a stronger impact on economic, political, social, cultural, security, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="Robots in agriculture: prospects, impacts, ethics, and policy" class="ltx_ref">44</a>]</cite>, and will be the only tool to satisfy the future food demand of our society <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="Agricultural robotics: the future of robotic agriculture" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Nevertheless, research on autonomous machines still requires further developments and improvements to meet the necessary industrial conditions of robustness and effectiveness. Global path planning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="DeepWay: a deep learning waypoint estimator for global path generation" class="ltx_ref">35</a>, <a href="#bib.bib2" title="Waypoint generation in row-based crops with deep learning and contrastive clustering" class="ltx_ref">43</a>]</cite>, mapping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="Semantics for robotic mapping, perception and interaction: a survey" class="ltx_ref">18</a>]</cite>, localization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="Navigating the landscape for real-time localization and mapping for robotics and virtual and augmented reality" class="ltx_ref">42</a>]</cite> and decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="Commonsense reasoning and deep learning for transparent decision making in robotics" class="ltx_ref">38</a>]</cite> are only some of the required tools that each year undergo heavy research from the scientific community to achieve the necessary requirements for full automation. Among all requirements, a low financial impact constitutes a fundamental goal in order to provide genuinely large-scale competitive solutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="High-accuracy adaptive low-cost location sensing subsystems for autonomous rover in precision agriculture" class="ltx_ref">28</a>]</cite>. Indeed, expensive sensors and demanding computational algorithms significantly impact the actual usefulness of robotics platforms, essentially preventing their large-scale adoption and introduction into the agricultural world.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="417" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Field tests with the Jackal platform in different seasonal periods of the same crop. Lush vegetation and thick canopies greatly reduce the GPS accuracy, affecting its reliability and consequently the overall navigation pipeline. Nevertheless, our proposed segmentation-based algorithm exploits semantic segmentation properties to provide a proportional controller that drives the robotic platform along the whole row.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">Recently, deep learning methodologies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="Deep learning" class="ltx_ref">27</a>]</cite> revolutionized the entire computer perception field, endowing machines with an unprecedented representation of the surrounding environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="A survey of deep learning techniques for autonomous driving" class="ltx_ref">19</a>]</cite>. Moreover, the intrinsic robustness of representation learning techniques to different factor of variations opens the possibility to achieve noteworthy perception capabilities with low-cost and low-range sensors, greatly relieving the overall cost of the target machine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="A cost-effective person-following system for assistive unmanned vehicles with deep learning at the edge" class="ltx_ref">6</a>, <a href="#bib.bib3" title="Position-agnostic autonomous navigation in vineyards with deep reinforcement learning" class="ltx_ref">32</a>]</cite>. Finally, the latest advancement in deep learning optimization techniques, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="Quantization and training of neural networks for efficient integer-arithmetic-only inference" class="ltx_ref">22</a>]</cite>, have progressively reduced latency, inference cost, memory, and storage footprint of its algorithms. That effectively scales down computational requirements enabling low-power computational devices boosted by hardware accelerators such as visual processing units (VPUs), tensor processing units (TPUs), and embedded GP-GPUs, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="Real-time apple detection system using embedded systems with hardware accelerators: an edge ai application" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Building on the latest deep learning researches for computer perception and exclusively making use of low-range sensors, we present a complete algorithmic pipeline for autonomous navigation in row-based crops. The proposed robust solution is explicitly designed to adapt to seasonal variation, as shown in Fig. <a href="#S1.F1" title="Figure 1 ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, ensuring complete coverage of the field in the different situations. Moreover, the low financial impact of our methodology greatly reduces maintenance and production costs and enables a large-scale adoption of fully autonomous machines for precision agriculture.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">Firstly, we build on a robust data-driven methodology to generate a viable path for the autonomous machine, covering the full extension of the crop with only the occupancy grid map information of the field. Successively, depending on the vegetation growth status of the crop, we adopt a purely Global Navigation Satellite System (GNSS) local path planning or a vision-based algorithm that exclusively makes use of a low-cost RGB-D camera to navigate inside the inter-row space. Indeed, meteorological conditions and especially lush vegetation and thick canopies, significantly affect GNSS reliability, degrading its precision and consequently the overall navigation pipeline, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="Performance comparison of single and multi-gnss receivers under agricultural fields in korea" class="ltx_ref">23</a>, <a href="#bib.bib38" title="GPS-free localisation and navigation of an unmanned ground vehicle for yield forecasting in a vineyard" class="ltx_ref">31</a>]</cite>. Conversely, our vision-based system exploits semantic information of the environment to navigate between rows and depth information to refine the underline control smoothness, disentangling from the necessity of a precise localization. Moreover, we make exclusively usage of synthetic data and domain randomization, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="Domain randomization for transferring deep neural networks from simulation to the real world" class="ltx_ref">46</a>]</cite>, to enable affordable supervised learning and simultaneously bridging the domain gap between simulation and reality. Such technique allows us to easily construct and train a deep learning model able to efficiently generalize on different row-based crops.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p">The overall proposed methodology guarantees to autonomously navigate throughout row-based crops without expensive sensors and with every seasonal variation.
All the algorithmic pipeline has been developed ROS-compatible in order to make easier the communication among different software modules and to be easily deployed on our developing
platform, the Jackal Unmanned Ground Vehicle (UGV) by
Clearpath Robotics<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>
            <span class="ltx_tag ltx_tag_note">1</span>
            
            
          https://clearpathrobotics.com/</span></span></span>. Extensive experimentation and simulations against computer-generated environments and diverse real-world crops demonstrated the robustness and intrinsic generalizability of our solution. All of our code<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>
            <span class="ltx_tag ltx_tag_note">2</span>
            
            
          https://github.com</span></span></span> and data<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>
            <span class="ltx_tag ltx_tag_note">3</span>
            
            
          https://zenodo.com</span></span></span>
are open source and publicly available.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">I-A </span><span class="ltx_text ltx_font_italic">Related Work</span>
</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p class="ltx_p">Over the past years, autonomous systems designed to navigate in row-crops fields make largely usage of high-precision GNSS
receivers in combination with laser-based sensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="A fully autonomous robot for putting posts for trellising vineyard with centimetric accuracy" class="ltx_ref">29</a>, <a href="#bib.bib41" title="Automating orchards: a system of autonomous tractors for orchard maintenance" class="ltx_ref">37</a>]</cite>. Nevertheless, the canopies
on the sides of the row reduce the GNSS accuracy, affecting
its reliability and forcing the adoption of more expensive sensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="Performance comparison of single and multi-gnss receivers under agricultural fields in korea" class="ltx_ref">23</a>]</cite>. Indeed, more robust solutions fuse multiple sensor information (GNSS, inertial navigation systems, wheel encoders) to obtain a better estimation of the mobile platform location in presence of thick canopies and adverse meteorological conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="An overview of autonomous crop row navigation strategies for unmanned ground vehicles" class="ltx_ref">5</a>, <a href="#bib.bib44" title="Localization for precision navigation in agricultural fields‚Äîbeyond crop row following" class="ltx_ref">47</a>]</cite>. However, high adoption of high-range sensors leads to higher system and maintenance costs, preventing a large-scale adoption of self-driving agricultural machinery.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p class="ltx_p">On the other hand, visual odometry (VO), <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="Cost-effective visual odometry system for vehicle motion control in agricultural environments" class="ltx_ref">50</a>, <a href="#bib.bib51" title="Development of the architecture of the base platform agricultural robot for determining the trajectory using the method of visual odometry" class="ltx_ref">39</a>]</cite>, and computer vision based solutions, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="Autonomous navigation for a wolfberry picking robot using visual cues and fuzzy control" class="ltx_ref">30</a>, <a href="#bib.bib10" title="An approach for semantic segmentation of tree-like vegetation" class="ltx_ref">45</a>]</cite>, have been proposed as more affordable approaches. Nonetheless, VO
systems show poor performance on long distances due to the
accumulating error and struggle with highly similar patterns <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="Low-drift visual odometry in structured environments by decoupling rotational and translational motion" class="ltx_ref">26</a>]</cite> and unpredictable lighting conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="UAV localization in row crops" class="ltx_ref">4</a>]</cite>. Moreover, purely vision-based solutions cannot deal with seasonal variations and generalizability to different crops is difficult to achieve.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="830" height="241" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A representation of the overall pipeline. From left to rigth, an occupancy grid of the crop is provided as input to DeepWay neural network, that estimates the waypoints at start/end of vineyards row. Then, a custom algorithm is responsible to order the generated waypoints and compute a global path maintaining a safe distance from crops. Finally, a local path planning policy chooses the right navigation algorithm according to UGV‚Äôs position with respect to waypoint in order to drive the mobile platform along the whole path.</figcaption>
</figure>
<div id="S1.SS1.p3" class="ltx_para">
<p class="ltx_p">In this state-of-the-art landscape, our team started in 2019 a research project with the precise aim to develop a complete working pipeline to autonomously navigate in vineyard rows without relying on multiple expensive sensors. As already introduced, computer vision and deep learning based algorithms, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="Deep learning" class="ltx_ref">27</a>]</cite>, have demonstrated particular robustness in solving problems with noisy signals. Moreover, optimization and edge AI techniques have progressively made inference computational affordable, opening the usage of deep learning methodologies to diverse practical applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="Deep learning in agriculture: a survey" class="ltx_ref">24</a>]</cite>. Consequently, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="Autonomous navigation in vineyards with deep learning at the edge" class="ltx_ref">2</a>, <a href="#bib.bib1" title="Deep semantic segmentation at the edge for autonomous navigation in vineyard rows" class="ltx_ref">1</a>]</cite> were addressed navigation in vineyard rows with deep learning based algorithm and the exclusive usage of an RGB-D camera. Furthermore, starting with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="Automatic path planning for unmanned ground vehicle using uav imagery" class="ltx_ref">52</a>]</cite> it has been addressed the global path generation automation problem, which is commonly neglected by the research community. However, a suitable path generator is crucial for obtaining a complete autonomous navigation performance and its absence prevents the control of the platform on the field. For that reason, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="DeepWay: a deep learning waypoint estimator for global path generation" class="ltx_ref">35</a>]</cite>, moving from the clustering solution proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="Automatic path planning for unmanned ground vehicle using uav imagery" class="ltx_ref">52</a>]</cite>
to detect the rows of the vineyards, DeepWay, a robust data-driven approach for global path generation, was presented. Besides being more robust and easier to adopt, DeepWay is a highly general approach that can be extended to every kind of row-based crops.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">I-B </span><span class="ltx_text ltx_font_italic">Novelties</span>
</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p class="ltx_p">Based upon the aforementioned previous work, the main contributions of the presented approach herein are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p class="ltx_p">Introduction of a highly affordable complete algorithmic pipeline for autonomous navigation in row-based crops. The methodology makes use of only low-cost, low-range sensors to drive a platform for the full extension of a crop in the different seasonal periods.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p class="ltx_p">Building on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="Deep semantic segmentation at the edge for autonomous navigation in vineyard rows" class="ltx_ref">1</a>]</cite>, we extend the local navigation between vineyard rows to a general row-based crop. We propose a domain randomization based training procedure to easily obtain a segmentation network with only synthetic data.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p class="ltx_p">We introduce a novel global path planning procedure to connect two successive rows, avoiding the usage of too general algorithms that could easily introduce jerky and inefficient paths.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p class="ltx_p">Improved usage of semantic information of the crop to navigate between rows in case of lush vegetation and thick canopies.</p>
</div>
</li>
</ul>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p class="ltx_p">Compared to other existing autonomous navigation algorithms our solution makes use of low-cost sensors as: a cheap GNSS receiver with Real-Time Kinematic (RTK) corrections, an RGB-D camera, an Inertial Measurement Unit (IMU) and encoders. Our approach aims to fill the gaps between usage of low-cost sensors and robustness of autonomous navigation in the precision agriculture context exploiting the collaboration between Artificial Intelligence and standard navigation algorithms. On the other hand, existing solution exploits high cost and very accurate sensors, as: 3D LiDAR, expensive RTK-GNSS receiver to achieve a reliable and robust complete autonomous solution.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p class="ltx_p">The remainder of this paper is organized as follows. Section <a href="#S2" title="II System Overview ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> describes the overall system framework. The detailed explanation of the proposed full pipeline is introduced in Section <a href="#S3" title="III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. Section <a href="#S4" title="IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> presents the pipeline evaluation either against computer-generated environments or real-world crops. Finally, Section <a href="#S5" title="V Conclusion and Future Work ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> draws conclusions and suggests future work.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps">System Overview</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">The proposed work is intended for presenting a complete autonomous navigation system for general row-based crops. The designed autonomous system is organized in several software modules that should collaborate with each other in order to obtain effective and reliable driverless navigation throughout the whole field. A visualization of the complete pipeline is shown in Fig. <a href="#S1.F2" title="Figure 2 ‚Ä£ I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">Firstly, the system takes as input a georeferenced occupancy grid map of the considered crop to compute a global path made of geographic coordinates; in particular, it exploits the DeepWay network, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="DeepWay: a deep learning waypoint estimator for global path generation" class="ltx_ref">35</a>]</cite>, to estimate the start/end waypoints of each row, then a custom global path planner, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="An adaptive row crops path generator with deep learning synergy" class="ltx_ref">7</a>]</cite>, computes the desired path maintaining a safe distance from crops. Secondly, according to the season period and the amount of crop vegetation, it is possible to choose the kind of navigation to perform: only GNSS-based or GNSS and AI-assisted.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">In case a good view of sky is available both outside and inside the row space, the system exploits only Real Time Kinematic (RTK) corrections, GNSS signals, and inertial data to autonomously guide the mobile platform throughout the crops. On the other hand, when lush vegetation is present, the proposed navigation system makes use of RTK corrections, GNSS signals, and inertial data to perform the row switch, since outside the row space a good sky view is available, while along the rows it exploits the camera, the deep neural network, and the segmentation-based control to overcome GNSS signals unreliability.
In both scenarios, the system strongly relies on estimated global positions of the UGV in order to follow the provided global path. In our approach, the localization problem is tackled fusing the positioning information coming from an RTK enabled GNSS receiver and the inertial data provided by an IMU. All the data is loosely fused exploiting the well-known Extended Kalman Filter (EKF), that uses an omnidirectional model for prediction and outputs position estimations in the form of: <math id="S2.p3.m1" class="ltx_Math" alttext="x,y,yaw" display="inline"><mrow><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mrow><mi>y</mi><mo>‚Å¢</mo><mi>a</mi><mo>‚Å¢</mo><mi>w</mi></mrow></mrow></math>, since the navigation happens in 2-dimension. <math id="S2.p3.m2" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> and <math id="S2.p3.m3" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> are global spatial information represented in the East-North-Up (ENU) reference frame, while <math id="S2.p3.m4" class="ltx_Math" alttext="yaw" display="inline"><mrow><mi>y</mi><mo>‚Å¢</mo><mi>a</mi><mo>‚Å¢</mo><mi>w</mi></mrow></math> is the absolute orientation with respect to magnetic north, corrected with the actual magnetic declination.
Once the localization filter is set up, in case a clear view of the sky is available, the navigation algorithm uses the estimated UGV positions, the global path and a local planner based on the Dynamic Window Approach (DWA) to autonomously guide the mobile platform; otherwise, it makes use of both GNSS-based and AI-assisted navigation, that exploit GNSS signals and semantic information of crops to safely navigate in the whole field without colliding with the crops.
In the latter case, the navigation type selection occurs comparing the estimated UGV global positions and an ordered waypoints list computed by the DeepWay neural network and successively refined. Finally, it is important to underline that DWA navigation scheme is only one possible solution. It is adopted in the presented pipeline for its simplicity, flexibility and online collision avoidance capability. However, further experimentation with even simpler algorithms has been performed but not presented for the sake of conciseness. For instance, the Pure Pursuit controller <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="Implementation of the pure pursuit path tracking algorithm" class="ltx_ref">11</a>]</cite> demonstrated very promising results between and outside rows, bringing possible advantages in presence of more packed rows or larger vehicles.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">In this section, each block of the navigation system is presented, detailing all the aspects of the path planning and navigation processes. Our methodology requires as input a georeferenced occupancy grid of the target field <math id="S3.p1.m1" class="ltx_Math" alttext="\textbf{{X}}_{occ}\in\mathbb{R}^{H\times W}" display="inline"><mrow><msub><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>o</mi><mo>‚Å¢</mo><mi>c</mi><mo>‚Å¢</mo><mi>c</mi></mrow></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>H</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>W</mi></mrow></msup></mrow></math>, obtained by segmenting an aerial view of the environment. The system is developed and tested on satellite imagery, but the very same methodology can be applied to images obtained by drones flying over the target field.
The global path is computed by the first two blocks of the pipeline and is represented as an ordered set of points <math id="S3.p1.m2" class="ltx_Math" alttext="\textbf{\emph{P}}=\{(x,y)|x,y\in\mathbb{R}\}" display="inline"><mrow><mtext class="ltx_mathvariant_bold-italic"><em class="ltx_emph ltx_font_bold ltx_font_italic">P</em></mtext><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">|</mo><mrow><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>‚àà</mo><mi>‚Ñù</mi></mrow><mo stretchy="false">}</mo></mrow></mrow></math> to be followed by the UGV in order to reach full coverage of the target field. Fig. <a href="#S3.F3" title="Figure 3 ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows an example of occupancy grid with the predicted global path in red superimposed on the aerial image of the field. Once the global path <span class="ltx_text ltx_markedasmath ltx_font_bold"><span class="ltx_text ltx_font_italic">P</span></span> has been generated, the system exploits a local path planning policy to autonomously navigate in the considered crop choosing the proper control algorithm according to seasonal period and the presence of thick canopies on crops. Indeed, full autonomous navigation happens exploiting the GNSS information and the semantic information obtained employing the segmentation network.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="352" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Aerial view of a row crop field, together with the occupancy grid (white), the estimated waypoints (yellow), and the global path (red). The meters/pixel resolution is 0.1 m/px, the end-row distance margin <math id="S3.F3.m2" class="ltx_Math" alttext="d_{er}=20" display="inline"><mrow><msub><mi>d</mi><mrow><mi>e</mi><mo>‚Å¢</mo><mi>r</mi></mrow></msub><mo>=</mo><mn>20</mn></mrow></math> px, that results in a 2 meters real-world margin.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-A </span><span class="ltx_text ltx_font_italic">Waypoints Estimation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">The first block aims at predicting the list of <math id="S3.SS1.p1.m1" class="ltx_Math" alttext="l" display="inline"><mi>l</mi></math> waypoints <math id="S3.SS1.p1.m2" class="ltx_Math" alttext="{\textbf{\emph{W}}\in\mathbb{N}^{l\times 2}}" display="inline"><mrow><mtext class="ltx_mathvariant_bold-italic"><em class="ltx_emph ltx_font_bold ltx_font_italic">W</em></mtext><mo>‚àà</mo><msup><mi>‚Ñï</mi><mrow><mi>l</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>2</mn></mrow></msup></mrow></math> in the occupancy grid reference frame that represent the begin and end of each row of the target field. Since classical clustering methods fail with real-world conditions such as rows of different length, holes and outliers, we adopt the DeepWay framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="DeepWay: a deep learning waypoint estimator for global path generation" class="ltx_ref">35</a>]</cite>, which frames the waypoints prediction as a regression problem. DeepWay is a fully convolutional neural network that takes as input the occupancy grid <math id="S3.SS1.p1.m3" class="ltx_Math" alttext="\textbf{{X}}_{occ}" display="inline"><msub><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>o</mi><mo>‚Å¢</mo><mi>c</mi><mo>‚Å¢</mo><mi>c</mi></mrow></msub></math> of dimension <math id="S3.SS1.p1.m4" class="ltx_Math" alttext="H\times W" display="inline"><mrow><mi>H</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>W</mi></mrow></math> and outputs a map <math id="S3.SS1.p1.m5" class="ltx_Math" alttext="\hat{\textbf{{Y}}}" display="inline"><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">Y</mtext><mo>^</mo></mover></math> of dimension <math id="S3.SS1.p1.m6" class="ltx_Math" alttext="U_{H}\times U_{W}\times 3" display="inline"><mrow><msub><mi>U</mi><mi>H</mi></msub><mo lspace="0.222em" rspace="0.222em">√ó</mo><msub><mi>U</mi><mi>W</mi></msub><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>3</mn></mrow></math>:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1" class="ltx_Math" alttext="\hat{\textbf{{Y}}}=f_{DeepWay}(\textbf{\emph{X}}_{occ})" display="block"><mrow><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">Y</mtext><mo>^</mo></mover><mo>=</mo><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>p</mi><mo>‚Å¢</mo><mi>W</mi><mo>‚Å¢</mo><mi>a</mi><mo>‚Å¢</mo><mi>y</mi></mrow></msub><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><msub><mtext class="ltx_mathvariant_bold-italic"><em class="ltx_emph ltx_font_bold ltx_font_italic">X</em></mtext><mrow><mi>o</mi><mo>‚Å¢</mo><mi>c</mi><mo>‚Å¢</mo><mi>c</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The first channel of <math id="S3.SS1.p1.m7" class="ltx_Math" alttext="\hat{\textbf{{Y}}}" display="inline"><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">Y</mtext><mo>^</mo></mover></math> is a confidence map that outputs for each cell <math id="S3.SS1.p1.m8" class="ltx_Math" alttext="u" display="inline"><mi>u</mi></math> the probability <math id="S3.SS1.p1.m9" class="ltx_Math" alttext="P(u)" display="inline"><mrow><mi>P</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></mrow></math> that a waypoint falls inside the cell itself. The output map dimensions are obtained subsampling the input space of a factor <math id="S3.SS1.p1.m10" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1" class="ltx_Math" alttext="U_{H}=H/k\qquad U_{W}=W/k" display="block"><mrow><mrow><msub><mi>U</mi><mi>H</mi></msub><mo>=</mo><mrow><mi>H</mi><mo>/</mo><mi>k</mi></mrow></mrow><mspace width="2em"></mspace><mrow><msub><mi>U</mi><mi>W</mi></msub><mo>=</mo><mrow><mi>W</mi><mo>/</mo><mi>k</mi></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p class="ltx_p">Thus, each cell <math id="S3.SS1.p3.m1" class="ltx_Math" alttext="u" display="inline"><mi>u</mi></math> represents a square region of <math id="S3.SS1.p3.m2" class="ltx_Math" alttext="k\times k" display="inline"><mrow><mi>k</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>k</mi></mrow></math> pixels of the original occupancy grid. The other two channels of the output map <math id="S3.SS1.p3.m3" class="ltx_Math" alttext="\hat{\textbf{{Y}}}" display="inline"><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">Y</mtext><mo>^</mo></mover></math> predict for each cell <math id="S3.SS1.p3.m4" class="ltx_Math" alttext="u" display="inline"><mi>u</mi></math> two compensation factors <math id="S3.SS1.p3.m5" class="ltx_Math" alttext="\Delta x" display="inline"><mrow><mi mathvariant="normal">Œî</mi><mo>‚Å¢</mo><mi>x</mi></mrow></math> and <math id="S3.SS1.p3.m6" class="ltx_Math" alttext="\Delta y" display="inline"><mrow><mi mathvariant="normal">Œî</mi><mo>‚Å¢</mo><mi>y</mi></mrow></math> used to localize the waypoint inside the <math id="S3.SS1.p3.m7" class="ltx_Math" alttext="u" display="inline"><mi>u</mi></math> cell, as shown in Fig. <a href="#S3.F4" title="Figure 4 ‚Ä£ III-A Waypoints Estimation ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Those factors are normalized to <math id="S3.SS1.p3.m8" class="ltx_Math" alttext="[-1,1]" display="inline"><mrow><mo stretchy="false">[</mo><mrow><mo>‚àí</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></math> range so that they represent a distance from the center of the cell. Thus, a factor of 1 means a positive deviation on the corresponding axis of half the length of the cell. Eventually, the final location of the predicted waypoints in the input space can be recovered as:</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1" class="ltx_Math" alttext="\hat{\mathbf{y}}_{O}=k\left(\hat{\mathbf{y}}_{U}+\frac{\mathbf{\Delta+1}}{2}\right)" display="block"><mrow><msub><mover accent="true"><mi>ùê≤</mi><mo>^</mo></mover><mi>O</mi></msub><mo>=</mo><mrow><mi>k</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mrow><msub><mover accent="true"><mi>ùê≤</mi><mo>^</mo></mover><mi>U</mi></msub><mo>+</mo><mfrac><mrow><mi>ùö´</mi><mo>+</mo><mn>ùüè</mn></mrow><mn>2</mn></mfrac></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p class="ltx_p">where <math id="S3.SS1.p5.m1" class="ltx_Math" alttext="\hat{\mathbf{y}}_{O}" display="inline"><msub><mover accent="true"><mi>ùê≤</mi><mo>^</mo></mover><mi>O</mi></msub></math> and <math id="S3.SS1.p5.m2" class="ltx_Math" alttext="\hat{\mathbf{y}}_{U}" display="inline"><msub><mover accent="true"><mi>ùê≤</mi><mo>^</mo></mover><mi>U</mi></msub></math> are the vectors of <math id="S3.SS1.p5.m3" class="ltx_Math" alttext="(x,y)" display="inline"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></math> coordinates of a generic waypoint in the input and output reference systems, respectively; <math id="S3.SS1.p5.m4" class="ltx_Math" alttext="\mathbf{\Delta}" display="inline"><mi>ùö´</mi></math> is the vector of the <math id="S3.SS1.p5.m5" class="ltx_Math" alttext="(\Delta x,\Delta y)" display="inline"><mrow><mo stretchy="false">(</mo><mrow><mi mathvariant="normal">Œî</mi><mo>‚Å¢</mo><mi>x</mi></mrow><mo>,</mo><mrow><mi mathvariant="normal">Œî</mi><mo>‚Å¢</mo><mi>y</mi></mrow><mo stretchy="false">)</mo></mrow></math> normalized compensation factors.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p class="ltx_p">The prediction confidences stored in the first channel of the output map <math id="S3.SS1.p6.m1" class="ltx_Math" alttext="\hat{\textbf{{Y}}}" display="inline"><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">Y</mtext><mo>^</mo></mover></math> are compared to a confidence threshold <math id="S3.SS1.p6.m2" class="ltx_Math" alttext="c_{thr}" display="inline"><msub><mi>c</mi><mrow><mi>t</mi><mo>‚Å¢</mo><mi>h</mi><mo>‚Å¢</mo><mi>r</mi></mrow></msub></math>, and all the positions with <math id="S3.SS1.p6.m3" class="ltx_Math" alttext="P(u)&gt;c_{thr}" display="inline"><mrow><mrow><mi>P</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><msub><mi>c</mi><mrow><mi>t</mi><mo>‚Å¢</mo><mi>h</mi><mo>‚Å¢</mo><mi>r</mi></mrow></msub></mrow></math> are selected and projected in the input space as in Eq. <a href="#S3.E3" title="In III-A Waypoints Estimation ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Furthermore, a suppression mechanism is adopted as in standard object detection algorithms in order to avoid multiple predictions of the same waypoint: all the points falling within a distance threshold <math id="S3.SS1.p6.m4" class="ltx_Math" alttext="d_{thr}" display="inline"><msub><mi>d</mi><mrow><mi>t</mi><mo>‚Å¢</mo><mi>h</mi><mo>‚Å¢</mo><mi>r</mi></mrow></msub></math> from each other are replaced with the one with maximum confidence <math id="S3.SS1.p6.m5" class="ltx_Math" alttext="P(u)" display="inline"><mrow><mi>P</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></mrow></math>. The final waypoints are stored in the list <span class="ltx_text ltx_markedasmath ltx_font_bold"><span class="ltx_text ltx_font_italic">W</span></span>.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p class="ltx_p">The network is characterized by a stack of <math id="S3.SS1.p7.m1" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> Residual Reduction modules, that are based on 2D convolutions with Mish activation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="Mish: a self regularized non-monotonic neural activation function" class="ltx_ref">36</a>]</cite> and implement both channel and spatial attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="Cbam: convolutional block attention module" class="ltx_ref">48</a>]</cite>. Each module halves the spatial dimension with a Reduction block based on 2D convolutions with strides of two. After <math id="S3.SS1.p7.m2" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> modules, a 2D Transpose Convolution increases the spatial dimension by a factor of two. A last 2D convolution projects a concatenation of the features of the last two modules to the 3-dimensional space of the output. Since the first and last convolutional layers also have strides of two, the output tensor spatial dimension is reduced by a factor <math id="S3.SS1.p7.m3" class="ltx_Math" alttext="k=(N+1)^{2}" display="inline"><mrow><mi>k</mi><mo>=</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></math> with respect to the input. The final layer uses sigmoid activation for the first channel that encodes the waypoint probability and tanh activation for the other two channels that encode the normalized compensation factors. All code related to DeepWay is open-source and can be found online<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>
              <span class="ltx_tag ltx_tag_note">4</span>
              
              
            https://github.com/fsalv/DeepWay</span></span></span>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="261" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example of an output map <math id="S3.F4.m9" class="ltx_Math" alttext="\hat{\textbf{{Y}}}" display="inline"><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">Y</mtext><mo>^</mo></mover></math> of DeepWay <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="DeepWay: a deep learning waypoint estimator for global path generation" class="ltx_ref">35</a>]</cite>. Since <math id="S3.F4.m10" class="ltx_Math" alttext="k=8" display="inline"><mrow><mi>k</mi><mo>=</mo><mn>8</mn></mrow></math>, each cell <math id="S3.F4.m11" class="ltx_Math" alttext="u" display="inline"><mi>u</mi></math> represents a square area of <math id="S3.F4.m12" class="ltx_Math" alttext="8\times 8" display="inline"><mrow><mn>8</mn><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>8</mn></mrow></math> pixels of the original occupancy grid. The local reference system <math id="S3.F4.m13" class="ltx_Math" alttext="R_{u}" display="inline"><msub><mi>R</mi><mi>u</mi></msub></math> is at the centre of the cell and the compensation factors <math id="S3.F4.m14" class="ltx_Math" alttext="\Delta x" display="inline"><mrow><mi mathvariant="normal">Œî</mi><mo>‚Å¢</mo><mi>x</mi></mrow></math> and <math id="S3.F4.m15" class="ltx_Math" alttext="\Delta y" display="inline"><mrow><mi mathvariant="normal">Œî</mi><mo>‚Å¢</mo><mi>y</mi></mrow></math> are normalized to the <math id="S3.F4.m16" class="ltx_Math" alttext="[-1,1]" display="inline"><mrow><mo stretchy="false">[</mo><mrow><mo>‚àí</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></math> range, as a fraction of the semi-cell length.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-B </span><span class="ltx_text ltx_font_italic">Global Path Planning</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">The output list of waypoints <span class="ltx_text ltx_markedasmath ltx_font_bold"><span class="ltx_text ltx_font_italic">W</span></span> should be ordered in order to plan a global path that reaches a full coverage of the field. We adopt the same post-processing as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="DeepWay: a deep learning waypoint estimator for global path generation" class="ltx_ref">35</a>]</cite> with the following steps:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p class="ltx_p">the row crops orientation is estimated from the occupancy grid <math id="S3.I1.i1.p1.m1" class="ltx_Math" alttext="\textbf{{X}}_{occ}" display="inline"><msub><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>o</mi><mo>‚Å¢</mo><mi>c</mi><mo>‚Å¢</mo><mi>c</mi></mrow></msub></math> with the progressive probabilistic Hough transform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="Robust detection of lines using the progressive probabilistic hough transform" class="ltx_ref">33</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p class="ltx_p">the waypoints are clustered using the density-based algorithm DBSCAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="A density-based algorithm for discovering clusters in large spatial databases with noise." class="ltx_ref">15</a>]</cite> that creates a variable number of clusters depending on the space density of the waypoints.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p class="ltx_p">the points in each cluster are ordered projecting them along the normal to the direction estimated in step 1).</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p class="ltx_p">the clusters are merged with a heuristic approach based on their position and size until two main groups representing the two sides of the field are reached.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p class="ltx_p">the final ordered list of waypoints <math id="S3.I1.i5.p1.m1" class="ltx_Math" alttext="\textbf{\emph{W}}_{ord}\in\mathbb{N}^{l\times 2}" display="inline"><mrow><msub><mtext class="ltx_mathvariant_bold-italic"><em class="ltx_emph ltx_font_bold ltx_font_italic">W</em></mtext><mrow><mi>o</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>d</mi></mrow></msub><mo>‚àà</mo><msup><mi>‚Ñï</mi><mrow><mi>l</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>2</mn></mrow></msup></mrow></math> is obtained selecting the points from the two main clusters following an A-B-B-A scheme.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p class="ltx_p">The global path is generated from the ordered list <math id="S3.SS2.p3.m1" class="ltx_Math" alttext="\textbf{\emph{W}}_{ord}" display="inline"><msub><mtext class="ltx_mathvariant_bold-italic"><em class="ltx_emph ltx_font_bold ltx_font_italic">W</em></mtext><mrow><mi>o</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>d</mi></mrow></msub></math> in two steps. The intra-row paths are obtained with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="An adaptive row crops path generator with deep learning synergy" class="ltx_ref">7</a>]</cite>, which exploits a gradient-based planner between the starting and the ending waypoints of each row. On the other hand, the inter-row paths are generated with a circular pattern in order to keep a safe margin from the end of the rows to avoid collision during the turns. Considering an end-row waypoint <math id="S3.SS2.p3.m2" class="ltx_Math" alttext="\textbf{{p}}_{i}" display="inline"><msub><mtext class="ltx_mathvariant_bold-italic">p</mtext><mi>i</mi></msub></math> and the successive point that starts the following row <math id="S3.SS2.p3.m3" class="ltx_Math" alttext="\textbf{{p}}_{i+1}" display="inline"><msub><mtext class="ltx_mathvariant_bold-italic">p</mtext><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></math>, the waypoints are firstly moved along the row direction to get an end-row margin <math id="S3.SS2.p3.m4" class="ltx_Math" alttext="d_{er}" display="inline"><msub><mi>d</mi><mrow><mi>e</mi><mo>‚Å¢</mo><mi>r</mi></mrow></msub></math>:</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1" class="ltx_Math" alttext="\begin{split}\textbf{{p}}_{i}^{shifted}&amp;=\textbf{{p}}_{i}+d_{er}\begin{bmatrix%
}\text{cos}\alpha\\
\text{sin}\alpha\end{bmatrix}\\
\textbf{{p}}_{i+1}^{shifted}&amp;=\textbf{{p}}_{i+1}+(d_{er}+\Delta\textbf{{d}})%
\begin{bmatrix}\text{cos}\alpha\\
\text{sin}\alpha\end{bmatrix}\end{split}" display="block"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><msubsup><mtext class="ltx_mathvariant_bold-italic">p</mtext><mi>i</mi><mrow><mi>s</mi><mo>‚Å¢</mo><mi>h</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>f</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>d</mi></mrow></msubsup></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>=</mo><mrow><msub><mtext class="ltx_mathvariant_bold-italic">p</mtext><mi>i</mi></msub><mo>+</mo><mrow><msub><mi>d</mi><mrow><mi>e</mi><mo>‚Å¢</mo><mi>r</mi></mrow></msub><mo>‚Å¢</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mrow><mtext>cos</mtext><mo>‚Å¢</mo><mi>Œ±</mi></mrow></mtd></mtr><mtr><mtd><mrow><mtext>sin</mtext><mo>‚Å¢</mo><mi>Œ±</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><msubsup><mtext class="ltx_mathvariant_bold-italic">p</mtext><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>s</mi><mo>‚Å¢</mo><mi>h</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>f</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>d</mi></mrow></msubsup></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>=</mo><mrow><msub><mtext class="ltx_mathvariant_bold-italic">p</mtext><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>d</mi><mrow><mi>e</mi><mo>‚Å¢</mo><mi>r</mi></mrow></msub><mo>+</mo><mrow><mi mathvariant="normal">Œî</mi><mo>‚Å¢</mo><mtext class="ltx_mathvariant_bold-italic">d</mtext></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>‚Å¢</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mrow><mtext>cos</mtext><mo>‚Å¢</mo><mi>Œ±</mi></mrow></mtd></mtr><mtr><mtd><mrow><mtext>sin</mtext><mo>‚Å¢</mo><mi>Œ±</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow></mrow></mtd></mtr></mtable></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p class="ltx_p">where <math id="S3.SS2.p5.m1" class="ltx_Math" alttext="\alpha" display="inline"><mi>Œ±</mi></math> is the angle estimated during the post-processing steps and <math id="S3.SS2.p5.m2" class="ltx_Math" alttext="\Delta\textbf{{d}}" display="inline"><mrow><mi mathvariant="normal">Œî</mi><mo>‚Å¢</mo><mtext class="ltx_mathvariant_bold-italic">d</mtext></mrow></math> is the distance between the two points along the row direction:</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1" class="ltx_Math" alttext="\Delta\textbf{{d}}=(\textbf{{p}}_{i}-\textbf{{p}}_{i+1})\cdot\begin{bmatrix}%
\text{cos}\alpha\\
\text{sin}\alpha\end{bmatrix}" display="block"><mrow><mrow><mi mathvariant="normal">Œî</mi><mo>‚Å¢</mo><mtext class="ltx_mathvariant_bold-italic">d</mtext></mrow><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mtext class="ltx_mathvariant_bold-italic">p</mtext><mi>i</mi></msub><mo>‚àí</mo><msub><mtext class="ltx_mathvariant_bold-italic">p</mtext><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">‚ãÖ</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mrow><mtext>cos</mtext><mo>‚Å¢</mo><mi>Œ±</mi></mrow></mtd></mtr><mtr><mtd><mrow><mtext>sin</mtext><mo>‚Å¢</mo><mi>Œ±</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p class="ltx_p">The end-row margin <math id="S3.SS2.p7.m1" class="ltx_Math" alttext="d_{er}" display="inline"><msub><mi>d</mi><mrow><mi>e</mi><mo>‚Å¢</mo><mi>r</mi></mrow></msub></math> can be selected depending on the meters/pixel resolution of the occupancy grid in order to have a target margin in meters in the real environment. A circular interpolation is adopted to connect the shifted points by linearly interpolating the angles considering the mean point as the center of the circumference.
The whole sequence of points obtained by the intra-row and inter-rows planning creates the global path <math id="S3.SS2.p7.m2" class="ltx_Math" alttext="\textbf{\emph{P}}=\{(x,y)|x,y\in\mathbb{R}\}" display="inline"><mrow><mtext class="ltx_mathvariant_bold-italic"><em class="ltx_emph ltx_font_bold ltx_font_italic">P</em></mtext><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">|</mo><mrow><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>‚àà</mo><mi>‚Ñù</mi></mrow><mo stretchy="false">}</mo></mrow></mrow></math>, defined in the reference system of the occupancy grid <math id="S3.SS2.p7.m3" class="ltx_Math" alttext="\textbf{{X}}_{occ}" display="inline"><msub><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>o</mi><mo>‚Å¢</mo><mi>c</mi><mo>‚Å¢</mo><mi>c</mi></mrow></msub></math>. If the field map is georeferenced, it is possible to convert the global path into a list of geographic coordinates that can be directly used in the local planning phase to control the UGV motion. In Fig. <a href="#S3.F3" title="Figure 3 ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> an aerial view of a field is shown, together with the predicted waypoints in yellow and the global path in red.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-C </span><span class="ltx_text ltx_font_italic">Segmentation Network</span>
</h3>

<figure id="S3.F5" class="ltx_figure"><img src="x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="407" height="233" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Graphical representation of the architecture of the segmentation network. Features at different resolution feed the segmentation head that combines them producing the output binary map, <math id="S3.F5.m2" class="ltx_Math" alttext="\hat{\textbf{{X}}}_{seg}" display="inline"><msub><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">X</mtext><mo>^</mo></mover><mrow><mi>s</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>g</mi></mrow></msub></math>.</figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p">The overall segmentation network acts as a function <math id="S3.SS3.p1.m1" class="ltx_Math" alttext="H_{seg}" display="inline"><msub><mi>H</mi><mrow><mi>s</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>g</mi></mrow></msub></math>, parameterized by <math id="S3.SS3.p1.m2" class="ltx_Math" alttext="\Theta" display="inline"><mi mathvariant="normal">Œò</mi></math>, that at each temporal instant <math id="S3.SS3.p1.m3" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> takes as input the RGB frame from the onboard camera <math id="S3.SS3.p1.m4" class="ltx_Math" alttext="\textbf{{X}}_{rgb}\in\mathbb{R}^{h\times w\times c}" display="inline"><mrow><msub><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>r</mi><mo>‚Å¢</mo><mi>g</mi><mo>‚Å¢</mo><mi>b</mi></mrow></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>h</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>w</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>c</mi></mrow></msup></mrow></math> and produces a binary map, <math id="S3.SS3.p1.m5" class="ltx_Math" alttext="\hat{\textbf{{X}}}_{seg}\in\mathbb{R}^{h\times w}" display="inline"><mrow><msub><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">X</mtext><mo>^</mo></mover><mrow><mi>s</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>g</mi></mrow></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>h</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>w</mi></mrow></msup></mrow></math> with <math id="S3.SS3.p1.m6" class="ltx_Math" alttext="h" display="inline"><mi>h</mi></math>, <math id="S3.SS3.p1.m7" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> and <math id="S3.SS3.p1.m8" class="ltx_Math" alttext="c" display="inline"><mi>c</mi></math> as height, width and channels, respectively. The output positive class segments the crops and the foliage in the camera view. Ideally, it should be equally split on the sides of the frame for a perfectly centered path. Successively, the semantic information of the row, <math id="S3.SS3.p1.m9" class="ltx_Math" alttext="\hat{\textbf{{X}}}_{seg}" display="inline"><msub><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">X</mtext><mo>^</mo></mover><mrow><mi>s</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>g</mi></mrow></msub></math>, is used in conjunction with its corresponding depth map to control all movements of the platform inside the crops rows.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p class="ltx_p">Among all recent real-time semantic segmentation models, we carefully select an architecture that guarantees high accuracy levels by also containing hardware costs, optimization simplicity, and computational load. Indeed, the segmentation-based control does not considerably benefit from fine grained predictions and elaborated encoder-decoder networks, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="Real-time semantic segmentation with fast attention" class="ltx_ref">21</a>]</cite>, or two-pathway backbones, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="Bisenet v2: bilateral network with guided aggregation for real-time semantic segmentation" class="ltx_ref">49</a>]</cite>, does not bring any considerable improvement. Therefore, we adopt a very lightweight backbone, MobileNetV3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="Searching for mobilenetv3" class="ltx_ref">20</a>]</cite>, followed by a reduced
version of the Atrous Spatial Pyramid Pooling module, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="Rethinking atrous convolution for semantic image segmentation" class="ltx_ref">8</a>]</cite>, to capture richer contextual information with minimal computational impact. Indeed, the output of the last layer of the backbone can not be used directly to predict the segmentation mask due to the lack of spatial details.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p class="ltx_p">The overall architecture is depicted in Fig. <a href="#S3.F5" title="Figure 5 ‚Ä£ III-C Segmentation Network ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The backbone, with repeated spatial reductions, extracts contextual information and two of its branches at different resolution feed the segmentation head. One layer applies atrous convolution
to the 1/16 resolution to extract denser features, and the
other one is used to add a skip connection from the 1/4
resolution to work with more detailed information. Finally, in order to maintain real-time performance even without hardware accelerators, we employ a 224x224 low-resolution input. So, we rescale the global
average pooling layer setting the kernel size to 12√ó12 with
strides (4,5). Additionally, to have equal input and output
dimensions, we add a final bilinear upsampling with a factor of 8 at the end of the segmentation head.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-D </span><span class="ltx_text ltx_font_italic">Segmentation-based control</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p class="ltx_p">The segmentation masks <math id="S3.SS4.p1.m1" class="ltx_Math" alttext="\hat{\textbf{{X}}}_{seg}\in\mathbb{R}^{h\times w}" display="inline"><mrow><msub><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">X</mtext><mo>^</mo></mover><mrow><mi>s</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>g</mi></mrow></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>h</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>w</mi></mrow></msup></mrow></math> provided by the deep neural network are post-processed and fed into a custom control algorithm in order to generate consistent velocity commands to drive the UGV inside the inter-row space and maintain as much as possible the inter-row centrality. As in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="Deep semantic segmentation at the edge for autonomous navigation in vineyard rows" class="ltx_ref">1</a>]</cite>, we compute a sum of <math id="S3.SS4.p1.m2" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> segmentation maps along with an intersection with depth information provided by an RGB-D camera in order to obtain a more stable control. First, we pick <math id="S3.SS4.p1.m3" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> consecutive segmentation maps at times <math id="S3.SS4.p1.m4" class="ltx_Math" alttext="\{t-S,...,t\}" display="inline"><mrow><mo stretchy="false">{</mo><mrow><mi>t</mi><mo>‚àí</mo><mi>S</mi></mrow><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></math> and we fuse them</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1" class="ltx_Math" alttext="\hat{\textbf{{X}}}_{cumSeg}^{t}=\sum_{n=0}^{S}\hat{\textbf{{X}}}_{seg}^{t-n}" display="block"><mrow><msubsup><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">X</mtext><mo>^</mo></mover><mrow><mi>c</mi><mo>‚Å¢</mo><mi>u</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>S</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>g</mi></mrow><mi>t</mi></msubsup><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mi>S</mi></munderover><msubsup><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">X</mtext><mo>^</mo></mover><mrow><mi>s</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>g</mi></mrow><mrow><mi>t</mi><mo>‚àí</mo><mi>n</mi></mrow></msubsup></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p class="ltx_p">then, we join the depth information <math id="S3.SS4.p3.m1" class="ltx_Math" alttext="\textbf{{X}}_{depth}^{t}\in\mathbb{R}^{h\times w}" display="inline"><mrow><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>d</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>p</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>h</mi></mrow><mi>t</mi></msubsup><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>h</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>w</mi></mrow></msup></mrow></math> to reduce the line of sight of the actual scene and remove some background noise.
The line of sight is limited of a fixed value generating a binary map <math id="S3.SS4.p3.m2" class="ltx_Math" alttext="\textbf{{X}}_{depthT}^{t}\in\mathbb{N}^{h\times w}" display="inline"><mrow><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>d</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>p</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>h</mi><mo>‚Å¢</mo><mi>T</mi></mrow><mi>t</mi></msubsup><mo>‚àà</mo><msup><mi>‚Ñï</mi><mrow><mi>h</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>w</mi></mrow></msup></mrow></math>, as follows:</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1" class="ltx_Math" alttext="{\textit{X}_{depthT_{\begin{subarray}{c}i=0,...,h\\
j=0,...,w\end{subarray}}}^{t}}(i,j)=\begin{cases}0,&amp;\mbox{if }(\textit{X}_{%
depth}^{t})_{i,j}\geq d_{depth}\\
1,&amp;\mbox{if }(\textit{X}_{depth}^{t})_{i,j}&lt;d_{depth}\end{cases}" display="block"><mrow><mrow><msubsup><mtext class="ltx_mathvariant_italic">X</mtext><mrow><mi>d</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>p</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>h</mi><mo>‚Å¢</mo><msub><mi>T</mi><mtable rowspacing="0pt"><mtr><mtd><mrow><mi mathsize="140%">i</mi><mo mathsize="140%">=</mo><mrow><mn mathsize="140%">0</mn><mo mathsize="140%">,</mo><mi mathsize="140%" mathvariant="normal">‚Ä¶</mi><mo mathsize="140%">,</mo><mi mathsize="140%">h</mi></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mi mathsize="140%">j</mi><mo mathsize="140%">=</mo><mrow><mn mathsize="140%">0</mn><mo mathsize="140%">,</mo><mi mathsize="140%" mathvariant="normal">‚Ä¶</mi><mo mathsize="140%">,</mo><mi mathsize="140%">w</mi></mrow></mrow></mtd></mtr></mtable></msub></mrow><mi>t</mi></msubsup><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mtext>if¬†</mtext><mo>‚Å¢</mo><msub><mrow><mo stretchy="false">(</mo><msubsup><mtext class="ltx_mathvariant_italic">X</mtext><mrow><mi>d</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>p</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>h</mi></mrow><mi>t</mi></msubsup><mo stretchy="false">)</mo></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo>‚â•</mo><msub><mi>d</mi><mrow><mi>d</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>p</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>h</mi></mrow></msub></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mn>1</mn><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mtext>if¬†</mtext><mo>‚Å¢</mo><msub><mrow><mo stretchy="false">(</mo><msubsup><mtext class="ltx_mathvariant_italic">X</mtext><mrow><mi>d</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>p</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>h</mi></mrow><mi>t</mi></msubsup><mo stretchy="false">)</mo></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo>&lt;</mo><msub><mi>d</mi><mrow><mi>d</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>p</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>h</mi></mrow></msub></mrow></mtd></mtr></mtable></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S3.SS4.p3.m3" class="ltx_Math" alttext="d_{depth}" display="inline"><msub><mi>d</mi><mrow><mi>d</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>p</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>h</mi></mrow></msub></math> is a fixed experimental scalar.
Finally, exploiting an interception operation between the cumulative output <math id="S3.SS4.p3.m4" class="ltx_Math" alttext="\hat{\textbf{{X}}}_{cumSeg}^{t}" display="inline"><msubsup><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">X</mtext><mo>^</mo></mover><mrow><mi>c</mi><mo>‚Å¢</mo><mi>u</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>S</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>g</mi></mrow><mi>t</mi></msubsup></math>, computed in equation (<a href="#S3.E6" title="In III-D Segmentation-based control ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) and the binary map <math id="S3.SS4.p3.m5" class="ltx_Math" alttext="\textbf{{X}}_{depthT}^{t}" display="inline"><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>d</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>p</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>h</mi><mo>‚Å¢</mo><mi>T</mi></mrow><mi>t</mi></msubsup></math> previously generated, we obtain the pre-processed input <math id="S3.SS4.p3.m6" class="ltx_Math" alttext="\textbf{{X}}_{ctrl}^{t}\in\mathbb{R}^{h\times w}" display="inline"><mrow><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>c</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>l</mi></mrow><mi>t</mi></msubsup><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>h</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>w</mi></mrow></msup></mrow></math> for the control algorithm:</p>
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1" class="ltx_Math" alttext="\textbf{{X}}_{ctrl}^{t}=\hat{\textbf{{X}}}_{cumSeg}^{t}\cap\textbf{{X}}_{%
depthT}^{t}" display="block"><mrow><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>c</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>l</mi></mrow><mi>t</mi></msubsup><mo>=</mo><mrow><msubsup><mover accent="true"><mtext class="ltx_mathvariant_bold-italic">X</mtext><mo>^</mo></mover><mrow><mi>c</mi><mo>‚Å¢</mo><mi>u</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>S</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>g</mi></mrow><mi>t</mi></msubsup><mo>‚à©</mo><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>d</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>p</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>h</mi><mo>‚Å¢</mo><mi>T</mi></mrow><mi>t</mi></msubsup></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In the <math id="S3.SS4.p3.m7" class="ltx_Math" alttext="\textbf{{X}}_{ctrl}^{t}" display="inline"><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>c</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>l</mi></mrow><mi>t</mi></msubsup></math> binary map <math id="S3.SS4.p3.m8" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math> stands for obstacles and <math id="S3.SS4.p3.m9" class="ltx_Math" alttext="0" display="inline"><mn>0</mn></math> free-space.
The segmentation-based control algorithm is developed building over the SPC algorithm presdented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="Deep semantic segmentation at the edge for autonomous navigation in vineyard rows" class="ltx_ref">1</a>]</cite>. Indeed, we propose a simplified version of that control function to avoid useless conditional blocks and obtain a more real-time control algorithm.
<span class="ltx_ERROR undefined">{algorithm}</span>[t]
<span class="ltx_text ltx_caption">Segmentation-based algorithm</span>

<span class="ltx_ERROR undefined">\lx@orig@algorithmic</span>[1]
<span class="ltx_ERROR undefined">\REQUIRE</span><em class="ltx_emph ltx_font_bold ltx_font_italic">X</em><math id="S3.SS4.p3.m10" class="ltx_Math" alttext="{}_{ctrl}^{t}" display="inline"><mmultiscripts><mi></mi><mprescripts></mprescripts><mrow></mrow><mi>t</mi><mrow><mi>c</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>l</mi></mrow><mrow></mrow></mmultiscripts></math>: Pre-processed segmented image
<span class="ltx_ERROR undefined">\ENSURE</span><math id="S3.SS4.p3.m11" class="ltx_Math" alttext="v_{x}" display="inline"><msub><mi>v</mi><mi>x</mi></msub></math>,<math id="S3.SS4.p3.m12" class="ltx_Math" alttext="\omega_{z}" display="inline"><msub><mi>œâ</mi><mi>z</mi></msub></math>: Continuous control commands
<span class="ltx_ERROR undefined">\STATE</span>noise_reduction_function()
<span class="ltx_ERROR undefined">\FOR</span>i=0,<math id="S3.SS4.p3.m13" class="ltx_Math" alttext="\cdots,w" display="inline"><mrow><mi mathvariant="normal">‚ãØ</mi><mo>,</mo><mi>w</mi></mrow></math>
<span class="ltx_ERROR undefined">\STATE</span> <math id="S3.SS4.p3.m14" class="ltx_Math" alttext="\textbf{{c}}\leftarrow" display="inline"><mrow><mtext class="ltx_mathvariant_bold-italic">c</mtext><mo stretchy="false">‚Üê</mo><mi></mi></mrow></math>sum_colums(<math id="S3.SS4.p3.m15" class="ltx_Math" alttext="\textbf{{X}}_{ctrl}^{t}" display="inline"><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>c</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>l</mi></mrow><mi>t</mi></msubsup></math>)
<span class="ltx_ERROR undefined">\ENDFOR</span><span class="ltx_ERROR undefined">\STATE</span><math id="S3.SS4.p3.m16" class="ltx_Math" alttext="\textbf{{zeros}}\leftarrow" display="inline"><mrow><mtext class="ltx_mathvariant_bold-italic">zeros</mtext><mo stretchy="false">‚Üê</mo><mi></mi></mrow></math> list_zero_clusters(<span class="ltx_text ltx_markedasmath ltx_font_bold ltx_font_italic">c</span>)
<span class="ltx_ERROR undefined">\STATE</span><math id="S3.SS4.p3.m18" class="ltx_Math" alttext="\textbf{{max\_cluster}}\leftarrow" display="inline"><mrow><mtext class="ltx_mathvariant_bold-italic">max_cluster</mtext><mo stretchy="false">‚Üê</mo><mi></mi></mrow></math> find_max_cluster(<span class="ltx_text ltx_markedasmath ltx_font_bold ltx_font_italic">zeros</span>)
<span class="ltx_ERROR undefined">\IF</span>cluster_lenght(<span class="ltx_text ltx_markedasmath ltx_font_bold ltx_font_italic">max_cluster</span>) <math id="S3.SS4.p3.m21" class="ltx_Math" alttext="\geq anomaly_{th}" display="inline"><mrow><mi></mi><mo>‚â•</mo><mrow><mi>a</mi><mo>‚Å¢</mo><mi>n</mi><mo>‚Å¢</mo><mi>o</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>a</mi><mo>‚Å¢</mo><mi>l</mi><mo>‚Å¢</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>‚Å¢</mo><mi>h</mi></mrow></msub></mrow></mrow></math>
<span class="ltx_ERROR undefined">\STATE</span><math id="S3.SS4.p3.m22" class="ltx_Math" alttext="v_{x}" display="inline"><msub><mi>v</mi><mi>x</mi></msub></math>,<math id="S3.SS4.p3.m23" class="ltx_Math" alttext="\omega_{z}\leftarrow 0,0" display="inline"><mrow><msub><mi>œâ</mi><mi>z</mi></msub><mo stretchy="false">‚Üê</mo><mrow><mn>0</mn><mo>,</mo><mn>0</mn></mrow></mrow></math>
<span class="ltx_ERROR undefined">\ELSE</span><span class="ltx_ERROR undefined">\STATE</span>compute_cluster_center()
<span class="ltx_ERROR undefined">\STATE</span><math id="S3.SS4.p3.m24" class="ltx_Math" alttext="v_{x}" display="inline"><msub><mi>v</mi><mi>x</mi></msub></math>,<math id="S3.SS4.p3.m25" class="ltx_Math" alttext="\omega_{z}\leftarrow" display="inline"><mrow><msub><mi>œâ</mi><mi>z</mi></msub><mo stretchy="false">‚Üê</mo><mi></mi></mrow></math> control_function()
<span class="ltx_ERROR undefined">\ENDIF</span>

Algorithm <a href="#S3.E8" title="In III-D Segmentation-based control ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> contains the pseudo-code of the proposed custom control, that starting from a pre-processed segmented image <math id="S3.SS4.p3.m26" class="ltx_Math" alttext="\textbf{{X}}_{ctrl}^{t}" display="inline"><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>c</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>l</mi></mrow><mi>t</mi></msubsup></math>, is responsible for computing the driving velocity commands. First, a noise reduction function gets rid of undesired noise in the bottom part of the cumulative segmentation mask <math id="S3.SS4.p3.m27" class="ltx_Math" alttext="\textbf{{X}}_{ctrl}^{t}" display="inline"><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>c</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>l</mi></mrow><mi>t</mi></msubsup></math> due to grass on the terrain, that may be wrongly segmented by the neural network. To perform such operation, we compute the sum over rows of <math id="S3.SS4.p3.m28" class="ltx_Math" alttext="\textbf{{X}}_{ctrl}^{t}" display="inline"><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>c</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>l</mi></mrow><mi>t</mi></msubsup></math> obtaining an array <math id="S3.SS4.p3.m29" class="ltx_Math" alttext="\textbf{{g}}_{noise}\in\mathbb{R}^{h}" display="inline"><mrow><msub><mtext class="ltx_mathvariant_bold-italic">g</mtext><mrow><mi>n</mi><mo>‚Å¢</mo><mi>o</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>s</mi><mo>‚Å¢</mo><mi>e</mi></mrow></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mi>h</mi></msup></mrow></math>, then we set <math id="S3.SS4.p3.m30" class="ltx_Math" alttext="{\textbf{{X}}_{ctrl}^{t}}_{(indices,:)}=0" display="inline"><mrow><mmultiscripts><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>c</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>l</mi></mrow><mi>t</mi><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo>‚Å¢</mo><mi>n</mi><mo>‚Å¢</mo><mi>d</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>c</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>s</mi></mrow><mo>,</mo><mo rspace="0em">:</mo><mo stretchy="false">)</mo></mrow><mrow></mrow></mmultiscripts><mo>=</mo><mn>0</mn></mrow></math>, where <math id="S3.SS4.p3.m31" class="ltx_Math" alttext="indices" display="inline"><mrow><mi>i</mi><mo>‚Å¢</mo><mi>n</mi><mo>‚Å¢</mo><mi>d</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>c</mi><mo>‚Å¢</mo><mi>e</mi><mo>‚Å¢</mo><mi>s</mi></mrow></math> contains the matrix-row indices such that <math id="S3.SS4.p3.m32" class="ltx_Math" alttext="\textbf{{g}}_{noise}&lt;{th}_{noise}" display="inline"><mrow><msub><mtext class="ltx_mathvariant_bold-italic">g</mtext><mrow><mi>n</mi><mo>‚Å¢</mo><mi>o</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>s</mi><mo>‚Å¢</mo><mi>e</mi></mrow></msub><mo>&lt;</mo><mrow><mi>t</mi><mo>‚Å¢</mo><msub><mi>h</mi><mrow><mi>n</mi><mo>‚Å¢</mo><mi>o</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>s</mi><mo>‚Å¢</mo><mi>e</mi></mrow></msub></mrow></mrow></math>, with <math id="S3.SS4.p3.m33" class="ltx_Math" alttext="{th}_{noise}=0.03\cdot max(\textbf{{g}}_{noise})" display="inline"><mrow><mrow><mi>t</mi><mo>‚Å¢</mo><msub><mi>h</mi><mrow><mi>n</mi><mo>‚Å¢</mo><mi>o</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>s</mi><mo>‚Å¢</mo><mi>e</mi></mrow></msub></mrow><mo>=</mo><mrow><mrow><mn>0.03</mn><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><mi>m</mi></mrow><mo>‚Å¢</mo><mi>a</mi><mo>‚Å¢</mo><mi>x</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><msub><mtext class="ltx_mathvariant_bold-italic">g</mtext><mrow><mi>n</mi><mo>‚Å¢</mo><mi>o</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>s</mi><mo>‚Å¢</mo><mi>e</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></math> as threshold. We perform such operation because, in an ideal segmentation mask there are no 1s at the top of the image and on the bottom, whilst the majority of them are supposed to be in the central belt.
After the noise reduction phase, we store the sum over columns of the obtained matrix <math id="S3.SS4.p3.m34" class="ltx_Math" alttext="\textbf{{X}}_{ctrl}^{t}" display="inline"><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>c</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>l</mi></mrow><mi>t</mi></msubsup></math> in the array <math id="S3.SS4.p3.m35" class="ltx_Math" alttext="\textbf{{c}}\in\mathbb{R}^{w}" display="inline"><mrow><mtext class="ltx_mathvariant_bold-italic">c</mtext><mo>‚àà</mo><msup><mi>‚Ñù</mi><mi>w</mi></msup></mrow></math>, that contains the amount of segmented trees for each column. Therefore, every zero in <span class="ltx_text ltx_markedasmath ltx_font_bold ltx_font_italic">c</span> is a potential empty space where to route the mobile platform.
Then, we select the clusters of zeros in <span class="ltx_text ltx_markedasmath ltx_font_bold ltx_font_italic">c</span>, which are the groups of consecutive zeros, in order to store them in the list <span class="ltx_text ltx_markedasmath ltx_font_bold ltx_font_italic">zeros</span>.
Next, we look for the largest cluster of zeros <span class="ltx_text ltx_markedasmath ltx_font_bold ltx_font_italic">max_cluster</span> and in case of the length of such cluster is over an empirically chosen threshold, <math id="S3.SS4.p3.m40" class="ltx_Math" alttext="anomaly\_th=0.8\cdot w" display="inline"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>n</mi><mo>‚Å¢</mo><mi>o</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>a</mi><mo>‚Å¢</mo><mi>l</mi><mo>‚Å¢</mo><mi>y</mi><mo>‚Å¢</mo><mi mathvariant="normal">_</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>h</mi></mrow><mo>=</mo><mrow><mn>0.8</mn><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><mi>w</mi></mrow></mrow></math>, the driving commands are set to zero value, because it means the provided cumulative segmentation mask <math id="S3.SS4.p3.m41" class="ltx_Math" alttext="\textbf{{X}}_{ctrl}^{t}" display="inline"><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>c</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>l</mi></mrow><mi>t</mi></msubsup></math> has more zeros than ones, that is an anomaly, so for safety reason the mobile platform is stopped. While, in case of no anomalies, we compute the cluster center that is given as input to the control function.
The identified cluster contains the obstacle-free space information that can be exploited to safely drive the mobile platform; as a consequence the linear and angular velocities are computed using the center of the selected cluster, which ideally corresponds to the center position of the row in front of the UGV. The desired velocities are obtained by means of two custom functions:</p>
<table id="S3.E9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E9.m1" class="ltx_Math" alttext="\omega_{z}=-\omega_{z,gain}\cdot d" display="block"><mrow><msub><mi>œâ</mi><mi>z</mi></msub><mo>=</mo><mrow><mo>‚àí</mo><mrow><msub><mi>œâ</mi><mrow><mi>z</mi><mo>,</mo><mrow><mi>g</mi><mo>‚Å¢</mo><mi>a</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>n</mi></mrow></mrow></msub><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><mi>d</mi></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<table id="S3.E10" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E10.m1" class="ltx_Math" alttext="v_{x}=v_{x,max}\cdot\left[1-\left[\frac{d^{2}}{(\frac{w}{2})^{2}}\right]\right]" display="block"><mrow><msub><mi>v</mi><mi>x</mi></msub><mo>=</mo><mrow><msub><mi>v</mi><mrow><mi>x</mi><mo>,</mo><mrow><mi>m</mi><mo>‚Å¢</mo><mi>a</mi><mo>‚Å¢</mo><mi>x</mi></mrow></mrow></msub><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>‚àí</mo><mrow><mo>[</mo><mfrac><msup><mi>d</mi><mn>2</mn></msup><msup><mrow><mo stretchy="false">(</mo><mfrac><mi>w</mi><mn>2</mn></mfrac><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mfrac><mo>]</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S3.SS4.p4.m1" class="ltx_Math" alttext="\omega_{z,gain}=0.01" display="inline"><mrow><msub><mi>œâ</mi><mrow><mi>z</mi><mo>,</mo><mrow><mi>g</mi><mo>‚Å¢</mo><mi>a</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>n</mi></mrow></mrow></msub><mo>=</mo><mn>0.01</mn></mrow></math> and <math id="S3.SS4.p4.m2" class="ltx_Math" alttext="v_{x,max}=1.0" display="inline"><mrow><msub><mi>v</mi><mrow><mi>x</mi><mo>,</mo><mrow><mi>m</mi><mo>‚Å¢</mo><mi>a</mi><mo>‚Å¢</mo><mi>x</mi></mrow></mrow></msub><mo>=</mo><mn>1.0</mn></mrow></math> are two constants which define the angular gain and maximum linear velocity of the mobile platform respectively, <math id="S3.SS4.p4.m3" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> is the width of <math id="S3.SS4.p4.m4" class="ltx_Math" alttext="\textbf{{X}}_{ctrl}^{t}" display="inline"><msubsup><mtext class="ltx_mathvariant_bold-italic">X</mtext><mrow><mi>c</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>l</mi></mrow><mi>t</mi></msubsup></math> and <em class="ltx_emph ltx_font_italic">d</em> is defined as:</p>
<table id="S3.E11" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E11.m1" class="ltx_Math" alttext="d=x_{c}-\frac{w}{2}" display="block"><mrow><mi>d</mi><mo>=</mo><mrow><msub><mi>x</mi><mi>c</mi></msub><mo>‚àí</mo><mfrac><mi>w</mi><mn>2</mn></mfrac></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">with <math id="S3.SS4.p4.m5" class="ltx_Math" alttext="x_{c}" display="inline"><msub><mi>x</mi><mi>c</mi></msub></math> center coordinate of the selected cluster.
Equation (<a href="#S3.E9" title="In III-D Segmentation-based control ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>) represents the angular velocity control function, while equation (<a href="#S3.E10" title="In III-D Segmentation-based control ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>) has been used to compute the linear velocity, as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="Local motion planner for autonomous navigation in vineyards with a rgb-d camera-based algorithm and deep learning synergy" class="ltx_ref">3</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="Deep semantic segmentation at the edge for autonomous navigation in vineyard rows" class="ltx_ref">1</a>]</cite>.
Eventually, the control velocity commands sent to the actuators are smoothed using the Exponential Moving Average (EMA), formalized in equation (<a href="#S3.E12" title="In III-D Segmentation-based control ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>), in order to prevent the mobile platform from sharp motion.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<table id="S3.E12" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E12.m1" class="ltx_Math" alttext="\textbf{{EMA}}_{t}=\textbf{{EMA}}_{t-1}\cdot(1-\alpha_{EMA})+\begin{bmatrix}v_%
{x}\\
\omega_{z}\end{bmatrix}\cdot\alpha_{EMA}" display="block"><mrow><msub><mtext class="ltx_mathvariant_bold-italic">EMA</mtext><mi>t</mi></msub><mo>=</mo><mrow><mrow><msub><mtext class="ltx_mathvariant_bold-italic">EMA</mtext><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>‚àí</mo><msub><mi>Œ±</mi><mrow><mi>E</mi><mo>‚Å¢</mo><mi>M</mi><mo>‚Å¢</mo><mi>A</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>v</mi><mi>x</mi></msub></mtd></mtr><mtr><mtd><msub><mi>œâ</mi><mi>z</mi></msub></mtd></mtr></mtable><mo rspace="0.055em">]</mo></mrow><mo rspace="0.222em">‚ãÖ</mo><msub><mi>Œ±</mi><mrow><mi>E</mi><mo>‚Å¢</mo><mi>M</mi><mo>‚Å¢</mo><mi>A</mi></mrow></msub></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S3.SS4.p5.m1" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> is the time step and <math id="S3.SS4.p5.m2" class="ltx_Math" alttext="\alpha_{EMA}=0.18" display="inline"><mrow><msub><mi>Œ±</mi><mrow><mi>E</mi><mo>‚Å¢</mo><mi>M</mi><mo>‚Å¢</mo><mi>A</mi></mrow></msub><mo>=</mo><mn>0.18</mn></mrow></math> the multiplier for weighting the EMA, which value is found experimentally.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-E </span><span class="ltx_text ltx_font_italic">Local Path Planning Policy</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p class="ltx_p">Once the global path <span class="ltx_text ltx_markedasmath ltx_font_bold"><span class="ltx_text ltx_font_italic">P</span></span> and the start/end rows waypoints <math id="S3.SS5.p1.m2" class="ltx_Math" alttext="\textbf{\emph{W}}_{ord}" display="inline"><msub><mtext class="ltx_mathvariant_bold-italic"><em class="ltx_emph ltx_font_bold ltx_font_italic">W</em></mtext><mrow><mi>o</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mi>d</mi></mrow></msub></math> have been correctly generated, we exploit the provided information to locally navigate throughout the whole field.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p class="ltx_p">In case of lush vegetation and thick canopies that may distort GNSS signals inside the inter-row space, the local navigation problem is solved using the synergy of two different algorithms according to the kind of navigation requested in a determined place of the considered crop:</p>
<ol id="S3.I2" class="ltx_enumerate">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p class="ltx_p">Inside the inter-row space: we exploit the custom control algorithm, described in Section <a href="#S3.SS4" title="III-D Segmentation-based control ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III-D</span></a>, based on the segmentation information provided by the deep neural network, in order to overcome localization inaccuracies due to blocked GNSS signals by overgrown plant vegetation.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p class="ltx_p">Switch between different rows: we use the standard DWA, well described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="The dynamic window approach to collision avoidance" class="ltx_ref">17</a>]</cite>, with fine-tuned parameters to safely switch between two rows following the circular path generated in Section <a href="#S3.SS2" title="III-B Global Path Planning ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III-B</span></a>, since outside the inter-row space a clear view of satellites and sky should be available.</p>
</div>
</li>
</ol>
<p class="ltx_p">The choice of the suitable algorithm happens by comparing the estimated position by the EKF localization filter and the provided start/end rows waypoints; in case of start row recognition, the local path planning policy selects the segmentation-based control, otherwise it uses the DWA, as shown in Fig. <a href="#S1.F2" title="Figure 2 ‚Ä£ I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The comparison happens by computing a simple Euclidean distance between the start/end rows waypoint and the estimated positions. In case the calculated distance is lower than a threshold, <math id="S3.SS5.p2.m1" class="ltx_Math" alttext="waypoint\_th=0.5" display="inline"><mrow><mrow><mi>w</mi><mo>‚Å¢</mo><mi>a</mi><mo>‚Å¢</mo><mi>y</mi><mo>‚Å¢</mo><mi>p</mi><mo>‚Å¢</mo><mi>o</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>n</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi mathvariant="normal">_</mi><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><mi>h</mi></mrow><mo>=</mo><mn>0.5</mn></mrow></math>, the algorithm considers the waypoint as reached and selects the right local controller.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p class="ltx_p">On the other hand, during specific year periods, when plant vegetation is not so dense and overgrown, the local path planning policy exploits only DWA to navigate throughout the whole field, thanks to a good view of both sky and satellites in every place of the crop, following the complete global path generated in Section <a href="#S3.SS2" title="III-B Global Path Planning ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III-B</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps">Experimental Results and Discussions</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">In this section, we describe the main experiments conducted in simulation and real environments in order to better validate the algorithms. First, we discuss the synthetic datasets creation for the training of the two deep neural networks. Then, we illustrate the training process and the optimization techniques adopted to minimize inference costs. Finally, we conclude with the simulation and real environments evaluations.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">All the code have been developed ROS-compatible, in order to exploit some of the most used ROS packages in robotics research, as <span class="ltx_text ltx_font_italic">move_base<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup>
              <span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright">5</span></span>
              
              
            <span class="ltx_text ltx_font_upright">http://wiki.ros.org/move_base</span></span></span></span></span> and <span class="ltx_text ltx_font_italic">robot_localization<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup>
              <span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright">6</span></span>
              
              
            <span class="ltx_text ltx_font_upright">http://docs.ros.org/en/melodic/api/robot_localization/html/index.html</span></span></span></span></span> packages, that offer ready-to-use local planners and basic localization methods, respectively. Moreover, all tests have been performed using Ubuntu 18.04 and ROS Melodic.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-A </span><span class="ltx_text ltx_font_italic">Datasets creation</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">As far as the two presented deep neural networks are concerned, they require to be trained on specific datasets according to the desired final application. Supervised learning training algorithms are the easiest to adopt with usually the best final results. However, they all require supervision by means of a labeled training dataset. That greatly affects costs and makes data collection complex and time-consuming. Therefore, we make exclusively use of synthetic data and domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="Domain randomization for transferring deep neural networks from simulation to the real world" class="ltx_ref">46</a>]</cite> to enable affordable supervised learning and simultaneously bridge the domain gap between simulation and reality.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p">For DeepWay, a dataset of random occupancy grids is generated. All parameters such as number, orientation, depth, and length of the rows are randomly selected. The coordinates of starting and ending points of each row are generated by geometrical reasoning and the occupancy grid is obtained with circles of different radius for each location in between two corresponding points. Random holes in the rows are also created to increase the variability of the images. Ground truth waypoints are obtained to always lay inside the rows, since we experimentally found it helps the network prediction. Given two starting/ending points <span class="ltx_text ltx_markedasmath ltx_font_bold ltx_font_italic">a</span> and <span class="ltx_text ltx_markedasmath ltx_font_bold ltx_font_italic">b</span> of two successive rows, we compute the target waypoint as follows:</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<table id="S4.E13" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E13.m1" class="ltx_Math" alttext="\textbf{{p}}_{\textbf{{a}},\textbf{{b}}}=\begin{bmatrix}0&amp;\mp 1\\
\pm 1&amp;0\end{bmatrix}\frac{\textbf{{a}}-\textbf{{b}}}{2}+\frac{\textbf{{a}}+%
\textbf{{b}}}{2}" display="block"><mrow><msub><mtext class="ltx_mathvariant_bold-italic">p</mtext><mrow><mtext class="ltx_mathvariant_bold-italic">a</mtext><mo>,</mo><mtext class="ltx_mathvariant_bold-italic">b</mtext></mrow></msub><mo>=</mo><mrow><mrow><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><mn>0</mn></mtd><mtd><mrow><mo>‚àì</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd><mrow><mo>¬±</mo><mn>1</mn></mrow></mtd><mtd><mn>0</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>‚Å¢</mo><mfrac><mrow><mtext class="ltx_mathvariant_bold-italic">a</mtext><mo>‚àí</mo><mtext class="ltx_mathvariant_bold-italic">b</mtext></mrow><mn>2</mn></mfrac></mrow><mo>+</mo><mfrac><mrow><mtext class="ltx_mathvariant_bold-italic">a</mtext><mo>+</mo><mtext class="ltx_mathvariant_bold-italic">b</mtext></mrow><mn>2</mn></mfrac></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p class="ltx_p">that corresponds to a <math id="S4.SS1.p4.m1" class="ltx_Math" alttext="\pm 90" display="inline"><mrow><mo>¬±</mo><mn>90</mn></mrow></math> degrees rotation around the mean point, where the sign is selected to make the waypoint inside the row. We train DeepWay with a total of 3000 synthetic images and we validate it with 100 satellite images taken from the Google Maps database and manually annotated.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="x6.png" id="S4.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="124" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="x7.png" id="S4.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="165" height="124" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>An example of synthetic RGB image (a) and the corresponding segmentation mask (b).</figcaption>
</figure>
<div id="S4.SS1.p5" class="ltx_para">
<p class="ltx_p">On the other hand, the segmentation neural network requires a set of RGB images along with segmentation masks as inputs in order to be correctly trained. As a consequence, inspired by the work of Tejaswi Digumarti et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="An approach for semantic segmentation of tree-like vegetation" class="ltx_ref">45</a>]</cite>, we generate synthetic RGB images coupled with the corresponding segmentation masks. For such purpose, we exploit Blender<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup>
              <span class="ltx_tag ltx_tag_note">7</span>
              
              
            https://www.blender.org/</span></span></span> 2.8, which is an open-source 3D computer graphics software compatible with Python language, and the Modular Tree<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup>
              <span class="ltx_tag ltx_tag_note">8</span>
              
              
            https://github.com/MaximeHerpin/modular_tree/tree/blender_28</span></span></span> addon to speed up the tree generation. We design four main scenarios: two single different trees, one group of heterogeneous trees, and one row-based scenario with various backgrounds and soils. Then, exploiting Python language compatibility of Blender, we write a script able to automatically capture RGB images and the corresponding segmentation masks of the scene from different positions with respect to the central reference frame and with different illuminations conditions in order to obtain as much as possible a random and complete synthetic dataset. An example of a synthetic RGB image, along with the corresponding segmentation mask, is shown in Fig. <a href="#S4.F6" title="Figure 6 ‚Ä£ IV-A Datasets creation ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Every single rendering takes about <math id="S4.SS1.p5.m1" class="ltx_Math" alttext="30" display="inline"><mn>30</mn></math> seconds, multiplied by a total of 2776 rendering, which is about <math id="S4.SS1.p5.m2" class="ltx_Math" alttext="23" display="inline"><mn>23</mn></math> hours of continuous work on a RTX 2080 GPU. That total number of images in conjunction with transfer learning allows to train a segmentation network with high generalization capabilities while minimizing generation data costs. The overall segmentation training dataset is composed of 2776 RGB synthetic images for training and 100 manually annotated images for testing, acquired in a real environment (Italy, Valle San Giorgio di Baone).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-B </span><span class="ltx_text ltx_font_italic">Networks training and optimization</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">For training both networks, we employ the TensorFlow<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup>
              <span class="ltx_tag ltx_tag_note">9</span>
              
              
            https://www.tensorflow.org</span></span></span> 2
framework on a PC with 32-GB RAM, an Intel i7-9700K CPU, and an Nvidia 2080 Super GP-GPU.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">DeepWay is trained following the methodology presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="DeepWay: a deep learning waypoint estimator for global path generation" class="ltx_ref">35</a>]</cite>, with an input dimension of <math id="S4.SS2.p2.m1" class="ltx_Math" alttext="H=W=800" display="inline"><mrow><mi>H</mi><mo>=</mo><mi>W</mi><mo>=</mo><mn>800</mn></mrow></math>, and <math id="S4.SS2.p2.m2" class="ltx_Math" alttext="N=2" display="inline"><mrow><mi>N</mi><mo>=</mo><mn>2</mn></mrow></math> Residual Reduction modules, that result in a subsampling factor of <math id="S4.SS2.p2.m3" class="ltx_Math" alttext="k=8" display="inline"><mrow><mi>k</mi><mo>=</mo><mn>8</mn></mrow></math> and output dimensions of <math id="S4.SS2.p2.m4" class="ltx_Math" alttext="U_{H}=U_{W}=100" display="inline"><mrow><msub><mi>U</mi><mi>H</mi></msub><mo>=</mo><msub><mi>U</mi><mi>W</mi></msub><mo>=</mo><mn>100</mn></mrow></math>. We select a kernel size of 5 and 16 filters for all the convolutional layers, except the first and last ones, that have kernel sizes of 7 and 3, respectively. These hyperparameters have been selected by performing a grid search over reasonable sets of values and adopting those that experimentally provided the best convergence. As loss function, a weighted mean squared function (<math id="S4.SS2.p2.m5" class="ltx_Math" alttext="L_{2}" display="inline"><msub><mi>L</mi><mn>2</mn></msub></math>) is used to compensate the higher number of negative cells (i.e., with no waypoint in the target image) with respect to positive ones. We set these weights to 0.7 for the positive cells and 0.3 for the negative. The default distance threshold for the waypoints suppression algorithm is set to <math id="S4.SS2.p2.m6" class="ltx_Math" alttext="d_{thr}=8" display="inline"><mrow><msub><mi>d</mi><mrow><mi>t</mi><mo>‚Å¢</mo><mi>h</mi><mo>‚Å¢</mo><mi>r</mi></mrow></msub><mo>=</mo><mn>8</mn></mrow></math> pixels, that is the minimum inter-row distance of our dataset, and the confidence threshold to <math id="S4.SS2.p2.m7" class="ltx_Math" alttext="c_{thr}=0.9" display="inline"><mrow><msub><mi>c</mi><mrow><mi>t</mi><mo>‚Å¢</mo><mi>h</mi><mo>‚Å¢</mo><mi>r</mi></mrow></msub><mo>=</mo><mn>0.9</mn></mrow></math>, in order to select the most confident predictions only. As in standard object detection algorithms, we adopt the Average Precision (AP) as metric for the prediction quality. We compute the AP at different distance ranges <math id="S4.SS2.p2.m8" class="ltx_Math" alttext="d_{r}" display="inline"><msub><mi>d</mi><mi>r</mi></msub></math>. A prediction is considered a True Positive (TP) only if it falls within a distance <math id="S4.SS2.p2.m9" class="ltx_Math" alttext="d_{r}" display="inline"><msub><mi>d</mi><mi>r</mi></msub></math> form the target waypoint. On the 100 real-world images we reach an AP of 0.9794 with <math id="S4.SS2.p2.m10" class="ltx_Math" alttext="d_{r}=8" display="inline"><mrow><msub><mi>d</mi><mi>r</mi></msub><mo>=</mo><mn>8</mn></mrow></math> pixels, 0.9558 with 4 pixels and
0.7500 with 2 pixels.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison between different devices‚Äô energy consumption and inference performances with graph optimization (G.O.) and weight precision (W.P.).</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Device</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">GO</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">WP</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Latency [ms]</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">E<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">net</span></sub> [mJ]</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Size [MB]</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">RTX 2080</td>
<td class="ltx_td ltx_align_left ltx_border_t">N</td>
<td class="ltx_td ltx_align_left ltx_border_t">FP32</td>
<td class="ltx_td ltx_align_left ltx_border_t">28 <math id="S4.T1.m2" class="ltx_Math" alttext="\pm" display="inline"><mo>¬±</mo></math> 109</td>
<td class="ltx_td ltx_align_left ltx_border_t">819</td>
<td class="ltx_td ltx_align_left ltx_border_t">9.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_left">Y</td>
<td class="ltx_td ltx_align_left">FP32</td>
<td class="ltx_td ltx_align_left">0.1 <math id="S4.T1.m3" class="ltx_Math" alttext="\pm" display="inline"><mo>¬±</mo></math> 0.3</td>
<td class="ltx_td ltx_align_left">52</td>
<td class="ltx_td ltx_align_left">7.4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_left">Y</td>
<td class="ltx_td ltx_align_left">FP16</td>
<td class="ltx_td ltx_align_left">0.1 <math id="S4.T1.m4" class="ltx_Math" alttext="\pm" display="inline"><mo>¬±</mo></math> 0.2</td>
<td class="ltx_td ltx_align_left">39</td>
<td class="ltx_td ltx_align_left">4.9</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Cortex-A57</td>
<td class="ltx_td ltx_align_left">Y</td>
<td class="ltx_td ltx_align_left">FP32</td>
<td class="ltx_td ltx_align_left">111 <math id="S4.T1.m5" class="ltx_Math" alttext="\pm" display="inline"><mo>¬±</mo></math> 0.9</td>
<td class="ltx_td ltx_align_left">166</td>
<td class="ltx_td ltx_align_left">4.2</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_left">Y</td>
<td class="ltx_td ltx_align_left">FP16</td>
<td class="ltx_td ltx_align_left">111 <math id="S4.T1.m6" class="ltx_Math" alttext="\pm" display="inline"><mo>¬±</mo></math> 2.3</td>
<td class="ltx_td ltx_align_left">165</td>
<td class="ltx_td ltx_align_left">2.2</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Cortex-A76</td>
<td class="ltx_td ltx_align_left">Y</td>
<td class="ltx_td ltx_align_left">FP32</td>
<td class="ltx_td ltx_align_left">55.4 <math id="S4.T1.m7" class="ltx_Math" alttext="\pm" display="inline"><mo>¬±</mo></math> 10.6</td>
<td class="ltx_td ltx_align_left">210</td>
<td class="ltx_td ltx_align_left">4.2</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_bb"></td>
<td class="ltx_td ltx_align_left ltx_border_bb">Y</td>
<td class="ltx_td ltx_align_left ltx_border_bb">FP16</td>
<td class="ltx_td ltx_align_left ltx_border_bb">65.3 <math id="S4.T1.m8" class="ltx_Math" alttext="\pm" display="inline"><mo>¬±</mo></math> 9.5</td>
<td class="ltx_td ltx_align_left ltx_border_bb">248</td>
<td class="ltx_td ltx_align_left ltx_border_bb">2.2</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p class="ltx_p">Regarding the segmentation network, we train our model applying transfer learning to the selected backbone. Indeed, rather than using randomly initialized weights, we exploit MobileNetV3 variables derived from an initial training phase on the 1k classes and 1.3M
images of the ImageNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="Imagenet: a large-scale hierarchical image database" class="ltx_ref">12</a>]</cite>. Moreover, we pre-trained the overall segmentation network with Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="The cityscapes dataset for semantic urban scene understanding" class="ltx_ref">10</a>]</cite>, a publicly available dataset with 30 different classes and 5000 fine annotated images. Finally, we adopt a strong data augmentation with random crops, brightness, saturation, contrast, rotation, and flips. All together, those techniques largely improve the final robustness of the model and its final generalization capability with a reduced number of training samples.
We train the network with stochastic gradient descent with a learning rate of 0.03 and Intersection over Unit (IoU) as loss function. The accuracy over the test set is 0.8 with a IoU of 0.46. In comparison, the accuracy of the validation set with 0.1 of the synthetic dataset is 0.86 with a domain gap of 0.08. Moreover, our experimentation shows that larger input sizes improve segmentation over the synthetic dataset, but greatly reduces accuracy over real images.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p class="ltx_p">The trained network is optimized in order to reduce latency, inference cost, memory, and storage footprint. That is obtained with two distinct techniques: model
pruning and quantization. The first simplifies the topological structure, removing unnecessary parts of the architecture, and favors a more sparse model introducing
zeros to the parameter tensors. Subsequently, with
quantization, we reduce the precision of the numbers used
to represent model parameters from float32 to float16. That can be accomplished with a post-training quantization procedure.
In Table <a href="#S4.T1" title="TABLE I ‚Ä£ IV-B Networks training and optimization ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> experimentation results with some reference architectures are summarized.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="x8.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="260" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>A visual representation of the simulation environment.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-C </span><span class="ltx_text ltx_font_italic">Platform Hardware and Sensors Setup</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p">As mobile platform, we select the Jackal UGV by Clearpath Robotics, which can be briefly described as a small and weatherproof rover (IP62 code) with a 4x4 high-torque drivetrain. It is highly customizable and ROS-compatible allowing fast deployment and algorithm testing.
All the algorithms run on Jackal‚Äôs onboard Mini-ITX PC with a CPU Intel Core i3-4330TE @2.4GHz and 4GB DDR3 RAM. For what concern the localization sensors, the RTK enabled GNSS receiver is the Piksi Multi by Swift Navigation<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup>
              <span class="ltx_tag ltx_tag_note">10</span>
              
              
            https://www.swiftnav.com/</span></span></span> mounted on an evaluation board, that provides easy input/output communication with the receiver (used acquisition rate: <math id="S4.SS3.p1.m1" class="ltx_Math" alttext="10" display="inline"><mn>10</mn></math> Hz), while the inertial measurements are provided by the MPU-9250 IMU, with an acquisition rate of about <math id="S4.SS3.p1.m2" class="ltx_Math" alttext="100" display="inline"><mn>100</mn></math> Hz. In addition, to get a front view of the environment, we select the Intel Realsense D455 RGBD camera, that provides frames at <math id="S4.SS3.p1.m3" class="ltx_Math" alttext="30" display="inline"><mn>30</mn></math> FPS and is mounted on the front part of Jackal‚Äôs top plate. Finally, the odometry is provided by the on-board quadrature encoders that are able to run at <math id="S4.SS3.p1.m4" class="ltx_Math" alttext="78000" display="inline"><mn>78000</mn></math> pulses/m.
As mentioned in Section <a href="#S2" title="II System Overview ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, the IMU and GNSS receiver data are fused by means of an EKF in order to obtain a global position estimate of the mobile platform time by time, described in terms of <math id="S4.SS3.p1.m5" class="ltx_Math" alttext="x,y,yaw" display="inline"><mrow><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mrow><mi>y</mi><mo>‚Å¢</mo><mi>a</mi><mo>‚Å¢</mo><mi>w</mi></mrow></mrow></math>. However, GNSS positioning is highly inaccurate, about <math id="S4.SS3.p1.m6" class="ltx_Math" alttext="3m-5m" display="inline"><mrow><mrow><mn>3</mn><mo>‚Å¢</mo><mi>m</mi></mrow><mo>‚àí</mo><mrow><mn>5</mn><mo>‚Å¢</mo><mi>m</mi></mrow></mrow></math>, without implementing any corrections technique. As a consequence, we provide RTK corrections to Piksi Multi receiver, coming from the SPIN3 GNSS<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup>
              <span class="ltx_tag ltx_tag_note">11</span>
              
              
            https://www.spingnss.it/spiderweb/frmIndex.aspx</span></span></span> of Piemonte, Lombardia, and Valle d‚ÄôAosta, through the Internet. Then, the GNSS receiver directly uses such corrections to obtain more reliable and accurate global position estimates, with an error range of <math id="S4.SS3.p1.m7" class="ltx_Math" alttext="[0.05,0.10]m" display="inline"><mrow><mrow><mo stretchy="false">[</mo><mn>0.05</mn><mo>,</mo><mn>0.10</mn><mo stretchy="false">]</mo></mrow><mo>‚Å¢</mo><mi>m</mi></mrow></math>, in clear view of the sky and a good antenna position.
We exploit such corrections service because it is entirely free prior to an online subscription and it can send out RTK corrections through the Internet.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-D </span><span class="ltx_text ltx_font_italic">Simulation Environment Evaluation</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p class="ltx_p">All the presented pipeline is tested in a simulation environment prior to real world tests in order to check the basic performances and perform a first experimental setting of various gains and thresholds. First, we build a custom simulation environment made of vine plants organized in rows, and a bumpy and uneven terrain, as shown in Fig. <a href="#S4.F7" title="Figure 7 ‚Ä£ IV-B Networks training and optimization ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, using the Gazebo<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup>
              <span class="ltx_tag ltx_tag_note">12</span>
              
              
            http://gazebosim.org/</span></span></span> simulator, that is ROS-compatible and open-source. Moreover, it provides advanced 3D graphics, dynamics simulation, and several plugins to simulate sensors, as GNSS, IMU, and cameras.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p class="ltx_p">Then, we compare the UGV trajectory obtained with the proposed methodology with a ground truth line, in order to evaluate different error metrics: Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and Standard Deviation (<math id="S4.SS4.p2.m1" class="ltx_Math" alttext="\sigma" display="inline"><mi>œÉ</mi></math>), as shown in Table <a href="#S4.T2" title="TABLE II ‚Ä£ IV-D Simulation Environment Evaluation ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. The ground truth is computed in two steps:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p class="ltx_p">manual annotation of GNSS simulated positions that correspond to an ideal global path: centered in the inter-row space and with a safe distance from crops switching between two different rows.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p class="ltx_p">linear interpolation of such points.</p>
</div>
</li>
</ol>
<p class="ltx_p">We have performed six different tests in two different simulation environments (varying the vine plants positions and maintaining a row-based organization). The obtained performances are promising in terms of MAE, RMSE and <math id="S4.SS4.p2.m2" class="ltx_Math" alttext="\sigma" display="inline"><mi>œÉ</mi></math>, as shown in Table <a href="#S4.T2" title="TABLE II ‚Ä£ IV-D Simulation Environment Evaluation ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. The worse attained results is stated by an RMSE=<math id="S4.SS4.p2.m3" class="ltx_Math" alttext="0.265" display="inline"><mn>0.265</mn></math> m and an MAE=<math id="S4.SS4.p2.m4" class="ltx_Math" alttext="0.217" display="inline"><mn>0.217</mn></math> m in the sixth test, while the best one is achieved in the first test with an RMSE=<math id="S4.SS4.p2.m5" class="ltx_Math" alttext="0.089" display="inline"><mn>0.089</mn></math> m and an MAE=<math id="S4.SS4.p2.m6" class="ltx_Math" alttext="0.068" display="inline"><mn>0.068</mn></math> m. Finally, the mean time to complete a test is about <math id="S4.SS4.p2.m7" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math> minutes with a maximum speed of <math id="S4.SS4.p2.m8" class="ltx_Math" alttext="0.5" display="inline"><mn>0.5</mn></math> m/s. All considered, the overall achieved performances are satisfactory taking into account the worse results are obtained in the last two tests where the vineyard rows are organized with a slight curvature shape.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison of different error metrics in six different tests performed in two different simulation environments. The first row describes the number of visited rows in the corresponding test.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">T1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">T2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">T3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">T4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">T5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">T6</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">N. rows</th>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.m1" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.m2" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.m3" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.m4" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.m5" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.m6" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">Min. Error [m]</th>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m7" class="ltx_Math" alttext="0.001" display="inline"><mn>0.001</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m8" class="ltx_Math" alttext="0.002" display="inline"><mn>0.002</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m9" class="ltx_Math" alttext="0.002" display="inline"><mn>0.002</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m10" class="ltx_Math" alttext="0.001" display="inline"><mn>0.001</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m11" class="ltx_Math" alttext="0.002" display="inline"><mn>0.002</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m12" class="ltx_Math" alttext="0.002" display="inline"><mn>0.002</mn></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">Max. Error [m]</th>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m13" class="ltx_Math" alttext="0.726" display="inline"><mn>0.726</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m14" class="ltx_Math" alttext="0.678" display="inline"><mn>0.678</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m15" class="ltx_Math" alttext="0.600" display="inline"><mn>0.600</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m16" class="ltx_Math" alttext="0.633" display="inline"><mn>0.633</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m17" class="ltx_Math" alttext="1.21" display="inline"><mn>1.21</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m18" class="ltx_Math" alttext="1.21" display="inline"><mn>1.21</mn></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">MAE [m]</th>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m19" class="ltx_Math" alttext="0.068" display="inline"><mn>0.068</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m20" class="ltx_Math" alttext="0.085" display="inline"><mn>0.085</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m21" class="ltx_Math" alttext="0.077" display="inline"><mn>0.077</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m22" class="ltx_Math" alttext="0.082" display="inline"><mn>0.082</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m23" class="ltx_Math" alttext="0.215" display="inline"><mn>0.215</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m24" class="ltx_Math" alttext="0.217" display="inline"><mn>0.217</mn></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">RMSE [m]</th>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m25" class="ltx_Math" alttext="0.089" display="inline"><mn>0.089</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m26" class="ltx_Math" alttext="0.100" display="inline"><mn>0.100</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m27" class="ltx_Math" alttext="0.092" display="inline"><mn>0.092</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m28" class="ltx_Math" alttext="0.096" display="inline"><mn>0.096</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m29" class="ltx_Math" alttext="0.263" display="inline"><mn>0.263</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T2.m30" class="ltx_Math" alttext="0.265" display="inline"><mn>0.265</mn></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">
<math id="S4.T2.m31" class="ltx_Math" alttext="\sigma" display="inline"><mi>œÉ</mi></math> [m]</th>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T2.m32" class="ltx_Math" alttext="0.057" display="inline"><mn>0.057</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T2.m33" class="ltx_Math" alttext="0.053" display="inline"><mn>0.053</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T2.m34" class="ltx_Math" alttext="0.050" display="inline"><mn>0.050</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T2.m35" class="ltx_Math" alttext="0.050" display="inline"><mn>0.050</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T2.m36" class="ltx_Math" alttext="0.152" display="inline"><mn>0.152</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T2.m37" class="ltx_Math" alttext="0.152" display="inline"><mn>0.152</mn></math></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-E </span><span class="ltx_text ltx_font_italic">Real Environment Evaluation</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p class="ltx_p">The overall system is extensively tested in two real environment scenarios with multiple experiments in different seasonal periods (Fig. <a href="#S1.F1" title="Figure 1 ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>): a vineyard and a pear orchard, shown in Fig. <a href="#S4.F8" title="Figure 8 ‚Ä£ IV-E Real Environment Evaluation ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, for a total of more than 80 hours of experimentation from 9 a.m. to 6 p.m., but without particular adverse weather conditions (e.g., rain, snow, fog). Moreover, all the tests have been performed with the same hardware and software setup to obtain consistent data. The vineyard is located in Grugliasco and managed by the Department of Agricultural, Forestry and Food Sciences of Universit√† degli studi di Torino (UNITO). In contrast, the pear orchard is located in Montegrosso d‚ÄôAsti and managed by Mura Mura farm. The first scenario has an inter-row space of about <math id="S4.SS5.p1.m1" class="ltx_Math" alttext="2.80m" display="inline"><mrow><mn>2.80</mn><mo>‚Å¢</mo><mi>m</mi></mrow></math> and a height of about <math id="S4.SS5.p1.m2" class="ltx_Math" alttext="2.0m" display="inline"><mrow><mn>2.0</mn><mo>‚Å¢</mo><mi>m</mi></mrow></math>, while the second is organized in rows with an inter-row space of <math id="S4.SS5.p1.m3" class="ltx_Math" alttext="4.50m" display="inline"><mrow><mn>4.50</mn><mo>‚Å¢</mo><mi>m</mi></mrow></math> and a height of about <math id="S4.SS5.p1.m4" class="ltx_Math" alttext="3.0m" display="inline"><mrow><mn>3.0</mn><mo>‚Å¢</mo><mi>m</mi></mrow></math>.</p>
</div>
<figure id="S4.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="x9.jpg" id="S4.F8.sf1.g1" class="ltx_graphics ltx_img_square" width="165" height="145" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Vineyard Row</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="x10.jpg" id="S4.F8.sf2.g1" class="ltx_graphics ltx_img_square" width="165" height="145" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Pear Orchard Row</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>A visual representation of the real world testing environments.</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Comparison of different error metrics in three different tests performed in the pear orchard. The second column describes the number of visited rows in the corresponding test.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row">Test</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:22.8pt;">
<span class="ltx_p">N. rows</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:22.8pt;">
<span class="ltx_p">Min. Error [m]</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:22.8pt;">
<span class="ltx_p">Max. Error [m]</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:22.8pt;">
<span class="ltx_p">MAE [m]</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:22.8pt;">
<span class="ltx_p">RMSE [m]</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:21.3pt;">
<span class="ltx_p"><math id="S4.T3.m1" class="ltx_Math" alttext="\sigma" display="inline"><mi>œÉ</mi></math> [m]</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Test n. 1</th>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.m2" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.m3" class="ltx_Math" alttext="0.008" display="inline"><mn>0.008</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.m4" class="ltx_Math" alttext="1.395" display="inline"><mn>1.395</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.m5" class="ltx_Math" alttext="0.523" display="inline"><mn>0.523</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.m6" class="ltx_Math" alttext="0.627" display="inline"><mn>0.627</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.m7" class="ltx_Math" alttext="0.351" display="inline"><mn>0.351</mn></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">Test n. 2</th>
<td class="ltx_td ltx_align_center"><math id="S4.T3.m8" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T3.m9" class="ltx_Math" alttext="0.002" display="inline"><mn>0.002</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T3.m10" class="ltx_Math" alttext="1.105" display="inline"><mn>1.105</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T3.m11" class="ltx_Math" alttext="0.457" display="inline"><mn>0.457</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T3.m12" class="ltx_Math" alttext="0.551" display="inline"><mn>0.551</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T3.m13" class="ltx_Math" alttext="0.315" display="inline"><mn>0.315</mn></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">Test n. 3</th>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T3.m14" class="ltx_Math" alttext="2" display="inline"><mn>2</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T3.m15" class="ltx_Math" alttext="0.007" display="inline"><mn>0.007</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T3.m16" class="ltx_Math" alttext="1.320" display="inline"><mn>1.320</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T3.m17" class="ltx_Math" alttext="0.659" display="inline"><mn>0.659</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T3.m18" class="ltx_Math" alttext="0.755" display="inline"><mn>0.755</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T3.m19" class="ltx_Math" alttext="0.375" display="inline"><mn>0.375</mn></math></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Comparison of different error metrics in three different tests performed in the vineyard. The second column describes the number of visited rows in the corresponding test.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row">Test</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:22.8pt;">
<span class="ltx_p">N. rows</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:22.8pt;">
<span class="ltx_p">Min. Error [m]</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:22.8pt;">
<span class="ltx_p">Max. Error [m]</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:22.8pt;">
<span class="ltx_p">MAE [m]</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:22.8pt;">
<span class="ltx_p">RMSE [m]</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:21.3pt;">
<span class="ltx_p"><math id="S4.T4.m1" class="ltx_Math" alttext="\sigma" display="inline"><mi>œÉ</mi></math> [m]</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Test n. 1</th>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.m2" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.m3" class="ltx_Math" alttext="0.013" display="inline"><mn>0.013</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.m4" class="ltx_Math" alttext="0.621" display="inline"><mn>0.621</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.m5" class="ltx_Math" alttext="0.296" display="inline"><mn>0.296</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.m6" class="ltx_Math" alttext="0.332" display="inline"><mn>0.332</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.m7" class="ltx_Math" alttext="0.160" display="inline"><mn>0.160</mn></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">Test n. 2</th>
<td class="ltx_td ltx_align_center"><math id="S4.T4.m8" class="ltx_Math" alttext="6" display="inline"><mn>6</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T4.m9" class="ltx_Math" alttext="0.006" display="inline"><mn>0.006</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T4.m10" class="ltx_Math" alttext="0.598" display="inline"><mn>0.598</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T4.m11" class="ltx_Math" alttext="0.218" display="inline"><mn>0.218</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T4.m12" class="ltx_Math" alttext="0.240" display="inline"><mn>0.240</mn></math></td>
<td class="ltx_td ltx_align_center"><math id="S4.T4.m13" class="ltx_Math" alttext="0.119" display="inline"><mn>0.119</mn></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">Test n. 3</th>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T4.m14" class="ltx_Math" alttext="6" display="inline"><mn>6</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T4.m15" class="ltx_Math" alttext="0.003" display="inline"><mn>0.003</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T4.m16" class="ltx_Math" alttext="0.720" display="inline"><mn>0.720</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T4.m17" class="ltx_Math" alttext="0.204" display="inline"><mn>0.204</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T4.m18" class="ltx_Math" alttext="0.246" display="inline"><mn>0.246</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T4.m19" class="ltx_Math" alttext="0.145" display="inline"><mn>0.145</mn></math></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p class="ltx_p">The errors, described in Table <a href="#S4.T3" title="TABLE III ‚Ä£ IV-E Real Environment Evaluation ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> and Table <a href="#S4.T4" title="TABLE IV ‚Ä£ IV-E Real Environment Evaluation ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, are computed comparing the RTK-GNSS positions provided by the Piksi Multi receiver and the global path provided to the navigation system. All of this is possible due to the high accuracy GNSS estimated positions, thanks to a clear view of the sky and a good position of the high-end antenna on the UGV. All the collected data are represented in latitude and longitude coordinates. However, for analysis purposes, they have been transformed in meters with respect to a known GNSS coordinate of the georeferenced occupancy grid map: the top left corner pixel.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p class="ltx_p">Table <a href="#S4.T3" title="TABLE III ‚Ä£ IV-E Real Environment Evaluation ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> and Table <a href="#S4.T4" title="TABLE IV ‚Ä£ IV-E Real Environment Evaluation ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, together with the visual representation of Fig. <a href="#S4.F9" title="Figure 9 ‚Ä£ IV-E Real Environment Evaluation ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, shows some numerical results obtained by the proposed novel approach, and demonstrated that a methodology that exploits semantic segmentation along with a standard navigation approach based on the GNSS is able to provide complete and reliable navigation throughout the whole row-based crop. The results obtained in the pear orchard (Table <a href="#S4.T3" title="TABLE III ‚Ä£ IV-E Real Environment Evaluation ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>) are slightly worse than the vineyard ones; however, this effect may be due to the greater inter-row space of the pear orchard with respect to the vineyard one. Eventually, the mean time of a single test is about <math id="S4.SS5.p3.m1" class="ltx_Math" alttext="25" display="inline"><mn>25</mn></math> minutes, while the maximum velocity of the UGV is <math id="S4.SS5.p3.m2" class="ltx_Math" alttext="0.5" display="inline"><mn>0.5</mn></math> m/s. All considered, the overall achieved performances, in terms of MAE and RMSE, are adequate to the used low-cost sensors setup and demonstrate the effectiveness of our approach.</p>
</div>
<figure id="S4.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F9.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="x11.png" id="S4.F9.sf1.g1" class="ltx_graphics ltx_img_square" width="279" height="240" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Test n. 3 in the vineyard scenario</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F9.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="x12.png" id="S4.F9.sf2.g1" class="ltx_graphics ltx_img_square" width="279" height="240" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Test n. 2 in the orchard scenario</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>A visual representation of the obtained results. Both images contain the path followed by the UGV (red line), the provided global path (cyan x), the start/end row waypoints (blue dots) and the crop (green dots).</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion and Future Work</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">We presented a novel affordable algorithmic pipeline for autonomous navigation in row-based crops. Our methodology is a complete solution that covers all navigation stages, from global to local path planning, only relying on low-cost, low-range sensors. Moreover, the system efficiently tackles GNSS unreliability in presence of lush vegetation and thick canopies, allowing the platform to autonomously navigate in all seasonal periods. Finally, the adopted domain generalization and optimization techniques greatly make training and inference of deep neural network less time and computational costly. Further works will aim at assessing our proposed algorithmic pipeline onto a more cumbersome vehicle and introducing a collision avoidance algorithm in the segmentation-based control.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">This work has been developed with the contribution of the Politecnico di Torino Interdepartmental Centre for Service Robotics (PIC4SeR<span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup>
            <span class="ltx_tag ltx_tag_note">13</span>
            
            
          <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://pic4ser.polito.it/</span></span></span></span>) and SmartData@Polito<span id="footnote14" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup>
            <span class="ltx_tag ltx_tag_note">14</span>
            
            
          <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://smartdata.polito.it/</span></span></span></span>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="bib.L1" class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Aghi, S. Cerrato, V. Mazzia, and M. Chiaberge</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep semantic segmentation at the edge for autonomous navigation in vineyard rows</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†3421‚Äì3428</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.I1.i2.p1" title="In I-B Novelties ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2nd item</span></a>,
<a href="#S1.SS1.p3" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>,
<a href="#S3.SS4.p1" title="III-D Segmentation-based control ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIII-D</span></a>,
<a href="#S3.SS4.p3" title="III-D Segmentation-based control ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIII-D</span></a>,
<a href="#S3.SS4.p4" title="III-D Segmentation-based control ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIII-D</span></a>.
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Aghi, V. Mazzia, and M. Chiaberge</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Autonomous navigation in vineyards with deep learning at the edge</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Robotics in Alpe-Adria Danube Region</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†479‚Äì486</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p3" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>.
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Aghi, V. Mazzia, and M. Chiaberge</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Local motion planner for autonomous navigation in vineyards with a rgb-d camera-based algorithm and deep learning synergy</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Machines</span> <span class="ltx_text ltx_bib_volume">8</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†27</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS4.p4" title="III-D Segmentation-based control ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIII-D</span></a>.
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Anthony and C. Detweiler</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">UAV localization in row crops</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">34</span> (<span class="ltx_text ltx_bib_number">7</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†1275‚Äì1296</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p2" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>.
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Bonadies and S. A. Gadsden</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An overview of autonomous crop row navigation strategies for unmanned ground vehicles</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Engineering in Agriculture, Environment and Food</span> <span class="ltx_text ltx_bib_volume">12</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†24‚Äì31</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 1881-8366</span>,
<a href="https://dx.doi.org/https%3A//doi.org/10.1016/j.eaef.2018.09.001" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="https://www.sciencedirect.com/science/article/pii/S188183661730188X" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p1" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>.
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Boschi, F. Salvetti, V. Mazzia, and M. Chiaberge</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A cost-effective person-following system for assistive unmanned vehicles with deep learning at the edge</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">8</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†49</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Cerrato, D. Aghi, V. Mazzia, F. Salvetti, and M. Chiaberge</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An adaptive row crops path generator with deep learning synergy</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2021 6th Asia-Pacific Conference on Intelligent Robot Systems (ACIRS)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†6‚Äì12</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="II System Overview ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßII</span></a>,
<a href="#S3.SS2.p3" title="III-B Global Path Planning ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIII-B</span></a>.
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Chen, G. Papandreou, F. Schroff, and H. Adam</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Rethinking atrous convolution for semantic image segmentation</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p2" title="III-C Segmentation Network ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIII-C</span></a>.
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Comba, P. Gay, J. Primicerio, and D. R. Aimonino</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Vineyard detection from unmanned aerial systems images</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">114</span>, <span class="ltx_text ltx_bib_pages"> pp.¬†78‚Äì87</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele</span><span class="ltx_text ltx_bib_year"> (2016)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The cityscapes dataset for semantic urban scene understanding</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†3213‚Äì3223</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p3" title="IV-B Networks training and optimization ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIV-B</span></a>.
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem ltx_bib_report">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. C. Coulter</span><span class="ltx_text ltx_bib_year"> (1992)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Implementation of the pure pursuit path tracking algorithm</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Technical report</span>
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Carnegie-Mellon UNIV Pittsburgh PA Robotics INST</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p3" title="II System Overview ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßII</span></a>.
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei</span><span class="ltx_text ltx_bib_year"> (2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Imagenet: a large-scale hierarchical image database</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2009 IEEE conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†248‚Äì255</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p3" title="IV-B Networks training and optimization ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIV-B</span></a>.
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Deshmukh, D. K. Pratihar, A. K. Deb, H. Ray, and N. Bhattacharyya</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Design and development of intelligent pesticide spraying system for agricultural robot</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Hybrid Intelligent Systems</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†157‚Äì170</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Duckett, S. Pearson, S. Blackmore, B. Grieve, W. Chen, G. Cielniak, J. Cleaversmith, J. Dai, S. Davis, C. Fox, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Agricultural robotics: the future of robotic agriculture</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Ester, H. Kriegel, J. Sander, X. Xu, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (1996)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A density-based algorithm for discovering clusters in large spatial databases with noise.</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Kdd</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">96</span>, <span class="ltx_text ltx_bib_pages"> pp.¬†226‚Äì231</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.I1.i2.p1" title="In III-B Global Path Planning ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">item¬†2</span></a>.
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Feng, J. Zhou, E. D. Vories, K. A. Sudduth, and M. Zhang</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Yield estimation in cotton using uav-based multi-sensor imagery</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">193</span>, <span class="ltx_text ltx_bib_pages"> pp.¬†101‚Äì114</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Fox, W. Burgard, and S. Thrun</span><span class="ltx_text ltx_bib_year"> (1997)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The dynamic window approach to collision avoidance</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Robotics Automation Magazine</span> <span class="ltx_text ltx_bib_volume">4</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†23‚Äì33</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://dx.doi.org/10.1109/100.580977" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.I2.i2.p1" title="In III-E Local Path Planning Policy ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">item¬†2</span></a>.
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Garg, N. S√ºnderhauf, F. Dayoub, D. Morrison, A. Cosgun, G. Carneiro, Q. Wu, T. Chin, I. Reid, S. Gould, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Semantics for robotic mapping, perception and interaction: a survey</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Grigorescu, B. Trasnea, T. Cocias, and G. Macesanu</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A survey of deep learning techniques for autonomous driving</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">37</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†362‚Äì386</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Howard, M. Sandler, G. Chu, L. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Searching for mobilenetv3</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†1314‚Äì1324</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p2" title="III-C Segmentation Network ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIII-C</span></a>.
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Hu, F. Perazzi, F. C. Heilbron, O. Wang, Z. Lin, K. Saenko, and S. Sclaroff</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Real-time semantic segmentation with fast attention</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">6</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†263‚Äì270</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p2" title="III-C Segmentation Network ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIII-C</span></a>.
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Quantization and training of neural networks for efficient integer-arithmetic-only inference</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†2704‚Äì2713</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. S. N. Kabir, M. Song, N. Sung, S. Chung, Y. Kim, N. Noguchi, and S. Hong</span><span class="ltx_text ltx_bib_year"> (2016)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Performance comparison of single and multi-gnss receivers under agricultural fields in korea</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">9</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†27‚Äì35</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p1" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>,
<a href="#S1.p5" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Kamilaris and F. X. Prenafeta-Bold√∫</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep learning in agriculture: a survey</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">147</span>, <span class="ltx_text ltx_bib_pages"> pp.¬†70‚Äì90</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p3" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>.
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Khaliq, V. Mazzia, and M. Chiaberge</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Refining satellite imagery by using uav imagery for vineyard environment: a cnn based approach</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2019 IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†25‚Äì29</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Kim, B. Coltin, and H. J. Kim</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Low-drift visual odometry in structured environments by decoupling rotational and translational motion</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2018 IEEE international conference on Robotics and automation (ICRA)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†7247‚Äì7253</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p2" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>.
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. LeCun, Y. Bengio, and G. Hinton</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">521</span> (<span class="ltx_text ltx_bib_number">7553</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†436‚Äì444</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://dx.doi.org/10.1038/nature14539" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<span class="ltx_text isbn ltx_bib_external">ISBN 1476-4687</span>,
<a href="https://doi.org/10.1038/nature14539" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p3" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>,
<a href="#S1.p3" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. J. LeVoir, P. A. Farley, T. Sun, and C. Xu</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">High-accuracy adaptive low-cost location sensing subsystems for autonomous rover in precision agriculture</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">1</span>, <span class="ltx_text ltx_bib_pages"> pp.¬†74‚Äì94</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">O. Ly, H. Gimbert, G. Passault, and G. Baron</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A fully autonomous robot for putting posts for trellising vineyard with centimetric accuracy</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2015 IEEE International Conference on Autonomous Robot Systems and Competitions</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†44‚Äì49</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p1" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>.
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Ma, W. Zhang, W. S. Qureshi, C. Gao, C. Zhang, and W. Li</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Autonomous navigation for a wolfberry picking robot using visual cues and fuzzy control</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Information Processing in AgricultureIEEE Robotics and Automation LettersComputer vision and image understandingarXiv preprint arXiv:1908.08681Computers and Electronics in AgricultureService robotsAustralasian Plant PathologyBiosystems EngineeringComputers and Electronics in AgricultureComputer Networkscomputers and Electronics in AgricultureIEEE Open Journal of Industry ApplicationsPrecision AgriculturearXiv preprint arXiv:1806.06762NatureJournal of Field RoboticsIEEE Sensors JournalIEEE Transactions on Cognitive Communications and NetworkingMachinesarXiv preprint arXiv:2101.00443Proceedings of the IEEEIEEE AccessEngineering in agriculture, environment and foodarXiv preprint arXiv:2107.02792IFAC-PapersOnLineJournal of Field RoboticsComputers and Electronics in AgricultureComputers and electronics in agricultureJournal of Field RoboticsIEEE Robotics and Automation LettersarXiv preprint arXiv:2004.02147arXiv preprint arXiv:1706.05587</span> <span class="ltx_text ltx_bib_volume">8</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†15‚Äì26</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 2214-3173</span>,
<a href="https://dx.doi.org/https%3A//doi.org/10.1016/j.inpa.2020.04.005" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="https://www.sciencedirect.com/science/article/pii/S2214317319303269" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p2" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>.
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Marden and M. Whitty</span><span class="ltx_text ltx_bib_year"> (2014)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">GPS-free localisation and navigation of an unmanned ground vehicle for yield forecasting in a vineyard</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Recent Advances in Agricultural Robotics, International workshop collocated with the 13th International Conference on Intelligent Autonomous Systems (IAS-13)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Martini, S. Cerrato, F. Salvetti, S. Angarano, and M. Chiaberge</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Position-agnostic autonomous navigation in vineyards with deep reinforcement learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†477‚Äì484</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Matas, C. Galambos, and J. Kittler</span><span class="ltx_text ltx_bib_year"> (2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Robust detection of lines using the progressive probabilistic hough transform</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">78</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†119‚Äì137</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.I1.i1.p1" title="In III-B Global Path Planning ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">item¬†1</span></a>.
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">V. Mazzia, A. Khaliq, F. Salvetti, and M. Chiaberge</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Real-time apple detection system using embedded systems with hardware accelerators: an edge ai application</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">8</span>, <span class="ltx_text ltx_bib_pages"> pp.¬†9102‚Äì9114</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">V. Mazzia, F. Salvetti, D. Aghi, and M. Chiaberge</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">DeepWay: a deep learning waypoint estimator for global path generation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">184</span>, <span class="ltx_text ltx_bib_pages"> pp.¬†106091</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p3" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>,
<a href="#S1.p1" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>,
<a href="#S1.p2" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>,
<a href="#S2.p2" title="II System Overview ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßII</span></a>,
<a href="#S3.F4" title="In III-A Waypoints Estimation ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>,
<a href="#S3.SS1.p1" title="III-A Waypoints Estimation ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIII-A</span></a>,
<a href="#S3.SS2.p1" title="III-B Global Path Planning ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIII-B</span></a>,
<a href="#S4.SS2.p2" title="IV-B Networks training and optimization ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIV-B</span></a>.
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Misra</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mish: a self regularized non-monotonic neural activation function</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">4</span>, <span class="ltx_text ltx_bib_pages"> pp.¬†2</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p7" title="III-A Waypoints Estimation ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIII-A</span></a>.
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. J. Moorehead, C. K. Wellington, B. J. Gilmore, and C. Vallespi</span><span class="ltx_text ltx_bib_year"> (2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automating orchards: a system of autonomous tractors for orchard maintenance</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE international conference of intelligent robots and systems, workshop on agricultural robotics</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p1" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>.
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Mota, M. Sridharan, and A. Leonardis</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Commonsense reasoning and deep learning for transparent decision making in robotics</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">European conference on multiagent systems</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">I. Nevliudov, S. Novoselov, O. Sychova, and S. Tesliuk</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Development of the architecture of the base platform agricultural robot for determining the trajectory using the method of visual odometry</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2021 IEEE XVIIth International Conference on the Perspective Technologies and Methods in MEMS Design (MEMSTECH)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†64‚Äì68</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p2" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>.
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Radoglou-Grammatikis, P. Sarigiannidis, T. Lagkas, and I. Moscholios</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A compilation of uav applications for precision agriculture</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">172</span>, <span class="ltx_text ltx_bib_pages"> pp.¬†107148</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. J. Rold√°n, J. del Cerro, D. Garz√≥n-Ramos, P. Garcia-Aunon, M. Garz√≥n, J. de Le√≥n, and A. Barrientos</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Robots in agriculture: state of art and practical experiences</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†67‚Äì90</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Saeedi, B. Bodin, H. Wagstaff, A. Nisbet, L. Nardi, J. Mawer, N. Melot, O. Palomar, E. Vespa, T. Spink, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Navigating the landscape for real-time localization and mapping for robotics and virtual and augmented reality</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">106</span> (<span class="ltx_text ltx_bib_number">11</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†2020‚Äì2039</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. Salvetti, S. Angarano, M. Martini, S. Cerrato, and M. Chiaberge</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Waypoint generation in row-based crops with deep learning and contrastive clustering</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†203‚Äì218</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>,
<a href="#S1.p2" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Sparrow and M. Howard</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Robots in agriculture: prospects, impacts, ethics, and policy</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">22</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†818‚Äì833</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Tejaswi Digumarti, L. M. Schmid, G. M. Rizzi, J. Nieto, R. Siegwart, P. Beardsley, and C. Cadena</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An approach for semantic segmentation of tree-like vegetation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2019 International Conference on Robotics and Automation (ICRA)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume"></span>, <span class="ltx_text ltx_bib_pages"> pp.¬†1801‚Äì1807</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://dx.doi.org/10.1109/ICRA.2019.8793576" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p2" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>,
<a href="#S4.SS1.p5" title="IV-A Datasets creation ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIV-A</span></a>.
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Domain randomization for transferring deep neural networks from simulation to the real world</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†23‚Äì30</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>,
<a href="#S4.SS1.p1" title="IV-A Datasets creation ‚Ä£ IV Experimental Results and Discussions ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIV-A</span></a>.
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. Winterhalter, F. Fleckenstein, C. Dornhege, and W. Burgard</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Localization for precision navigation in agricultural fields‚Äîbeyond crop row following</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">38</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†429‚Äì451</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p1" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>.
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Woo, J. Park, J. Lee, and I. S. Kweon</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cbam: convolutional block attention module</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the European conference on computer vision (ECCV)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†3‚Äì19</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p7" title="III-A Waypoints Estimation ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIII-A</span></a>.
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Yu, C. Gao, J. Wang, G. Yu, C. Shen, and N. Sang</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bisenet v2: bilateral network with guided aggregation for real-time semantic segmentation</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p2" title="III-C Segmentation Network ‚Ä£ III Methodology ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßIII-C</span></a>.
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Zaman, L. Comba, A. Biglia, D. R. Aimonino, P. Barge, and P. Gay</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cost-effective visual odometry system for vehicle motion control in agricultural environments</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">162</span>, <span class="ltx_text ltx_bib_pages"> pp.¬†82‚Äì94</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p2" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>.
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Zhang, T. Xu, Y. Tian, H. Xu, J. Song, and Y. Lan</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Assessment of rice leaf blast severity using hyperspectral imaging during late vegetative growth</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">49</span> (<span class="ltx_text ltx_bib_number">5</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†571‚Äì578</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI</span></a>.
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Zoto, M. A. Musci, A. Khaliq, M. Chiaberge, and I. Aicardi</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic path planning for unmanned ground vehicle using uav imagery</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Robotics in Alpe-Adria Danube Region</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†223‚Äì230</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.SS1.p3" title="I-A Related Work ‚Ä£ I Introduction ‚Ä£ A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops" class="ltx_ref"><span class="ltx_text ltx_ref_tag">¬ßI-A</span></a>.
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Dec  5 01:36:34 2024 by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>
</body>
</html>
