<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models</title>
<!--Generated on Thu Dec  5 00:19:52 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Machine Learning,  ICML">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elman Mansimov
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alex Wang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sean Welleck
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kyunghyun Cho
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
    
<p class="ltx_p">Undirected neural sequence models such as BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib41" title="BERT: pre-training of deep bidirectional transformers for language understanding" class="ltx_ref">2019</a>)</cite> have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference.
The problem of generating sequences directly from these models has received relatively little attention, in part because generating from undirected models departs significantly from conventional monotonic generation in directed sequence models.
We investigate this problem by proposing a generalized model of sequence generation that unifies decoding in directed and undirected models.
The proposed framework models the process of generation rather than the resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models.
This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected sequence models.
We demonstrate this by evaluating various handcrafted and learned decoding strategies on a BERT-like machine translation model <cite class="ltx_cite ltx_citemacro_citep">(Lample and Conneau, <a href="#bib.bib15" title="Cross-lingual language model pretraining" class="ltx_ref">2019</a>)</cite>.
The proposed approach achieves constant-time translation results on par with linear-time translation results from the same undirected sequence model, while both are competitive with the state-of-the-art on WMT’14 English-German translation.</p>
  
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div id="p2" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Undirected neural sequence models such as BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib41" title="BERT: pre-training of deep bidirectional transformers for language understanding" class="ltx_ref">2019</a>)</cite> have recently brought significant improvements to a variety of discriminative language modeling tasks such as question-answering and natural language inference. Generating sequences from these models has received relatively little attention. Unlike directed sequence models, each word typically depends on the full left and right context around it in undirected sequence models. Thus, a decoding algorithm for an undirected sequence model must specify both how to select positions and what symbols to place in the selected positions.
We formalize this process of selecting positions and replacing symbols as a general framework of sequence generation,
and unify decoding from both directed and undirected sequence models under this framework. This framing enables us to study generation on its own, independent from the specific parameterization of the sequence models.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Our proposed framework casts sequence generation as a process of determining the length of the sequence, and then repeatedly alternating between selecting sequence positions followed by generation of symbols for those positions. A variety of sequence models can be derived under this framework by appropriately designing the length distribution, position selection distribution, and symbol replacement distribution. Specifically, we derive popular decoding algorithms such as monotonic autoregressive, non-autoregressive by iterative refinement, and monotonic semi-autoregressive decoding as special cases of the proposed model.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">This separation of coordinate selection and symbol replacement allows us to build a diverse set of decoding algorithms agnostic to the parameterization or training procedure of the underlying model.
We thus fix the symbol replacement distribution as a variant of BERT and focus on deriving novel generation procedures for undirected neural sequence models under the proposed framework.
We design a coordinate selection distribution using a log-linear model and a learned model with a reinforcement learning objective to demonstrate that our model generalizes various fixed-order generation strategies, while also being capable of adapting generation order based on the content of intermediate sequences.
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">We empirically validate our proposal on machine translation using a translation-variant of BERT called a masked translation model <cite class="ltx_cite ltx_citemacro_citep">(Lample and Conneau, <a href="#bib.bib15" title="Cross-lingual language model pretraining" class="ltx_ref">2019</a>)</cite>. We design several generation strategies based on features of intermediate sequence distributions
and compare them against the state-of-the-art monotonic autoregressive sequence model <cite class="ltx_cite ltx_citemacro_citep">(Vaswani<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib43" title="Attention is all you need" class="ltx_ref">2017</a>)</cite> on WMT’14 English-German. Our experiments show that generation from undirected sequence models, under our framework, is competitive with the state of the art, and that adaptive-order generation strategies generate sequences in different ways, including left-to-right, right-to-left and mixtures of these.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">Due to the flexibility in specifying a coordinate selection mechanism, we design constant-time variants of the proposed generation strategies, closely following the experimental setup of <cite class="ltx_cite ltx_citemacro_citet">Ghazvininejad<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib13" title="Mask-predict: parallel decoding of conditional masked language models" class="ltx_ref">2019</a>)</cite>. Our experiments reveal that we can do constant-time translation with the budget as low as 20 iterations (equivalently, generating a sentence of length 20 in the conventional approach) while achieving similar performance to the state-of-the-art-monotonic autoregressive sequence model and linear-time translation from the same masked translation model. This again confirms the potential of the proposed framework and generation strategies. We release the implementation, preprocessed datasets as well as trained models online at <a href="https://github.com/nyu-dl/dl4mt-seqgen" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/nyu-dl/dl4mt-seqgen</a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>A Generalized Framework of Sequence Generation</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">We propose a generalized framework of probabilistic sequence generation to unify generation from directed and undirected neural sequence models. In this generalized framework, we have a <span class="ltx_text ltx_font_italic">generation sequence</span> <math id="S2.p1.m1" class="ltx_Math" alttext="G" display="inline"><mi>G</mi></math> of pairs of an <span class="ltx_text ltx_font_italic">intermediate sequence</span> <math id="S2.p1.m2" class="ltx_Math" alttext="Y^{t}=(y^{t}_{1},\ldots,y^{t}_{L})" display="inline"><mrow><msup><mi>Y</mi><mi>t</mi></msup><mo>=</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>y</mi><mn>1</mn><mi>t</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>y</mi><mi>L</mi><mi>t</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></math> and the corresponding <span class="ltx_text ltx_font_italic">coordinate sequence</span> <math id="S2.p1.m3" class="ltx_Math" alttext="Z^{t}=(z^{t}_{1},\ldots,z^{t}_{L})" display="inline"><mrow><msup><mi>Z</mi><mi>t</mi></msup><mo>=</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>z</mi><mn>1</mn><mi>t</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>z</mi><mi>L</mi><mi>t</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></math>, where <math id="S2.p1.m4" class="ltx_Math" alttext="V" display="inline"><mi>V</mi></math> is a vocabulary, <math id="S2.p1.m5" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> is a length of a sequence, <math id="S2.p1.m6" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> is a number of generation steps, <math id="S2.p1.m7" class="ltx_Math" alttext="y^{t}_{i}\in V" display="inline"><mrow><msubsup><mi>y</mi><mi>i</mi><mi>t</mi></msubsup><mo>∈</mo><mi>V</mi></mrow></math>, and <math id="S2.p1.m8" class="ltx_Math" alttext="z^{t}_{i}\in\left\{0,1\right\}" display="inline"><mrow><msubsup><mi>z</mi><mi>i</mi><mi>t</mi></msubsup><mo>∈</mo><mrow><mo>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>}</mo></mrow></mrow></math>. The coordinate sequence indicates which of the current intermediate sequence are to be replaced. That is, consecutive pairs are related to each other by
<math id="S2.p1.m9" class="ltx_Math" alttext="y^{t+1}_{i}=(1-z^{t+1}_{i})y^{t}_{i}+z^{t+1}_{i}\tilde{y}^{t+1}_{i}" display="inline"><mrow><msubsup><mi>y</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msubsup><mi>z</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msubsup><mi>y</mi><mi>i</mi><mi>t</mi></msubsup></mrow><mo>+</mo><mrow><msubsup><mi>z</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msubsup><mover accent="true"><mi>y</mi><mo>~</mo></mover><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></mrow></mrow></math>,
where <math id="S2.p1.m10" class="ltx_Math" alttext="\tilde{y}^{t+1}_{i}\in V" display="inline"><mrow><msubsup><mover accent="true"><mi>y</mi><mo>~</mo></mover><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>∈</mo><mi>V</mi></mrow></math> is a new symbol for the position <math id="S2.p1.m11" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>. This sequence of pairs <math id="S2.p1.m12" class="ltx_Math" alttext="G" display="inline"><mi>G</mi></math> describes a procedure that starts from an empty sequence <math id="S2.p1.m13" class="ltx_Math" alttext="Y^{1}=(\left&lt;\text{mask}\right&gt;,\ldots,\left&lt;\text{mask}\right&gt;)" display="inline"><mrow><msup><mi>Y</mi><mn>1</mn></msup><mo>=</mo><mrow><mo stretchy="false">(</mo><mrow><mo>⟨</mo><mtext>mask</mtext><mo>⟩</mo></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mo>⟨</mo><mtext>mask</mtext><mo>⟩</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></math> and empty coordinate sequence <math id="S2.p1.m14" class="ltx_Math" alttext="Z^{1}=(0,...,0)" display="inline"><mrow><msup><mi>Z</mi><mn>1</mn></msup><mo>=</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow></math>, iteratively fills in tokens, and terminates after <math id="S2.p1.m15" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> steps with final sequence <math id="S2.p1.m16" class="ltx_Math" alttext="Y^{T}" display="inline"><msup><mi>Y</mi><mi>T</mi></msup></math>.
We model this procedure probabilistically as <math id="S2.p1.m17" class="ltx_Math" alttext="p(G|X)" display="inline"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>G</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math>:

<span class="ltx_inline-block ltx_transformed_outer" style="width:411.9pt;height:18.6pt;vertical-align:-6.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.5pt,0.4pt) scale(0.94274982205199,0.94274982205199) ;">
<span class="ltx_block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<span id="A4.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<span id="S2.E1"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_right ltx_eqn_cell"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E1.m2" class="ltx_Math" alttext="\displaystyle\underbrace{p(L|X)}_{\text{(c) length predict}}\prod_{t=1}^{T}%
\prod_{i=1}^{L}\underbrace{p(z^{t+1}_{i}|Y^{\leq t},Z^{t},X)}_{\text{(a) %
coordinate selection}}{\underbrace{p(y^{t+1}_{i}|Y^{\leq t},X)}_{\text{(b) %
symbol replacement}}}^{z^{t+1}_{i}}" display="inline"><mrow><munder><munder accentunder="true"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>L</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>⏟</mo></munder><mtext>(c) length predict</mtext></munder><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover></mstyle><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover></mstyle><mrow><munder><munder accentunder="true"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>z</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><msup><mi>Z</mi><mi>t</mi></msup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>⏟</mo></munder><mtext>(a) coordinate selection</mtext></munder><mo>⁢</mo><msup><munder><munder accentunder="true"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>y</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>⏟</mo></munder><mtext>(b) symbol replacement</mtext></munder><msubsup><mi>z</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></msup></mrow></mrow></mrow></mrow></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></span></span></span>
</span>
</span>
</span></span>
We condition the whole process on an input variable <math id="S2.p1.m18" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> to indicate that the proposed model is applicable to both conditional and unconditional sequence generation. In the latter case, <math id="S2.p1.m19" class="ltx_Math" alttext="X=\emptyset" display="inline"><mrow><mi>X</mi><mo>=</mo><mi mathvariant="normal">∅</mi></mrow></math>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">We first predict the length <math id="S2.p2.m1" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> of a target sequence <math id="S2.p2.m2" class="ltx_Math" alttext="Y" display="inline"><mi>Y</mi></math> according to <math id="S2.p2.m3" class="ltx_Math" alttext="p(L|X)" display="inline"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>L</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math> distribution to which we refer as (c) length prediction. At each generation step <math id="S2.p2.m4" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math>, we first select the next coordinates <math id="S2.p2.m5" class="ltx_Math" alttext="Z^{t+1}" display="inline"><msup><mi>Z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup></math> for which the corresponding symbols will be replaced according to <math id="S2.p2.m6" class="ltx_Math" alttext="p(z^{t+1}_{i}|Y^{\leq t},Z^{t},X)" display="inline"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>z</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><msup><mi>Z</mi><mi>t</mi></msup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></math>, to which we refer as (a) coordinate selection. Once the coordinate sequence is determined, we replace the corresponding symbols according to the distribution <math id="S2.p2.m7" class="ltx_Math" alttext="p(y^{t+1}_{i}|Y^{\leq t},Z^{t+1},X)" display="inline"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>y</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><msup><mi>Z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></math>, leading to the next intermediate sequence <math id="S2.p2.m8" class="ltx_Math" alttext="Y^{t+1}" display="inline"><msup><mi>Y</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup></math>. From this sequence generation framework, we recover the sequence distribution <math id="S2.p2.m9" class="ltx_Math" alttext="p(Y|X)" display="inline"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Y</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math> by marginalizing out all the intermediate and coordinate sequences except for the final sequence <math id="S2.p2.m10" class="ltx_Math" alttext="Y^{T}" display="inline"><msup><mi>Y</mi><mi>T</mi></msup></math>. In the remainder of this section, we describe several special cases of the proposed framework, which are monotonic autoregressive, non-autoregressive, semi-autoregressive neural sequence models.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Special Cases</h3>

<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Monotonic autoregressive neural sequence models</h4>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">We first consider one extreme case of the generalized sequence generation model, where we replace one symbol at a time, monotonically moving from the left-most position to the right-most.
In this case, we define the coordinate selection distribution of the generalized sequence generation model in Eq. (<a href="#S2.E1" title="In 2 A Generalized Framework of Sequence Generation ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) (a) as
<math id="S2.SS1.SSS0.Px1.p1.m1" class="ltx_Math" alttext="p(z_{i+1}^{t+1}=1|Y^{\leq t},Z^{t},X)=\mathds{1}(z^{t}_{i}=1)" display="inline"><mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>z</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mrow><mn>1</mn><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><msup><mi>Z</mi><mi>t</mi></msup><mo>,</mo><mi>X</mi></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mn>𝟙</mn><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>z</mi><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math>,
where <math id="S2.SS1.SSS0.Px1.p1.m2" class="ltx_Math" alttext="\mathds{1}(\cdot)" display="inline"><mrow><mn>𝟙</mn><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow></math> is an indicator function and <math id="S2.SS1.SSS0.Px1.p1.m3" class="ltx_Math" alttext="z^{1}_{1}=1" display="inline"><mrow><msubsup><mi>z</mi><mn>1</mn><mn>1</mn></msubsup><mo>=</mo><mn>1</mn></mrow></math>. This coordinate selection distribution is equivalent to saying that we replace one symbol at a time, shifting from the left-most symbol to the right-most symbol, regardless of the content of intermediate sequences.
We then choose the symbol replacement distribution in Eq. (<a href="#S2.E1" title="In 2 A Generalized Framework of Sequence Generation ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) (b) to be
<math id="S2.SS1.SSS0.Px1.p1.m4" class="ltx_Math" alttext="p(y_{i+1}^{t+1}|Y^{\leq t},X)=p(y_{i+1}^{t+1}|y_{1}^{t},y_{2}^{t},\ldots,y_{i}%
^{t},X)," display="inline"><mrow><mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo fence="false">|</mo><mrow><msubsup><mi>y</mi><mn>1</mn><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mn>2</mn><mi>t</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>t</mi></msubsup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>
for <math id="S2.SS1.SSS0.Px1.p1.m5" class="ltx_Math" alttext="z_{i+1}^{t+1}=1" display="inline"><mrow><msubsup><mi>z</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mn>1</mn></mrow></math>. Intuitively, we limit the dependency of <math id="S2.SS1.SSS0.Px1.p1.m6" class="ltx_Math" alttext="y_{i+1}^{t+1}" display="inline"><msubsup><mi>y</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math> only to the symbols to its left in the previous intermediate sequence <math id="S2.SS1.SSS0.Px1.p1.m7" class="ltx_Math" alttext="y_{&lt;(i+1)}^{t}" display="inline"><msubsup><mi>y</mi><mrow><mi></mi><mo>&lt;</mo><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mi>t</mi></msubsup></math> and the input variable <math id="S2.SS1.SSS0.Px1.p1.m8" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math>.
The length distribution (<a href="#S2.E1" title="In 2 A Generalized Framework of Sequence Generation ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) (c) is implicitly defined by considering how often the special token <math id="S2.SS1.SSS0.Px1.p1.m9" class="ltx_Math" alttext="\left&lt;\text{eos}\right&gt;" display="inline"><mrow><mo>⟨</mo><mtext>eos</mtext><mo>⟩</mo></mrow></math>, which indicates the end of a sequence, appears after <math id="S2.SS1.SSS0.Px1.p1.m10" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> generation steps:
<math id="S2.SS1.SSS0.Px1.p1.m11" class="ltx_Math" alttext="p(L|X)\propto\sum_{y_{1:L-1}}\prod_{l=1}^{L-1}p(y_{l+1}^{l+1}=\left&lt;\text{eos}%
\right&gt;|y_{\leq l}^{\leq l},X)" display="inline"><mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>L</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">∝</mo><mrow><msub><mo rspace="0em">∑</mo><msub><mi>y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub></msub><mrow><msubsup><mo>∏</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>y</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mrow><mrow><mo>⟨</mo><mtext>eos</mtext><mo>⟩</mo></mrow><mo fence="false">|</mo><mrow><msubsup><mi>y</mi><mrow><mi></mi><mo>≤</mo><mi>l</mi></mrow><mrow><mi></mi><mo>≤</mo><mi>l</mi></mrow></msubsup><mo>,</mo><mi>X</mi></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></math>.
With these choices, the proposed generalized model reduces to
<math id="S2.SS1.SSS0.Px1.p1.m12" class="ltx_Math" alttext="p(G|X)=\prod_{i=1}^{L}p(y_{i}|y_{&lt;i},X)" display="inline"><mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>G</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo fence="false">|</mo><mrow><msub><mi>y</mi><mrow><mi></mi><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math>
which is a widely-used monotonic autoregressive neural sequence model.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Non-autoregressive neural sequence modeling by iterative refinement</h4>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">We next consider the other extreme in which we replace the symbols in all positions at every single generation step <cite class="ltx_cite ltx_citemacro_citep">(Lee<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib26" title="Deterministic non-autoregressive neural sequence modeling by iterative refinement" class="ltx_ref">2018</a>)</cite>. We design the coordinate selection distribution to be implying that we replace the symbols in all the positions. We then choose the symbol replacement distribution to be as it was in Eq. (<a href="#S2.E1" title="In 2 A Generalized Framework of Sequence Generation ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) (b). That is, the distribution over the symbols in the position <math id="S2.SS1.SSS0.Px2.p1.m1" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> in a new intermediate sequence <math id="S2.SS1.SSS0.Px2.p1.m2" class="ltx_Math" alttext="y_{i}^{t+1}" display="inline"><msubsup><mi>y</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math> is conditioned on the entire current sequence <math id="S2.SS1.SSS0.Px2.p1.m3" class="ltx_Math" alttext="Y^{t}" display="inline"><msup><mi>Y</mi><mi>t</mi></msup></math> and the input variable <math id="S2.SS1.SSS0.Px2.p1.m4" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math>. We do not need to assume any relationship between the number of generation steps <math id="S2.SS1.SSS0.Px2.p1.m5" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> and the length of a sequence <math id="S2.SS1.SSS0.Px2.p1.m6" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> in this case. The length prediction distribution <math id="S2.SS1.SSS0.Px2.p1.m7" class="ltx_Math" alttext="p(L|X)" display="inline"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>L</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math> is estimated from training data.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Semi-autoregressive neural sequence models</h4>

<div id="S2.SS1.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite ltx_citemacro_citep">(Wang<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib17" title="Semi-autoregressive neural machine translation" class="ltx_ref">2018</a>)</cite> recently proposed a compromise between autoregressive and non-autoregresive sequence models by predicting a chunk of symbols in parallel at a time. This approach can also be put under the proposed generalized model. We first extend the coordinate selection distribution of the autoregressive sequence model
into
</p>
<table id="A4.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex1.m1" class="ltx_Math" alttext="\displaystyle p(z_{k(i+1)+j}^{t+1}=1|Y^{\leq t},Z^{t},X)=" display="inline"><mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>z</mi><mrow><mrow><mi>k</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>j</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mrow><mn>1</mn><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><msup><mi>Z</mi><mi>t</mi></msup><mo>,</mo><mi>X</mi></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi></mi></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex2.m1" class="ltx_Math" alttext="\displaystyle=\begin{cases}1,&amp;\text{ if }z^{t}_{ki+j}=1,\forall j\in\left\{0,1%
,\ldots,k\right\}\\
0,&amp;\text{ otherwise},\end{cases}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mn>1</mn><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mrow><mtext> if </mtext><mo>⁢</mo><msubsup><mi>z</mi><mrow><mrow><mi>k</mi><mo>⁢</mo><mi>i</mi></mrow><mo>+</mo><mi>j</mi></mrow><mi>t</mi></msubsup></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>j</mi></mrow><mo>∈</mo><mrow><mo>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>k</mi><mo>}</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mtext> otherwise</mtext><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S2.SS1.SSS0.Px3.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> is a <span class="ltx_text ltx_font_italic">group size</span>. Similarly we modify the symbol replacement distribution:</p>
<table id="A4.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex3.m1" class="ltx_Math" alttext="\displaystyle p(y_{k(i+1)+j}^{t+1}|Y^{\leq t},X)=" display="inline"><mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>y</mi><mrow><mrow><mi>k</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>j</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi></mi></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex3.m2" class="ltx_Math" alttext="\displaystyle p(y_{k(i+1)+j}^{t+1}|y^{t}_{&lt;k(i+1)},X)," display="inline"><mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>y</mi><mrow><mrow><mi>k</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>j</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo fence="false">|</mo><mrow><msubsup><mi>y</mi><mrow><mi></mi><mo>&lt;</mo><mrow><mi>k</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mi>t</mi></msubsup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex4.m1" class="ltx_Math" alttext="\displaystyle\forall j\in\left\{0,1,\ldots,k\right\}," display="inline"><mrow><mrow><mrow><mo rspace="0.167em">∀</mo><mi>j</mi></mrow><mo>∈</mo><mrow><mo>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>k</mi><mo>}</mo></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">for <math id="S2.SS1.SSS0.Px3.p1.m2" class="ltx_Math" alttext="z_{i}^{t}=1" display="inline"><mrow><msubsup><mi>z</mi><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mn>1</mn></mrow></math>. This naturally implies that <math id="S2.SS1.SSS0.Px3.p1.m3" class="ltx_Math" alttext="T=\left\lceil L/k\right\rceil" display="inline"><mrow><mi>T</mi><mo>=</mo><mrow><mo>⌈</mo><mrow><mi>L</mi><mo>/</mo><mi>k</mi></mrow><mo>⌉</mo></mrow></mrow></math>.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Decoding from Masked Language Models</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">In this section, we give an overview of masked language models like BERT,
cast Gibbs sampling under the proposed framework,
and then use this connection to design a set of approximate, deterministic decoding algorithms for undirected sequence models.
</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>BERT as an undirected sequence model</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib41" title="BERT: pre-training of deep bidirectional transformers for language understanding" class="ltx_ref">2019</a>)</cite> is a masked language model: It is trained to predict a word given the word’s left and right context.
Because the model gets the full context, there are no directed dependencies among words, so the model is undirected.
The word to be predicted is masked with a special <math id="S3.SS1.p1.m1" class="ltx_Math" alttext="\left&lt;\text{mask}\right&gt;" display="inline"><mrow><mo>⟨</mo><mtext>mask</mtext><mo>⟩</mo></mrow></math> symbol and the model is trained to predict
<math id="S3.SS1.p1.m2" class="ltx_Math" alttext="p(y_{i}|y_{&lt;i},\left&lt;\text{mask}\right&gt;,y_{&gt;i},X)" display="inline"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo fence="false">|</mo><mrow><msub><mi>y</mi><mrow><mi></mi><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>,</mo><mrow><mo>⟨</mo><mtext>mask</mtext><mo>⟩</mo></mrow><mo>,</mo><msub><mi>y</mi><mrow><mi></mi><mo>&gt;</mo><mi>i</mi></mrow></msub><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></math>. We refer to this as the <span class="ltx_text ltx_font_italic">conditional BERT distribution</span>.
This objective was interpreted as a stochastic approximation to the pseudo log-likelihood objective <cite class="ltx_cite ltx_citemacro_citep">(Besag, <a href="#bib.bib19" title="Efficiency of pseudolikelihood estimation for simple gaussian fields" class="ltx_ref">1977</a>)</cite> by <cite class="ltx_cite ltx_citemacro_citet">Wang and Cho (<a href="#bib.bib14" title="BERT has a mouth, and it must speak: bert as a markov random field language model" class="ltx_ref">2019</a>)</cite>.
This approach of full-context generation with pseudo log-likelihood maximization for recurrent networks was introduced earlier by <cite class="ltx_cite ltx_citemacro_citet">Berglund<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib1" title="Bidirectional recurrent neural networks as generative models" class="ltx_ref">2015</a>)</cite>.
More recently, <cite class="ltx_cite ltx_citemacro_citet">Sun<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib20" title="Bidirectional beam search: forward-backward inference in neural sequence models for fill-in-the-blank image captioning" class="ltx_ref">2017</a>)</cite> use it for image caption generation.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">Recent work <cite class="ltx_cite ltx_citemacro_citep">(Wang and Cho, <a href="#bib.bib14" title="BERT has a mouth, and it must speak: bert as a markov random field language model" class="ltx_ref">2019</a>; Ghazvininejad<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib13" title="Mask-predict: parallel decoding of conditional masked language models" class="ltx_ref">2019</a>)</cite> has demonstrated that undirected neural sequence models like BERT can learn complex sequence distributions and generate well-formed sequences. In such models, it is relatively straightforward to collect unbiased samples using, for instance, Gibbs sampling.
But due to high variance of Gibbs sampling, the generated sequence is not guaranteed to be high-quality relative to a ground-truth sequence.
Finding a good sequence in a deterministic manner is also nontrivial.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p class="ltx_p">A number of papers have explored using pretrained language models like BERT to initialize sequence generation models.
<cite class="ltx_cite ltx_citemacro_citet">Ramachandran<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib9" title="Unsupervised pretraining for sequence to sequence learning" class="ltx_ref">2017</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Song<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib11" title="MASS: masked sequence to sequence pre-training for language generation" class="ltx_ref">2019</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Lample and Conneau (<a href="#bib.bib15" title="Cross-lingual language model pretraining" class="ltx_ref">2019</a>)</cite> use a pretrained undirected language model to initialize a conventional monotonic autoregressive sequence model, while
<cite class="ltx_cite ltx_citemacro_citet">Edunov<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib16" title="Pre-trained language model representations for language generation" class="ltx_ref">2019</a>)</cite> use a BERT-like model to initialize the lower layers of a generator, without finetuning.
Our work differs from these in that we attempt to directly generate from the pretrained model, rather than using it as a starting point to learn another model.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Gibbs sampling in the generalized sequence generation model</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Gibbs sampling: uniform coordinate selection</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">To cast Gibbs sampling into our framework,
we first assume that the length prediction distribution <math id="S3.SS2.SSS0.Px1.p1.m1" class="ltx_Math" alttext="P(L|X)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>L</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math> is estimated from training data, as is the case in the non-autoregressive neural sequence model.
In Gibbs sampling, we often uniformly select a new coordinate at random, which corresponds to
<math id="S3.SS2.SSS0.Px1.p1.m2" class="ltx_Math" alttext="p(z_{i}^{t+1}=1|Y^{\leq t},Z^{t},X)=1/L" display="inline"><mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>z</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mrow><mn>1</mn><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><msup><mi>Z</mi><mi>t</mi></msup><mo>,</mo><mi>X</mi></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>L</mi></mrow></mrow></math>
with the constraint that <math id="S3.SS2.SSS0.Px1.p1.m3" class="ltx_Math" alttext="\sum_{i=1}^{L}z_{i}^{t}=1" display="inline"><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><msubsup><mi>z</mi><mi>i</mi><mi>t</mi></msubsup></mrow><mo>=</mo><mn>1</mn></mrow></math>. By using the conditional BERT distribution as a symbol replacement distribution, we end up with Gibbs sampling.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Adaptive Gibbs sampling: non-uniform coordinate selection</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">Instead of selecting coordinates uniformly at random,
we can base selections on the intermediate sequences. We propose a log-linear model with features <math id="S3.SS2.SSS0.Px2.p1.m1" class="ltx_Math" alttext="\phi_{i}" display="inline"><msub><mi>ϕ</mi><mi>i</mi></msub></math> based on the intermediate and coordinate sequences:

<span class="ltx_inline-block ltx_transformed_outer" style="width:416.3pt;height:19.7pt;vertical-align:-7.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-10.3pt,0.3pt) scale(0.952683202465637,0.952683202465637) ;">
<span class="ltx_block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<span id="A4.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<span id="S3.E2"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1" class="ltx_Math" alttext="\displaystyle p(z_{i}^{t+1}=1|Y^{\leq t},Z^{t},X)\propto\exp\left\{\frac{1}{%
\tau}\sum_{i=1}^{L}\alpha_{i}\phi_{i}(Y^{t},Z^{t},X,i)\right\}" display="inline"><mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>z</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mrow><mn>1</mn><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><msup><mi>Z</mi><mi>t</mi></msup><mo>,</mo><mi>X</mi></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>{</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>τ</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover></mstyle><mrow><msub><mi>α</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>ϕ</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>Y</mi><mi>t</mi></msup><mo>,</mo><msup><mi>Z</mi><mi>t</mi></msup><mo>,</mo><mi>X</mi><mo>,</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></span></span></span>
</span>
</span>
</span></span>
again with the constraint that <math id="S3.SS2.SSS0.Px2.p1.m2" class="ltx_Math" alttext="\sum_{i=1}^{L}z_{i}^{t}=1" display="inline"><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><msubsup><mi>z</mi><mi>i</mi><mi>t</mi></msubsup></mrow><mo>=</mo><mn>1</mn></mrow></math>.
<math id="S3.SS2.SSS0.Px2.p1.m3" class="ltx_Math" alttext="\tau&gt;0" display="inline"><mrow><mi>τ</mi><mo>&gt;</mo><mn>0</mn></mrow></math> is a temperature parameter controlling the sharpness of the coordinate selection distribution. A moderately high <math id="S3.SS2.SSS0.Px2.p1.m4" class="ltx_Math" alttext="\tau" display="inline"><mi>τ</mi></math> smooths the coordinate selection distribution and ensures that all the coordinates are replaced in the infinite limit of <math id="S3.SS2.SSS0.Px2.p1.m5" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math>, making it a valid Gibbs sampler <cite class="ltx_cite ltx_citemacro_citep">(Levine and Casella, <a href="#bib.bib12" title="Optimizing random scan gibbs samplers" class="ltx_ref">2006</a>)</cite>.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p">We investigate three features <math id="S3.SS2.SSS0.Px2.p2.m1" class="ltx_Math" alttext="\phi_{i}" display="inline"><msub><mi>ϕ</mi><mi>i</mi></msub></math>: (1) We compute how peaked the conditional distribution of each position is given the symbols in all the other positions by measuring its <span class="ltx_text ltx_font_italic">negative entropy</span>:
<math id="S3.SS2.SSS0.Px2.p2.m2" class="ltx_Math" alttext="\phi_{\text{negent}}(Y^{t},Z^{t},X,i)=-\mathcal{H}(y^{t+1}_{i}|y^{t}_{&lt;i},%
\left&lt;\text{mask}\right&gt;,y^{t}_{&gt;i},X)." display="inline"><mrow><mrow><mrow><msub><mi>ϕ</mi><mtext>negent</mtext></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>Y</mi><mi>t</mi></msup><mo>,</mo><msup><mi>Z</mi><mi>t</mi></msup><mo>,</mo><mi>X</mi><mo>,</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><mi class="ltx_font_mathcaligraphic">ℋ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>y</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo fence="false">|</mo><mrow><msubsup><mi>y</mi><mrow><mi></mi><mo>&lt;</mo><mi>i</mi></mrow><mi>t</mi></msubsup><mo>,</mo><mrow><mo>⟨</mo><mtext>mask</mtext><mo>⟩</mo></mrow><mo>,</mo><msubsup><mi>y</mi><mrow><mi></mi><mo>&gt;</mo><mi>i</mi></mrow><mi>t</mi></msubsup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math>
In other words, we prefer a position <math id="S3.SS2.SSS0.Px2.p2.m3" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> if we know the change in <math id="S3.SS2.SSS0.Px2.p2.m4" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> has a high potential to alter the joint probability <math id="S3.SS2.SSS0.Px2.p2.m5" class="ltx_Math" alttext="p(Y|X)=p(y_{1},y_{2},...,y_{L}|X)" display="inline"><mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Y</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><msub><mi>y</mi><mi>L</mi></msub><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math>.
(2) For each position <math id="S3.SS2.SSS0.Px2.p2.m6" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> we measure how unlikely the <span class="ltx_text ltx_font_italic">current </span> symbol (<math id="S3.SS2.SSS0.Px2.p2.m7" class="ltx_Math" alttext="y^{t}_{i}" display="inline"><msubsup><mi>y</mi><mi>i</mi><mi>t</mi></msubsup></math>, not <math id="S3.SS2.SSS0.Px2.p2.m8" class="ltx_Math" alttext="y^{t+1}_{i}" display="inline"><msubsup><mi>y</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>) is under the <span class="ltx_text ltx_font_italic">new</span> conditional distribution:
<math id="S3.SS2.SSS0.Px2.p2.m9" class="ltx_Math" alttext="\phi_{\text{logp}}(Y^{t},Z^{t},X,i)=-\log p(y_{i}=y^{t}_{i}|y^{t}_{&lt;i},\left&lt;%
\text{mask}\right&gt;,y^{t}_{&gt;i},X)" display="inline"><mrow><mrow><msub><mi>ϕ</mi><mtext>logp</mtext></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>Y</mi><mi>t</mi></msup><mo>,</mo><msup><mi>Z</mi><mi>t</mi></msup><mo>,</mo><mi>X</mi><mo>,</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo rspace="0.167em">−</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mrow><msubsup><mi>y</mi><mi>i</mi><mi>t</mi></msubsup><mo fence="false">|</mo><mrow><msubsup><mi>y</mi><mrow><mi></mi><mo>&lt;</mo><mi>i</mi></mrow><mi>t</mi></msubsup><mo>,</mo><mrow><mo>⟨</mo><mtext>mask</mtext><mo>⟩</mo></mrow><mo>,</mo><msubsup><mi>y</mi><mrow><mi></mi><mo>&gt;</mo><mi>i</mi></mrow><mi>t</mi></msubsup><mo>,</mo><mi>X</mi></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math>.
Intuitively, we prefer to replace a symbol if it is highly incompatible with the input variable and all the other symbols in the current sequence.
(3) We encode a <span class="ltx_text ltx_font_italic">positional preference</span> that does not consider the content of intermediate sequences:
<math id="S3.SS2.SSS0.Px2.p2.m10" class="ltx_Math" alttext="\phi_{\text{pos}}(i)=-\log(|t-i|+\epsilon)" display="inline"><mrow><mrow><msub><mi>ϕ</mi><mtext>pos</mtext></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo rspace="0.167em">−</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">|</mo><mrow><mi>t</mi><mo>−</mo><mi>i</mi></mrow><mo stretchy="false">|</mo></mrow><mo>+</mo><mi>ϵ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math>,
where <math id="S3.SS2.SSS0.Px2.p2.m11" class="ltx_Math" alttext="\epsilon&gt;0" display="inline"><mrow><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow></math> is a small constant scalar to prevent <math id="S3.SS2.SSS0.Px2.p2.m12" class="ltx_Math" alttext="\log 0" display="inline"><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mn>0</mn></mrow></math>. This feature encodes our preference to generate from left to right if there is no information about
the input variable nor of any intermediate sequences.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p3" class="ltx_para">
<p class="ltx_p">Unlike the special cases of the proposed generalized model in §<a href="#S2" title="2 A Generalized Framework of Sequence Generation ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the coordinate at each generation step is selected based on the intermediate sequences, previous coordinate sequences, and the input variable.
We mix the features using scalar coefficients <math id="S3.SS2.SSS0.Px2.p3.m1" class="ltx_Math" alttext="\alpha_{\text{negent}}" display="inline"><msub><mi>α</mi><mtext>negent</mtext></msub></math>, <math id="S3.SS2.SSS0.Px2.p3.m2" class="ltx_Math" alttext="\alpha_{\text{logp}}" display="inline"><msub><mi>α</mi><mtext>logp</mtext></msub></math> and <math id="S3.SS2.SSS0.Px2.p3.m3" class="ltx_Math" alttext="\alpha_{\text{pos}}" display="inline"><msub><mi>α</mi><mtext>pos</mtext></msub></math>, which are selected or estimated to maximize a target quality measure on the validation set.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Adaptive Gibbs sampling: learned coordinate selection</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p">We learn a coordinate selection distribution that selects coordinates in order to maximize a reward function that we specify.
In this case, we refer to the coordinate selection distribution as a
<span class="ltx_text ltx_font_italic">policy</span>, <math id="S3.SS2.SSS0.Px3.p1.m1" class="ltx_Math" alttext="\pi_{\theta}(a_{t}|s_{t})" display="inline"><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>a</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi>s</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></math>,
where a state <math id="S3.SS2.SSS0.Px3.p1.m2" class="ltx_Math" alttext="s_{t}" display="inline"><msub><mi>s</mi><mi>t</mi></msub></math> is <math id="S3.SS2.SSS0.Px3.p1.m3" class="ltx_Math" alttext="(Y^{\leq t},Z^{t},X)" display="inline"><mrow><mo stretchy="false">(</mo><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><msup><mi>Z</mi><mi>t</mi></msup><mo>,</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></math>,
an action <math id="S3.SS2.SSS0.Px3.p1.m4" class="ltx_Math" alttext="a_{t}\in\{1,\ldots,L\}" display="inline"><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>L</mi><mo stretchy="false">}</mo></mrow></mrow></math> is a coordinate,
so that <math id="S3.SS2.SSS0.Px3.p1.m5" class="ltx_Math" alttext="Z^{t+1}" display="inline"><msup><mi>Z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup></math> is 1 at position <math id="S3.SS2.SSS0.Px3.p1.m6" class="ltx_Math" alttext="a_{t}" display="inline"><msub><mi>a</mi><mi>t</mi></msub></math> and 0 elsewhere,
and <math id="S3.SS2.SSS0.Px3.p1.m7" class="ltx_Math" alttext="\pi_{\theta}" display="inline"><msub><mi>π</mi><mi>θ</mi></msub></math> is parameterized using a neural network. Beginning at a state <math id="S3.SS2.SSS0.Px3.p1.m8" class="ltx_Math" alttext="s_{1}\sim p(s_{1})" display="inline"><mrow><msub><mi>s</mi><mn>1</mn></msub><mo>∼</mo><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></math> corresponding to an input <math id="S3.SS2.SSS0.Px3.p1.m9" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math>
along with an empty coordinate and output sequence, we obtain a generation by repeatedly sampling a coordinate <math id="S3.SS2.SSS0.Px3.p1.m10" class="ltx_math_unparsed" alttext="a_{t}\sim\pi_{\theta}(\cdot|s_{t})" display="inline"><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>∼</mo><msub><mi>π</mi><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></math> and transitioning to a new state for <math id="S3.SS2.SSS0.Px3.p1.m11" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> steps.
Each transition, <math id="S3.SS2.SSS0.Px3.p1.m12" class="ltx_math_unparsed" alttext="s_{t+1}\sim p(\cdot|s_{t},a_{t})" display="inline"><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∼</mo><mi>p</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></math>, consists of generating a symbol at position <math id="S3.SS2.SSS0.Px3.p1.m13" class="ltx_Math" alttext="a_{t}" display="inline"><msub><mi>a</mi><mi>t</mi></msub></math>.
Given a scalar reward function <math id="S3.SS2.SSS0.Px3.p1.m14" class="ltx_Math" alttext="r(s_{t},a_{t},s_{t+1})" display="inline"><mrow><mi>r</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></math>, the objective is to find a policy that maximizes expected reward, with the expectation taken over the distribution of generations obtained using the policy for coordinate selection,</p>
<table id="A4.EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1" class="ltx_Math" alttext="\displaystyle J(\theta)" display="inline"><mrow><mi>J</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3.m2" class="ltx_Math" alttext="\displaystyle=\mathbb{E}_{\tau\sim\pi_{\theta}(\tau)}\left[\sum_{t=1}^{T}%
\gamma^{t-1}r(s_{t},a_{t},s_{t+1})\right]," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><msub><mi>𝔼</mi><mrow><mi>τ</mi><mo>∼</mo><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>τ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msub><mo>⁢</mo><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover></mstyle><mrow><msup><mi>γ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1" class="ltx_Math" alttext="\displaystyle\pi_{\theta}(\tau)" display="inline"><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>τ</mi><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4.m2" class="ltx_Math" alttext="\displaystyle=p(s_{1})\prod_{t=1}^{T}\pi_{\theta}(a_{t}|s_{t})p(s_{t+1}|a_{t},%
s_{t})," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover></mstyle><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>a</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi>s</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>s</mi><mi>t</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S3.SS2.SSS0.Px3.p1.m15" class="ltx_Math" alttext="\tau=(s_{1},a_{1},s_{2},\ldots,a_{T},s_{T+1})" display="inline"><mrow><mi>τ</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>a</mi><mi>T</mi></msub><mo>,</mo><msub><mi>s</mi><mrow><mi>T</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></math>, and <math id="S3.SS2.SSS0.Px3.p1.m16" class="ltx_Math" alttext="\gamma\in[0,1]" display="inline"><mrow><mi>γ</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow></math> is a discount factor (with <math id="S3.SS2.SSS0.Px3.p1.m17" class="ltx_Math" alttext="0^{0}=1" display="inline"><mrow><msup><mn>0</mn><mn>0</mn></msup><mo>=</mo><mn>1</mn></mrow></math>).
We maximize this objective by estimating its gradient using policy gradient methods <cite class="ltx_cite ltx_citemacro_citep">(Williams, <a href="#bib.bib2" title="Simple statistical gradient-following algorithms for connectionist reinforcement learning" class="ltx_ref">1992</a>)</cite>.
We discuss our choice of reward function, policy parameterization, and hyperparameters later in Section <a href="#S4" title="4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Optimistic decoding and beam search from a masked language model</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p">Based on the adaptive Gibbs sampler with the non-uniform and learned coordinate selection distributions we can now design an inference procedure to approximately find the most likely sequence <math id="S3.SS3.p1.m1" class="ltx_Math" alttext="\operatorname*{argmax}_{Y}p(Y|X)" display="inline"><mrow><mrow><msub><mo>argmax</mo><mi>Y</mi></msub><mi>p</mi></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Y</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math> from the sequence distribution by exploiting the corresponding model of sequence generation. In doing so, a naive approach is to marginalize out the generation procedure <math id="S3.SS3.p1.m2" class="ltx_Math" alttext="G" display="inline"><mi>G</mi></math> using a Monte Carlo method:
<math id="S3.SS3.p1.m3" class="ltx_Math" alttext="\operatorname*{argmax}_{Y^{T}}\frac{1}{M}\sum_{G^{m}}p(Y^{T}|Y^{m,&lt;T},Z^{m,%
\leq T},X)" display="inline"><mrow><mrow><msub><mo>argmax</mo><msup><mi>Y</mi><mi>T</mi></msup></msub><mfrac><mn>1</mn><mi>M</mi></mfrac></mrow><mo>⁢</mo><mrow><msub><mo>∑</mo><msup><mi>G</mi><mi>m</mi></msup></msub><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>Y</mi><mi>T</mi></msup><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mrow><mi>m</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>T</mi></mrow></mrow></msup><mo>,</mo><msup><mi>Z</mi><mrow><mi>m</mi><mo>,</mo><mrow><mi></mi><mo>≤</mo><mi>T</mi></mrow></mrow></msup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math>
where <math id="S3.SS3.p1.m4" class="ltx_Math" alttext="G^{m}" display="inline"><msup><mi>G</mi><mi>m</mi></msup></math> is the <math id="S3.SS3.p1.m5" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math>-th sample from the sequence generation model. This approach suffers from a high variance and non-deterministic behavior, and is less appropriate for practical use.
We instead propose an optimistic decoding approach following equation (<a href="#S2.E1" title="In 2 A Generalized Framework of Sequence Generation ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>):

<span class="ltx_inline-block ltx_transformed_outer" style="width:429.3pt;height:41.3pt;vertical-align:-17.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.8pt,0.2pt) scale(0.982453137156281,0.982453137156281) ;">
<span class="ltx_block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<span id="A4.EGx6" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<span id="S3.E5"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1" class="ltx_Math" alttext="\displaystyle\underset{\begin{subarray}{c}L,Y^{1},\ldots,Y^{T}\\
Z^{1},\ldots,Z^{T}\end{subarray}}{\operatorname*{argmax}}\log p(L|X)+" display="inline"><mrow><mrow><munder accentunder="true"><mo>argmax</mo><mtable rowspacing="0pt"><mtr><mtd><mrow><mi mathsize="70%">L</mi><mo mathsize="70%">,</mo><msup><mi mathsize="70%">Y</mi><mn mathsize="70%">1</mn></msup><mo mathsize="70%">,</mo><mi mathsize="70%" mathvariant="normal">…</mi><mo mathsize="70%">,</mo><msup><mi mathsize="70%">Y</mi><mi mathsize="70%">T</mi></msup></mrow></mtd></mtr><mtr><mtd><mrow><msup><mi mathsize="70%">Z</mi><mn mathsize="70%">1</mn></msup><mo mathsize="70%">,</mo><mi mathsize="70%" mathvariant="normal">…</mi><mo mathsize="70%">,</mo><msup><mi mathsize="70%">Z</mi><mi mathsize="70%">T</mi></msup></mrow></mtd></mtr></mtable></munder><mo lspace="0.167em">⁢</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>L</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo></mrow></math></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E5.m2" class="ltx_math_unparsed" alttext="\displaystyle\sum_{t=1}^{T}\sum_{i=1}^{L}\Big{(}\log p(z^{t+1}_{i}|Y^{\leq t},%
Z^{t},X)" display="inline"><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover></mstyle><mrow><mo maxsize="160%" minsize="160%">(</mo><mi>log</mi><mi>p</mi><mrow><mo stretchy="false">(</mo><msubsup><mi>z</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><msup><mi>Z</mi><mi>t</mi></msup><mo>,</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></span></span></span>
<span id="S3.Ex5"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_eqn_cell"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex5.m1" class="ltx_math_unparsed" alttext="\displaystyle+z^{t+1}_{i}\log p(y^{t+1}_{i}|Y^{\leq t},X)\Big{)}" display="inline"><mrow><mo>+</mo><msubsup><mi>z</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mi>log</mi><mi>p</mi><mrow><mo stretchy="false">(</mo><msubsup><mi>y</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><mi>X</mi><mo stretchy="false">)</mo></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span>
</span></span>
The proposed procedure is <span class="ltx_text ltx_font_italic">optimistic</span> in that we consider a sequence generated by following the most likely generation path to be highly likely under the sequence distribution obtained by marginalizing out the generation path. This optimism in the criterion more readily admits a deterministic approximation scheme such as greedy and beam search, although it is as intractable to solve this problem as the original problem
which required marginalization of the generation path.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Length-conditioned beam search</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">To solve this intractable optimization problem, we design a heuristic algorithm, called length-conditioned beam search.
Intuitively, given a length <math id="S3.SS3.SSS0.Px1.p1.m1" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math>, this algorithm performs beam search over the coordinate and intermediate token sequences.
At each step <math id="S3.SS3.SSS0.Px1.p1.m2" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> of this iterative algorithm, we start from the hypothesis set <math id="S3.SS3.SSS0.Px1.p1.m3" class="ltx_Math" alttext="\mathcal{H}^{t-1}" display="inline"><msup><mi class="ltx_font_mathcaligraphic">ℋ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></math> that contains <math id="S3.SS3.SSS0.Px1.p1.m4" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> generation hypotheses:
<math id="S3.SS3.SSS0.Px1.p1.m5" class="ltx_Math" alttext="\mathcal{H}^{t-1}=\left\{h^{t-1}_{k}=((\hat{Y}_{k}^{1},\ldots,\hat{Y}_{k}^{t-1%
}),(\hat{Z}_{k}^{1},\ldots,\hat{Z}_{k}^{t-1}))\right\}_{k=1}^{K}" display="inline"><mrow><msup><mi class="ltx_font_mathcaligraphic">ℋ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><msubsup><mrow><mo>{</mo><mrow><msubsup><mi>h</mi><mi>k</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mi>k</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mi>k</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo>,</mo><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>Z</mi><mo>^</mo></mover><mi>k</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mover accent="true"><mi>Z</mi><mo>^</mo></mover><mi>k</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow></math>.
Each generation hypothesis has a score:

<span class="ltx_inline-block ltx_transformed_outer" style="width:429.3pt;height:55.5pt;vertical-align:-24.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.8pt,0.3pt) scale(0.982453137156281,0.982453137156281) ;">
<span class="ltx_block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<span id="A4.EGx7" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<span id="S3.Ex6"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex6.m1" class="ltx_Math" alttext="\displaystyle s(h^{t-1}_{k})=\log p(L|X)+" display="inline"><mrow><mrow><mi>s</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>h</mi><mi>k</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>L</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo></mrow></mrow></math></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex6.m2" class="ltx_math_unparsed" alttext="\displaystyle\sum_{t^{\prime}=1}^{t-1}\sum_{i=1}^{L}\Bigg{(}\log p(\hat{z}^{t^%
{\prime}}_{i}|\hat{Y}^{&lt;t^{\prime}}_{k},\hat{Z}^{t^{\prime}-1},X)" display="inline"><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><msup><mi>t</mi><mo>′</mo></msup><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover></mstyle><mrow><mo maxsize="260%" minsize="260%">(</mo><mi>log</mi><mi>p</mi><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>i</mi><msup><mi>t</mi><mo>′</mo></msup></msubsup><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msubsup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mi>k</mi><mrow><mi></mi><mo>&lt;</mo><msup><mi>t</mi><mo>′</mo></msup></mrow></msubsup><mo>,</mo><msup><mover accent="true"><mi>Z</mi><mo>^</mo></mover><mrow><msup><mi>t</mi><mo>′</mo></msup><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
<span id="S3.Ex7"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_eqn_cell"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex7.m1" class="ltx_math_unparsed" alttext="\displaystyle+\hat{z}^{t^{\prime}}_{i}\log p(\hat{y}^{t^{\prime}}_{i}|\hat{Y}^%
{\leq t},X)\Bigg{)}." display="inline"><mrow><mo>+</mo><msubsup><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>i</mi><msup><mi>t</mi><mo>′</mo></msup></msubsup><mi>log</mi><mi>p</mi><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi><msup><mi>t</mi><mo>′</mo></msup></msubsup><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><mi>X</mi><mo stretchy="false">)</mo></mrow><mo maxsize="260%" minsize="260%">)</mo><mo lspace="0em">.</mo></mrow></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span>
</span></span></p>
</div>
<div id="S3.SS3.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p">For notational simplicity, we drop the time superscript <math id="S3.SS3.SSS0.Px1.p2.m1" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math>.
Each of the <math id="S3.SS3.SSS0.Px1.p2.m2" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> generation hypotheses is first expanded with <math id="S3.SS3.SSS0.Px1.p2.m3" class="ltx_Math" alttext="K^{\prime}" display="inline"><msup><mi>K</mi><mo>′</mo></msup></math> candidate positions according to the coordinate selection distribution:

<span class="ltx_inline-block ltx_transformed_outer" style="width:429.3pt;height:14.1pt;vertical-align:-4.1pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.8pt,0.1pt) scale(0.982453137156281,0.982453137156281) ;">
<span class="ltx_block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<span id="A4.EGx8" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<span id="S3.Ex8"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex8.m1" class="ltx_Math" alttext="\displaystyle\text{arg~{}top-$K^{\prime}$}_{u\in\{1,\dots,L\}}\underbrace{s(h_%
{k})+\log p(z_{k,u}=1|\hat{Y}^{&lt;t},\hat{Z}^{t-1},X)}_{=s(h_{k}\|\text{one-hot}%
(u))}" display="inline"><mrow><msub><mrow><mtext>arg top-</mtext><msup><mi>K</mi><mo>′</mo></msup></mrow><mrow><mi>u</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>L</mi><mo stretchy="false">}</mo></mrow></mrow></msub><mo>⁢</mo><munder><munder accentunder="true"><mrow><mrow><mi>s</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mrow><mi>k</mi><mo>,</mo><mi>u</mi></mrow></msub><mo>=</mo><mrow><mn>1</mn><mo fence="false">|</mo><mrow><msup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></msup><mo>,</mo><msup><mover accent="true"><mi>Z</mi><mo>^</mo></mover><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><mi>X</mi></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>⏟</mo></munder><mrow><mi></mi><mo>=</mo><mrow><mi>s</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>h</mi><mi>k</mi></msub><mo>∥</mo><mrow><mtext>one-hot</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></munder></mrow></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span>
</span></span>
so that we have <math id="S3.SS3.SSS0.Px1.p2.m4" class="ltx_Math" alttext="K\times K^{\prime}" display="inline"><mrow><mi>K</mi><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mi>K</mi><mo>′</mo></msup></mrow></math> candidates <math id="S3.SS3.SSS0.Px1.p2.m5" class="ltx_Math" alttext="\left\{\hat{h}_{k,k^{\prime}}\right\}" display="inline"><mrow><mo>{</mo><msub><mover accent="true"><mi>h</mi><mo>^</mo></mover><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub><mo>}</mo></mrow></math>, where each candidate consists of a hypothesis <math id="S3.SS3.SSS0.Px1.p2.m6" class="ltx_Math" alttext="h_{k}" display="inline"><msub><mi>h</mi><mi>k</mi></msub></math> with the position sequence extended by the selected position
<math id="S3.SS3.SSS0.Px1.p2.m7" class="ltx_Math" alttext="u_{k,k^{\prime}}" display="inline"><msub><mi>u</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub></math>
and has a score
<math id="S3.SS3.SSS0.Px1.p2.m8" class="ltx_Math" alttext="s(h_{k}\|\text{one-hot}(u_{k,k^{\prime}}))" display="inline"><mrow><mi>s</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>h</mi><mi>k</mi></msub><mo>∥</mo><mrow><mtext>one-hot</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>u</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></math>.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>
                <span class="ltx_tag ltx_tag_note">1</span>
                
                
                
              
<math id="footnote1.m1" class="ltx_Math" alttext="h_{k}\|\text{one-hot}(u_{k,k^{\prime}})" display="inline"><mrow><msub><mi>h</mi><mi>k</mi></msub><mo>∥</mo><mrow><mtext>one-hot</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>u</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></math> appends <math id="footnote1.m2" class="ltx_Math" alttext="\text{one-hot}(u_{k,k^{\prime}})" display="inline"><mrow><mtext>one-hot</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>u</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></math> at the end of the sequence of the coordinate sequences in <math id="footnote1.m3" class="ltx_Math" alttext="h_{k}" display="inline"><msub><mi>h</mi><mi>k</mi></msub></math>
</span></span></span>
We then expand each candidate with the symbol replacement distribution:

<span class="ltx_inline-block ltx_transformed_outer" style="width:429.3pt;height:15.1pt;vertical-align:-4.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.8pt,0.1pt) scale(0.982453137156281,0.982453137156281) ;">
<span class="ltx_block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<span id="A4.EGx9" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<span id="S3.Ex9"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex9.m1" class="ltx_Math" alttext="\displaystyle\text{arg~{}top-$K^{\prime\prime}$}_{v\in V}\underbrace{s(h_{k}\|%
\text{one-hot}(u_{k,k^{\prime}}))+\log p(y_{z_{k,k^{\prime}}}=v|\hat{Y}^{\leq t%
},X)}_{=s(h_{k,k^{\prime}}\|(\hat{Y}^{t-1}_{&lt;z_{k,k^{\prime}}},v,\hat{Y}^{t-1}%
_{&gt;z_{k,k^{\prime}}}))}." display="inline"><mrow><mrow><msub><mrow><mtext>arg top-</mtext><msup><mi>K</mi><mo>′′</mo></msup></mrow><mrow><mi>v</mi><mo>∈</mo><mi>V</mi></mrow></msub><mo>⁢</mo><munder><munder accentunder="true"><mrow><mrow><mi>s</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>h</mi><mi>k</mi></msub><mo>∥</mo><mrow><mtext>one-hot</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>u</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>y</mi><msub><mi>z</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub></msub><mo>=</mo><mrow><mi>v</mi><mo fence="false">|</mo><mrow><msup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><mi>X</mi></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>⏟</mo></munder><mrow><mi></mi><mo>=</mo><mrow><mi>s</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>h</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub><mo>∥</mo><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mrow><mi></mi><mo>&lt;</mo><msub><mi>z</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo>,</mo><mi>v</mi><mo>,</mo><msubsup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mrow><mi></mi><mo>&gt;</mo><msub><mi>z</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></munder></mrow><mo lspace="0em">.</mo></mrow></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span>
</span></span>
This results in <math id="S3.SS3.SSS0.Px1.p2.m9" class="ltx_Math" alttext="K\times K^{\prime}\times K^{\prime\prime}" display="inline"><mrow><mi>K</mi><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mi>K</mi><mo>′</mo></msup><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mi>K</mi><mo>′′</mo></msup></mrow></math> candidates <math id="S3.SS3.SSS0.Px1.p2.m10" class="ltx_Math" alttext="\left\{\hat{\hat{h}}_{k,k^{\prime},k^{\prime\prime}}\right\}" display="inline"><mrow><mo>{</mo><msub><mover accent="true"><mover accent="true"><mi>h</mi><mo>^</mo></mover><mo>^</mo></mover><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup><mo>,</mo><msup><mi>k</mi><mo>′′</mo></msup></mrow></msub><mo>}</mo></mrow></math>, each consisting of hypothesis <math id="S3.SS3.SSS0.Px1.p2.m11" class="ltx_Math" alttext="h_{k}" display="inline"><msub><mi>h</mi><mi>k</mi></msub></math> with intermediate and coordinate sequence respectively extended by <math id="S3.SS3.SSS0.Px1.p2.m12" class="ltx_Math" alttext="v_{k,k^{\prime},k^{\prime\prime}}" display="inline"><msub><mi>v</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup><mo>,</mo><msup><mi>k</mi><mo>′′</mo></msup></mrow></msub></math> and <math id="S3.SS3.SSS0.Px1.p2.m13" class="ltx_Math" alttext="u_{k,k^{\prime}}" display="inline"><msub><mi>u</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub></math>. Each hypothesis has a score
<math id="S3.SS3.SSS0.Px1.p2.m14" class="ltx_Math" alttext="s(h_{k,k^{\prime}}\|(\hat{Y}^{t-1}_{&lt;z_{k,k^{\prime}}},v_{k,k^{\prime},k^{%
\prime\prime}},\hat{Y}^{t-1}_{&gt;z_{k,k^{\prime}}}))" display="inline"><mrow><mi>s</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>h</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub><mo>∥</mo><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mrow><mi></mi><mo>&lt;</mo><msub><mi>z</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo>,</mo><msub><mi>v</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup><mo>,</mo><msup><mi>k</mi><mo>′′</mo></msup></mrow></msub><mo>,</mo><msubsup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mrow><mi></mi><mo>&gt;</mo><msub><mi>z</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></math>,<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>
                <span class="ltx_tag ltx_tag_note">2</span>
                
                
                
              
<math id="footnote2.m1" class="ltx_Math" alttext="h_{k,k^{\prime}}\|(\hat{Y}^{t-1}_{&lt;z_{k,k^{\prime}}},v_{k,k^{\prime},k^{\prime%
\prime}},\hat{Y}^{t-1}_{&gt;z_{k,k^{\prime}}})" display="inline"><mrow><msub><mi>h</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub><mo>∥</mo><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mrow><mi></mi><mo>&lt;</mo><msub><mi>z</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo>,</mo><msub><mi>v</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup><mo>,</mo><msup><mi>k</mi><mo>′′</mo></msup></mrow></msub><mo>,</mo><msubsup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mrow><mi></mi><mo>&gt;</mo><msub><mi>z</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></math> denotes
creating a new sequence from <math id="footnote2.m2" class="ltx_Math" alttext="\hat{Y}^{t-1}" display="inline"><msup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></math> by replacing the <math id="footnote2.m3" class="ltx_Math" alttext="z_{k,k^{\prime}}" display="inline"><msub><mi>z</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub></math>-th symbol with <math id="footnote2.m4" class="ltx_Math" alttext="v_{k,k^{\prime},k^{\prime\prime}}" display="inline"><msub><mi>v</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup><mo>,</mo><msup><mi>k</mi><mo>′′</mo></msup></mrow></msub></math>, and appending this sequence to the intermediate sequences in <math id="footnote2.m5" class="ltx_Math" alttext="h_{k,k^{\prime}}" display="inline"><msub><mi>h</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub></math>.
</span></span></span>
which we use to select <math id="S3.SS3.SSS0.Px1.p2.m15" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> candidates to form a new hypothesis set
<math id="S3.SS3.SSS0.Px1.p2.m16" class="ltx_Math" alttext="\mathcal{H}^{t}=\text{arg~{}top-$K$}_{h\in\left\{\hat{\hat{h}}_{k,k^{\prime},k%
^{\prime\prime}}\right\}_{k,k^{\prime},k^{\prime\prime}}}s(h)" display="inline"><mrow><msup><mi class="ltx_font_mathcaligraphic">ℋ</mi><mi>t</mi></msup><mo>=</mo><mrow><msub><mrow><mtext>arg top-</mtext><mi>K</mi></mrow><mrow><mi>h</mi><mo>∈</mo><msub><mrow><mo>{</mo><msub><mover accent="true"><mover accent="true"><mi>h</mi><mo>^</mo></mover><mo>^</mo></mover><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup><mo>,</mo><msup><mi>k</mi><mo>′′</mo></msup></mrow></msub><mo>}</mo></mrow><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup><mo>,</mo><msup><mi>k</mi><mo>′′</mo></msup></mrow></msub></mrow></msub><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math>.</p>
</div>
<div id="S3.SS3.SSS0.Px1.p3" class="ltx_para">
<p class="ltx_p">After iterating for a predefined number <math id="S3.SS3.SSS0.Px1.p3.m1" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> of steps, the algorithm terminates with the final set of <math id="S3.SS3.SSS0.Px1.p3.m2" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> generation hypotheses. We then choose one of them according to a prespecified criterion, such as Eq. (<a href="#S3.E5" title="In 3.3 Optimistic decoding and beam search from a masked language model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), and return the final symbol sequence <math id="S3.SS3.SSS0.Px1.p3.m3" class="ltx_Math" alttext="\hat{Y}^{T}" display="inline"><msup><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mi>T</mi></msup></math>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_top">
<tr class="ltx_tr">
<td class="ltx_td"></td>
<td class="ltx_td"></td>
<td class="ltx_td ltx_border_rr"></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">Baseline</span></td>
<td class="ltx_td ltx_align_center" colspan="4"><span class="ltx_text" style="font-size:90%;">Decoding from an undirected sequence model</span></td>
<td class="ltx_td"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center"><math id="S3.T1.m1" class="ltx_Math" alttext="b" display="inline"><mi mathsize="90%">b</mi></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr"><math id="S3.T1.m2" class="ltx_Math" alttext="T" display="inline"><mi mathsize="90%">T</mi></math></td>
<td class="ltx_td ltx_align_center ltx_border_r">
<div class="ltx_inline-block ltx_transformed_outer" style="width:57.4pt;height:7.899999999999999pt;vertical-align:-1.8pt;"><span class="ltx_transformed_inner" style="width:57.4pt;transform:translate(0pt,2.63pt) rotate(-0deg) ;">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Autoregressive</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center">
<div class="ltx_inline-block ltx_transformed_outer" style="width:32.5pt;height:6.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.5pt;transform:translate(0pt,0pt) rotate(-0deg) ;">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Uniform</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center">
<div class="ltx_inline-block ltx_transformed_outer" style="width:42.3pt;height:8.1pt;vertical-align:-1.8pt;"><span class="ltx_transformed_inner" style="width:42.3pt;transform:translate(0pt,2.63pt) rotate(-0deg) ;">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Left2Right</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center">
<div class="ltx_inline-block ltx_transformed_outer" style="width:45.5pt;height:6.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:45.5pt;transform:translate(0pt,0pt) rotate(-0deg) ;">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Least2Most</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center">
<div class="ltx_inline-block ltx_transformed_outer" style="width:40.9pt;height:7.899999999999999pt;vertical-align:-1.8pt;"><span class="ltx_transformed_inner" style="width:40.9pt;transform:translate(0pt,2.63pt) rotate(-0deg) ;">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Easy-First</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center">
<div class="ltx_inline-block ltx_transformed_outer" style="width:31.7pt;height:6.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:31.7pt;transform:translate(0pt,0pt) rotate(-0deg) ;">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Learned</span></p>
</span></div>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt" rowspan="4"><span class="ltx_text" style="font-size:90%;">
<span class="ltx_inline-block ltx_transformed_outer" style="width:6.4pt;height:26.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:26.5pt;transform:translate(-10.04pt,-10.04pt) rotate(-90deg) ;">
<span class="ltx_p">En<math id="S3.T1.m3" class="ltx_Math" alttext="\to" display="inline"><mo stretchy="false">→</mo></math>De</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><math id="S3.T1.m4" class="ltx_Math" alttext="1" display="inline"><mn mathsize="90%">1</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><math id="S3.T1.m5" class="ltx_Math" alttext="L" display="inline"><mi mathsize="90%">L</mi></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span class="ltx_text" style="font-size:90%;">25.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">21.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">24.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">23.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">23.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">24.10</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math id="S3.T1.m6" class="ltx_Math" alttext="4" display="inline"><mn mathsize="90%">4</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr"><math id="S3.T1.m7" class="ltx_Math" alttext="L" display="inline"><mi mathsize="90%">L</mi></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">26.84</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">22.16</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">25.15</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">23.81</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.13</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.87</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math id="S3.T1.m8" class="ltx_Math" alttext="4" display="inline"><mn mathsize="90%">4</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr">
<math id="S3.T1.m9" class="ltx_Math" alttext="L" display="inline"><mi mathsize="90%">L</mi></math><span class="ltx_text" style="font-size:90%;">*</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">–</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">22.74</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">25.66</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.42</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.69</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">25.28</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math id="S3.T1.m10" class="ltx_Math" alttext="1" display="inline"><mn mathsize="90%">1</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr"><math id="S3.T1.m11" class="ltx_Math" alttext="2L" display="inline"><mrow><mn mathsize="90%">2</mn><mo>⁢</mo><mi mathsize="90%">L</mi></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">–</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">21.16</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.45</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">23.32</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">23.87</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.15</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center"><math id="S3.T1.m12" class="ltx_Math" alttext="4" display="inline"><mn mathsize="90%">4</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr"><math id="S3.T1.m13" class="ltx_Math" alttext="2L" display="inline"><mrow><mn mathsize="90%">2</mn><mo>⁢</mo><mi mathsize="90%">L</mi></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">–</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">21.99</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">25.14</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">23.81</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.14</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.86</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span class="ltx_text" style="font-size:90%;">
<span class="ltx_inline-block ltx_transformed_outer" style="width:6.4pt;height:26.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:26.5pt;transform:translate(-10.04pt,-10.04pt) rotate(-90deg) ;">
<span class="ltx_p">De<math id="S3.T1.m14" class="ltx_Math" alttext="\to" display="inline"><mo stretchy="false">→</mo></math>En</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T1.m15" class="ltx_Math" alttext="1" display="inline"><mn mathsize="90%">1</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><math id="S3.T1.m16" class="ltx_Math" alttext="L" display="inline"><mi mathsize="90%">L</mi></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">29.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">26.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">28.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">28.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">29.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">28.47</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math id="S3.T1.m17" class="ltx_Math" alttext="4" display="inline"><mn mathsize="90%">4</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr"><math id="S3.T1.m18" class="ltx_Math" alttext="L" display="inline"><mi mathsize="90%">L</mi></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">30.92</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">27.07</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">29.52</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">29.03</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">29.41</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">29.73</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math id="S3.T1.m19" class="ltx_Math" alttext="4" display="inline"><mn mathsize="90%">4</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr">
<math id="S3.T1.m20" class="ltx_Math" alttext="L" display="inline"><mi mathsize="90%">L</mi></math><span class="ltx_text" style="font-size:90%;">*</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">–</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">28.07</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">30.46</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">29.84</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">30.32</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">30.58</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math id="S3.T1.m21" class="ltx_Math" alttext="1" display="inline"><mn mathsize="90%">1</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr"><math id="S3.T1.m22" class="ltx_Math" alttext="2L" display="inline"><mrow><mn mathsize="90%">2</mn><mo>⁢</mo><mi mathsize="90%">L</mi></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">–</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">26.24</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">28.64</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">28.60</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">29.12</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">28.45</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center"><math id="S3.T1.m23" class="ltx_Math" alttext="4" display="inline"><mn mathsize="90%">4</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr"><math id="S3.T1.m24" class="ltx_Math" alttext="2L" display="inline"><mrow><mn mathsize="90%">2</mn><mo>⁢</mo><mi mathsize="90%">L</mi></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">–</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">26.98</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">29.50</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">29.02</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">29.41</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">29.71</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results (BLEU<math id="S3.T1.m31" class="ltx_Math" alttext="\uparrow" display="inline"><mo stretchy="false">↑</mo></math>) on WMT’14 En<math id="S3.T1.m32" class="ltx_Math" alttext="\leftrightarrow" display="inline"><mo stretchy="false">↔</mo></math>De translation using various decoding algorithms and different settings of beam search width (<math id="S3.T1.m33" class="ltx_Math" alttext="b" display="inline"><mi>b</mi></math>) and number of iterations (<math id="S3.T1.m34" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math>) as a function of sentence length (<math id="S3.T1.m35" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math>). For each sentence we use <math id="S3.T1.m36" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math> most likely sentence lengths. * denotes rescoring generated hypotheses using autoregressive model instead of proposed model.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Settings</h2>

<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data and preprocessing</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">We evaluate our framework on WMT’14 English-German translation.
The dataset consists of 4.5M parallel sentence pairs.
We preprocess this dataset by tokenizing each sentence using a script from Moses <cite class="ltx_cite ltx_citemacro_citep">(Koehn<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib22" title="Moses: open source toolkit for statistical machine translation" class="ltx_ref">2007</a>)</cite> and then segmenting each word into subword units using byte pair encoding <cite class="ltx_cite ltx_citemacro_citep">(Sennrich<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib23" title="Neural machine translation of rare words with subword units" class="ltx_ref">2016</a>)</cite> with a joint vocabulary of 60k tokens. We use newstest-2013 and newstest-2014 as validation and test sets respectively.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Sequence models</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">We base our models off those of <cite class="ltx_cite ltx_citemacro_citet">Lample and Conneau (<a href="#bib.bib15" title="Cross-lingual language model pretraining" class="ltx_ref">2019</a>)</cite>.
Specifically, we use a Transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib43" title="Attention is all you need" class="ltx_ref">2017</a>)</cite> with 1024 hidden units, 6 layers, 8 heads, and Gaussian error linear units <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks and Gimpel, <a href="#bib.bib25" title="Bridging nonlinearities and stochastic regularizers with gaussian error linear units" class="ltx_ref">2016</a>)</cite>.
We use a pretrained model<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>
              <span class="ltx_tag ltx_tag_note">3</span>
              
              
              
             <a href="https://dl.fbaipublicfiles.com/XLM/mlm_ende_1024.pth" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dl.fbaipublicfiles.com/XLM/mlm_ende_1024.pth</a></span></span></span> trained using a masked language modeling objective <cite class="ltx_cite ltx_citemacro_citep">(Lample and Conneau, <a href="#bib.bib15" title="Cross-lingual language model pretraining" class="ltx_ref">2019</a>)</cite> on 5M monolingual sentences from WMT NewsCrawl 2007-2008.
To distinguish between English and German sentences, a special language embedding is added as an additional input to the model.
</p>
</div>
<div id="S4.SS0.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p">We adapt the pretrained model to translation by finetuning it with a masked translation objective <cite class="ltx_cite ltx_citemacro_citep">(Lample and Conneau, <a href="#bib.bib15" title="Cross-lingual language model pretraining" class="ltx_ref">2019</a>)</cite>. We concatenate parallel English and German sentences,
mask out a subset of the tokens in either the English or German sentence, and predict the masked out tokens. We uniformly mask out <math id="S4.SS0.SSS0.Px2.p2.m1" class="ltx_Math" alttext="0-100\%" display="inline"><mrow><mn>0</mn><mo>−</mo><mrow><mn>100</mn><mo>%</mo></mrow></mrow></math> tokens as in <cite class="ltx_cite ltx_citemacro_citet">Ghazvininejad<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib13" title="Mask-predict: parallel decoding of conditional masked language models" class="ltx_ref">2019</a>)</cite>.
Training this way more closely matches the generation setting, where the model starts with an input sequence of all masks.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Baseline model</h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p">We compare against a standard Transformer encoder-decoder autoregressive neural sequence model <cite class="ltx_cite ltx_citemacro_citep">(Vaswani<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib43" title="Attention is all you need" class="ltx_ref">2017</a>)</cite> trained for left-to-right generation and initialized with the same pretrained model. We train a separate autoregressive model to translate an English sentence to a German sentence and vice versa, with the same hyperparameters as our model.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Training details</h4>

<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p">We train the models using Adam <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a href="#bib.bib4" title="Adam: a method for stochastic optimization" class="ltx_ref">2014</a>)</cite> with an inverse square root learning rate schedule, learning rate of <math id="S4.SS0.SSS0.Px4.p1.m1" class="ltx_Math" alttext="10^{-4}" display="inline"><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></math>, <math id="S4.SS0.SSS0.Px4.p1.m2" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow></math>, <math id="S4.SS0.SSS0.Px4.p1.m3" class="ltx_Math" alttext="\beta_{2}=0.98" display="inline"><mrow><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>0.98</mn></mrow></math>, and dropout rate of <math id="S4.SS0.SSS0.Px4.p1.m4" class="ltx_Math" alttext="0.1" display="inline"><mn>0.1</mn></math> <cite class="ltx_cite ltx_citemacro_citep">(Srivastava<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib24" title="Dropout: a simple way to prevent neural networks from overfitting" class="ltx_ref">2014</a>)</cite>.
Our models are trained on <math id="S4.SS0.SSS0.Px4.p1.m5" class="ltx_Math" alttext="8" display="inline"><mn>8</mn></math> GPUs with a batch size of <math id="S4.SS0.SSS0.Px4.p1.m6" class="ltx_Math" alttext="256" display="inline"><mn>256</mn></math> sentences.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Handcrafted decoding strategies</h4>

<figure id="S4.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_rr"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_center" colspan="4"><span class="ltx_text" style="font-size:90%;"># of length candidates</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_rr"></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">Gold</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">4</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">
<span class="ltx_text" style="font-size:90%;">En</span><math id="S4.T2.m1" class="ltx_Math" alttext="\to" display="inline"><mo mathsize="90%" stretchy="false">→</mo></math><span class="ltx_text" style="font-size:90%;">De</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span class="ltx_text" style="font-size:90%;">22.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">22.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">22.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">23.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">23.22</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_rr">
<span class="ltx_text" style="font-size:90%;">De</span><math id="S4.T2.m2" class="ltx_Math" alttext="\to" display="inline"><mo mathsize="90%" stretchy="false">→</mo></math><span class="ltx_text" style="font-size:90%;">En</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">28.05</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">26.77</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">27.32</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">27.79</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">28.15</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Effect of the number of length candidates considered during decoding on BLEU, measured on the validation set (newstest-2013) using the <span class="ltx_text ltx_font_bold">easy-first</span> strategy.</figcaption>
</figure>
<div id="S4.SS0.SSS0.Px5.p1" class="ltx_para">
<p class="ltx_p">We design four generation strategies for the masked translation model based on the log-linear coordinate selection distribution in §<a href="#S3.E2" title="In Adaptive Gibbs sampling: non-uniform coordinate selection ‣ 3.2 Gibbs sampling in the generalized sequence generation model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Uniform</span>: <math id="S4.I1.i1.p1.m1" class="ltx_Math" alttext="\tau\to\infty" display="inline"><mrow><mi>τ</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow></math>, i.e., sample a position uniformly at random without replacement</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Left2Right</span>: <math id="S4.I1.i2.p1.m1" class="ltx_Math" alttext="\alpha_{\text{negent}}=0" display="inline"><mrow><msub><mi>α</mi><mtext>negent</mtext></msub><mo>=</mo><mn>0</mn></mrow></math>, <math id="S4.I1.i2.p1.m2" class="ltx_Math" alttext="\alpha_{\text{logp}}=0" display="inline"><mrow><msub><mi>α</mi><mtext>logp</mtext></msub><mo>=</mo><mn>0</mn></mrow></math>, <math id="S4.I1.i2.p1.m3" class="ltx_Math" alttext="\alpha_{\text{pos}}=1" display="inline"><mrow><msub><mi>α</mi><mtext>pos</mtext></msub><mo>=</mo><mn>1</mn></mrow></math></p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Least2Most</span> <cite class="ltx_cite ltx_citemacro_citep">(Ghazvininejad<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib13" title="Mask-predict: parallel decoding of conditional masked language models" class="ltx_ref">2019</a>)</cite>: <math id="S4.I1.i3.p1.m1" class="ltx_Math" alttext="\alpha_{\text{negent}}=0" display="inline"><mrow><msub><mi>α</mi><mtext>negent</mtext></msub><mo>=</mo><mn>0</mn></mrow></math>, <math id="S4.I1.i3.p1.m2" class="ltx_Math" alttext="\alpha_{\text{logp}}=1" display="inline"><mrow><msub><mi>α</mi><mtext>logp</mtext></msub><mo>=</mo><mn>1</mn></mrow></math>, <math id="S4.I1.i3.p1.m3" class="ltx_Math" alttext="\alpha_{\text{pos}}=0" display="inline"><mrow><msub><mi>α</mi><mtext>pos</mtext></msub><mo>=</mo><mn>0</mn></mrow></math></p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Easy-First</span>: <math id="S4.I1.i4.p1.m1" class="ltx_Math" alttext="\alpha_{\text{negent}}=1" display="inline"><mrow><msub><mi>α</mi><mtext>negent</mtext></msub><mo>=</mo><mn>1</mn></mrow></math>, <math id="S4.I1.i4.p1.m2" class="ltx_Math" alttext="\alpha_{\text{logp}}=1" display="inline"><mrow><msub><mi>α</mi><mtext>logp</mtext></msub><mo>=</mo><mn>1</mn></mrow></math>,<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>
                    <span class="ltx_tag ltx_tag_note">4</span>
                    
                    
                    
                  
We set <math id="footnote4.m1" class="ltx_Math" alttext="\alpha_{\text{logp}}=0.9" display="inline"><mrow><msub><mi>α</mi><mtext>logp</mtext></msub><mo>=</mo><mn>0.9</mn></mrow></math> for De<math id="footnote4.m2" class="ltx_Math" alttext="\to" display="inline"><mo stretchy="false">→</mo></math>En based on the validation set performance.
</span></span></span>
<math id="S4.I1.i4.p1.m3" class="ltx_Math" alttext="\alpha_{\text{pos}}=0" display="inline"><mrow><msub><mi>α</mi><mtext>pos</mtext></msub><mo>=</mo><mn>0</mn></mrow></math></p>
</div>
</li>
</ol>
</div>
<div id="S4.SS0.SSS0.Px5.p2" class="ltx_para">
<p class="ltx_p">We use beam search described in §<a href="#S3.SS3" title="3.3 Optimistic decoding and beam search from a masked language model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> with <math id="S4.SS0.SSS0.Px5.p2.m1" class="ltx_Math" alttext="K^{\prime}" display="inline"><msup><mi>K</mi><mo>′</mo></msup></math> fixed to <math id="S4.SS0.SSS0.Px5.p2.m2" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math>, i.e., we consider only one possible position for replacing a symbol per hypothesis each time of generation. We vary <math id="S4.SS0.SSS0.Px5.p2.m3" class="ltx_Math" alttext="K=K^{\prime\prime}" display="inline"><mrow><mi>K</mi><mo>=</mo><msup><mi>K</mi><mo>′′</mo></msup></mrow></math> between <math id="S4.SS0.SSS0.Px5.p2.m4" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math> (greedy) and <math id="S4.SS0.SSS0.Px5.p2.m5" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math>. For each source sentence, we consider four length candidates according to the length distribution estimated from the training pairs,
based on early experiments showing that using only four length candidates performs as well as using the ground-truth length
(see Table <a href="#S4.T2" title="Table 2 ‣ Handcrafted decoding strategies ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
Given the four candidate translations,
we choose the best one according to the pseudo log-probability of the final sequence <cite class="ltx_cite ltx_citemacro_citep">(Wang and Cho, <a href="#bib.bib14" title="BERT has a mouth, and it must speak: bert as a markov random field language model" class="ltx_ref">2019</a>)</cite>. Additionally, we experimented with choosing best translation according to log-probability of the final sequence calculated by an autoregressive neural sequence model.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Learned decoding strategies</h4>

<div id="S4.SS0.SSS0.Px6.p1" class="ltx_para">
<p class="ltx_p">We train a parameterized coordinate selection policy to maximize expected reward (Eq. <a href="#S3.E3" title="In Adaptive Gibbs sampling: learned coordinate selection ‣ 3.2 Gibbs sampling in the generalized sequence generation model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). As the reward function, we use the change in edit distance from the reference,</p>
<table id="A4.EGx10" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex10.m1" class="ltx_Math" alttext="\displaystyle r(s_{t},a_{t},s_{t+1})=(d_{\text{edit}}(Y^{\leq t},Y)-d_{\text{%
edit}}(Y^{\leq{t+1}},Y))," display="inline"><mrow><mrow><mrow><mi>r</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>d</mi><mtext>edit</mtext></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi>d</mi><mtext>edit</mtext></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow></msup><mo>,</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S4.SS0.SSS0.Px6.p1.m1" class="ltx_Math" alttext="s_{t}" display="inline"><msub><mi>s</mi><mi>t</mi></msub></math> is <math id="S4.SS0.SSS0.Px6.p1.m2" class="ltx_Math" alttext="(Y^{\leq t},Z^{t},X)" display="inline"><mrow><mo stretchy="false">(</mo><msup><mi>Y</mi><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></msup><mo>,</mo><msup><mi>Z</mi><mi>t</mi></msup><mo>,</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></math>. The policy is parameterized as,</p>
<table id="A4.EGx11" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex11.m1" class="ltx_Math" alttext="\displaystyle\pi_{\theta}(a_{t}|s_{t})=\text{softmax}\left(f_{\theta}(h_{1},%
\bar{h}),\ldots,f_{\theta}(h_{L},\bar{h})\right)," display="inline"><mrow><mrow><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>a</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi>s</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mtext>softmax</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mn>1</mn></msub><mo>,</mo><mover accent="true"><mi>h</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>L</mi></msub><mo>,</mo><mover accent="true"><mi>h</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S4.SS0.SSS0.Px6.p1.m3" class="ltx_Math" alttext="h_{i}\in\mathbb{R}^{1024}" display="inline"><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mn>1024</mn></msup></mrow></math> is the masked language model’s output vector for position <math id="S4.SS0.SSS0.Px6.p1.m4" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>, and <math id="S4.SS0.SSS0.Px6.p1.m5" class="ltx_Math" alttext="\bar{h}\in\mathbb{R}^{1024}" display="inline"><mrow><mover accent="true"><mi>h</mi><mo>¯</mo></mover><mo>∈</mo><msup><mi>ℝ</mi><mn>1024</mn></msup></mrow></math> is a history of the previous <math id="S4.SS0.SSS0.Px6.p1.m6" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> selected positions, <math id="S4.SS0.SSS0.Px6.p1.m7" class="ltx_Math" alttext="\bar{h}=\frac{1}{k}\sum_{j=1}^{k}(\text{emb}_{\theta}(j)+h^{j}_{a_{j}})" display="inline"><mrow><mover accent="true"><mi>h</mi><mo>¯</mo></mover><mo>=</mo><mrow><mfrac><mn>1</mn><mi>k</mi></mfrac><mo>⁢</mo><mrow><msubsup><mo rspace="0em">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mtext>emb</mtext><mi>θ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msubsup><mi>h</mi><msub><mi>a</mi><mi>j</mi></msub><mi>j</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math>.
We use a 2-layer MLP for <math id="S4.SS0.SSS0.Px6.p1.m8" class="ltx_Math" alttext="f_{\theta}" display="inline"><msub><mi>f</mi><mi>θ</mi></msub></math> which concatenates its inputs and has hidden dimension of size 1024.</p>
</div>
<div id="S4.SS0.SSS0.Px6.p2" class="ltx_para">
<p class="ltx_p">Policies are trained with linear time decoding (<math id="S4.SS0.SSS0.Px6.p2.m1" class="ltx_Math" alttext="T=L" display="inline"><mrow><mi>T</mi><mo>=</mo><mi>L</mi></mrow></math>), with positions sampled from the current policy, and symbols selected greedily.
At each training iteration we sample a batch of generations, add the samples to a FIFO buffer, then perform gradient updates on batches sampled from the buffer.
We use proximal policy optimization (PPO), specifically the clipped surrogate objective from <cite class="ltx_cite ltx_citemacro_citet">Schulman<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib3" title="Proximal Policy Optimization Algorithms" class="ltx_ref">2017</a>)</cite> with a learned value function <math id="S4.SS0.SSS0.Px6.p2.m2" class="ltx_Math" alttext="V_{\theta}(s_{t})" display="inline"><mrow><msub><mi>V</mi><mi>θ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></math> to compute advantages.
This objective resulted in stable training compared to initial experiments with REINFORCE <cite class="ltx_cite ltx_citemacro_citep">(Williams, <a href="#bib.bib2" title="Simple statistical gradient-following algorithms for connectionist reinforcement learning" class="ltx_ref">1992</a>)</cite>. The value function is a 1-layer MLP, <math id="S4.SS0.SSS0.Px6.p2.m3" class="ltx_Math" alttext="V_{\theta}(\frac{1}{L}\sum_{i=1}^{L}(h_{i},\bar{h}))" display="inline"><mrow><msub><mi>V</mi><mi>θ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mfrac><mn>1</mn><mi>L</mi></mfrac><mo>⁢</mo><mrow><msubsup><mo rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo>,</mo><mover accent="true"><mi>h</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></math>.</p>
</div>
<div id="S4.SS0.SSS0.Px6.p3" class="ltx_para">
<p class="ltx_p">Training hyperparameters were selected based on validation BLEU in an initial grid search of <math id="S4.SS0.SSS0.Px6.p3.m1" class="ltx_Math" alttext="\text{generation batch size}\in\{4,16\}" display="inline"><mrow><mtext>generation batch size</mtext><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>4</mn><mo>,</mo><mn>16</mn><mo stretchy="false">}</mo></mrow></mrow></math> (sequences), <math id="S4.SS0.SSS0.Px6.p3.m2" class="ltx_Math" alttext="\text{FIFO buffer size}\in\{1k,10k\}" display="inline"><mrow><mtext>FIFO buffer size</mtext><mo>∈</mo><mrow><mo stretchy="false">{</mo><mrow><mn>1</mn><mo>⁢</mo><mi>k</mi></mrow><mo>,</mo><mrow><mn>10</mn><mo>⁢</mo><mi>k</mi></mrow><mo stretchy="false">}</mo></mrow></mrow></math> (timesteps), and <math id="S4.SS0.SSS0.Px6.p3.m3" class="ltx_Math" alttext="\text{update batch size}\in\{32,128\}" display="inline"><mrow><mtext>update batch size</mtext><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>32</mn><mo>,</mo><mn>128</mn><mo stretchy="false">}</mo></mrow></mrow></math> (timesteps). Our final model was then selected based on validation BLEU with a grid search on <math id="S4.SS0.SSS0.Px6.p3.m4" class="ltx_Math" alttext="\text{discount }\gamma\in\{0.1,0.9,0.99\}" display="inline"><mrow><mrow><mtext>discount </mtext><mo>⁢</mo><mi>γ</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0.1</mn><mo>,</mo><mn>0.9</mn><mo>,</mo><mn>0.99</mn><mo stretchy="false">}</mo></mrow></mrow></math> and <math id="S4.SS0.SSS0.Px6.p3.m5" class="ltx_Math" alttext="\text{history }k\in\{0,20,50\}" display="inline"><mrow><mrow><mtext>history </mtext><mo>⁢</mo><mi>k</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>20</mn><mo>,</mo><mn>50</mn><mo stretchy="false">}</mo></mrow></mrow></math> for each language pair, resulting in a discount <math id="S4.SS0.SSS0.Px6.p3.m6" class="ltx_Math" alttext="\gamma" display="inline"><mi>γ</mi></math> of <math id="S4.SS0.SSS0.Px6.p3.m7" class="ltx_Math" alttext="0.9" display="inline"><mn>0.9</mn></math> for both pairs, and history sizes of <math id="S4.SS0.SSS0.Px6.p3.m8" class="ltx_Math" alttext="0" display="inline"><mn>0</mn></math> for De<math id="S4.SS0.SSS0.Px6.p3.m9" class="ltx_Math" alttext="\rightarrow" display="inline"><mo stretchy="false">→</mo></math>En and <math id="S4.SS0.SSS0.Px6.p3.m10" class="ltx_Math" alttext="50" display="inline"><mn>50</mn></math> for En<math id="S4.SS0.SSS0.Px6.p3.m11" class="ltx_Math" alttext="\rightarrow" display="inline"><mo stretchy="false">→</mo></math>De.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px7" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Decoding scenarios</h4>

<div id="S4.SS0.SSS0.Px7.p1" class="ltx_para">
<p class="ltx_p">We consider two decoding scenarios: linear-time and constant-time decoding. In the linear-time scenario, the number of decoding iterations <math id="S4.SS0.SSS0.Px7.p1.m1" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> grows linearly w.r.t. the length of a target sequence <math id="S4.SS0.SSS0.Px7.p1.m2" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math>. We test setting <math id="S4.SS0.SSS0.Px7.p1.m3" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> to <math id="S4.SS0.SSS0.Px7.p1.m4" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> and <math id="S4.SS0.SSS0.Px7.p1.m5" class="ltx_Math" alttext="2L" display="inline"><mrow><mn>2</mn><mo>⁢</mo><mi>L</mi></mrow></math>. In the constant-time scenario, the number of iterations is constant w.r.t. the length of a translation, i.e., <math id="S4.SS0.SSS0.Px7.p1.m6" class="ltx_Math" alttext="T=O(1)" display="inline"><mrow><mi>T</mi><mo>=</mo><mrow><mi>O</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow></math>. At the <math id="S4.SS0.SSS0.Px7.p1.m7" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math>-th iteration of generation, we replace <math id="S4.SS0.SSS0.Px7.p1.m8" class="ltx_Math" alttext="o_{t}" display="inline"><msub><mi>o</mi><mi>t</mi></msub></math>-many symbols, where <math id="S4.SS0.SSS0.Px7.p1.m9" class="ltx_Math" alttext="o_{t}" display="inline"><msub><mi>o</mi><mi>t</mi></msub></math> is either a constant <math id="S4.SS0.SSS0.Px7.p1.m10" class="ltx_Math" alttext="\lceil L/T\rceil" display="inline"><mrow><mo stretchy="false">⌈</mo><mrow><mi>L</mi><mo>/</mo><mi>T</mi></mrow><mo stretchy="false">⌉</mo></mrow></math> or linearly anneals from <math id="S4.SS0.SSS0.Px7.p1.m11" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> to <math id="S4.SS0.SSS0.Px7.p1.m12" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math> (<math id="S4.SS0.SSS0.Px7.p1.m13" class="ltx_Math" alttext="L\to 1" display="inline"><mrow><mi>L</mi><mo stretchy="false">→</mo><mn>1</mn></mrow></math>) as done by <cite class="ltx_cite ltx_citemacro_citet">Ghazvininejad<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib13" title="Mask-predict: parallel decoding of conditional masked language models" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Linear-Time Decoding: Result and Analysis</h2>

<figure id="S5.F1" class="ltx_figure"><img src="x1.png" id="S5.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" style="font-size:90%;">Generation orders given by <span class="ltx_text ltx_font_bold">easy-first</span>, <span class="ltx_text ltx_font_bold">least2most</span>, and <span class="ltx_text ltx_font_bold">learned</span> coordinate selection.
We use greedy search with <math id="S5.F1.m2" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> iterations on the development set.
We group the orders into five clusters using and visualize cluster centers with normalized positions (x-axis) over normalized generation steps (y-axis).
The thickness of a line is proportional to the number of examples in the corresponding cluster.</span></figcaption>
</figure>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Main findings</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">We present translation quality measured by BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib8" title="BLEU: a method for automatic evaluation of machine translation" class="ltx_ref">2002</a>)</cite> in Table <a href="#S3.T1" title="Table 1 ‣ Length-conditioned beam search ‣ 3.3 Optimistic decoding and beam search from a masked language model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We identify a number of important trends.
(1) The deterministic coordinate selection strategies (<span class="ltx_text ltx_font_bold">left2right</span>, <span class="ltx_text ltx_font_bold">least2most</span>, <span class="ltx_text ltx_font_bold">easy-first</span> and <span class="ltx_text ltx_font_bold">learned</span>) significantly outperform selecting coordinates uniformly at random, by up to 3 BLEU in both directions. Deterministic coordinate selection strategies produce generations that not only have higher BLEU compared to uniform coordinate selection, but are also more likely according to the model as shown in Figures 1-2 in Appendix. The success of these relatively simple handcrafted and learned coordinate selection strategies suggest avenues for further improvement for generation from undirected sequence models.
(2) The proposed beam search algorithm for undirected sequence models provides an improvement of about 1 BLEU over greedy search, confirming the utility of the proposed framework as a way to move decoding techniques across different paradigms of sequence modeling.
(3) Rescoring generated translations
with an autoregressive model adds about 1 BLEU across all coordinate selection strategies.
Rescoring adds minimal overhead as it is run in parallel since the left-to-right constraint is enforced by masking out future tokens.
(4) Different generation strategies result in translations of varying qualities depending on the setting. <span class="ltx_text ltx_font_bold">Learned</span> and <span class="ltx_text ltx_font_bold">left2right</span> were consistently the best performing among all generation strategies. On English-German translation, <span class="ltx_text ltx_font_bold">left2right</span> is the best performing strategy slightly outperforming the <span class="ltx_text ltx_font_bold">learned</span> strategy, achieving 25.66 BLEU. On German-English translation, <span class="ltx_text ltx_font_bold">learned</span> is the best performing strategy, slightly outperforming the <span class="ltx_text ltx_font_bold">left2right</span> strategy while achieving 30.58 BLEU.
(5) We see little improvement in refining a sequence beyond the first pass.
(6) Lastly, the masked translation model is competitive with the state of the art neural autoregressive model, with a difference of less than 1 BLEU score in performance. We hypothesize that a difference between train and test settings causes a slight performance difference of the masked translation model compared to the conventional autoregressive model. In the standard autoregressive case, the model is explicitly trained to generate in left-to-right order, which matches the test time usage. By randomly selecting tokens to mask during training, our undirected sequence model is trained to follow all possible generation orders and to use context from both directions, which is not available when generating left-to-right at test time.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Adaptive generation order</h4>

<figure id="S5.T3" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_top">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math id="S5.T3.m1" class="ltx_Math" alttext="T" display="inline"><mi mathsize="90%">T</mi></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr"><math id="S5.T3.m2" class="ltx_Math" alttext="o_{t}" display="inline"><msub><mi mathsize="90%">o</mi><mi mathsize="90%">t</mi></msub></math></td>
<td class="ltx_td ltx_align_center">
<div class="ltx_inline-block ltx_transformed_outer" style="width:32.5pt;height:6.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.5pt;transform:translate(0pt,0pt) rotate(-0deg) ;">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Uniform</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center">
<div class="ltx_inline-block ltx_transformed_outer" style="width:42.3pt;height:8.1pt;vertical-align:-1.8pt;"><span class="ltx_transformed_inner" style="width:42.3pt;transform:translate(0pt,2.63pt) rotate(-0deg) ;">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Left2Right</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center">
<div class="ltx_inline-block ltx_transformed_outer" style="width:45.5pt;height:6.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:45.5pt;transform:translate(0pt,0pt) rotate(-0deg) ;">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Least2Most</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center">
<div class="ltx_inline-block ltx_transformed_outer" style="width:40.9pt;height:7.899999999999999pt;vertical-align:-1.8pt;"><span class="ltx_transformed_inner" style="width:40.9pt;transform:translate(0pt,2.63pt) rotate(-0deg) ;">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Easy-First</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center">
<div class="ltx_inline-block ltx_transformed_outer" style="width:41.7pt;height:6.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:41.7pt;transform:translate(0pt,0pt) rotate(-0deg) ;">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Hard-First</span></p>
</span></div>
</td>
<td class="ltx_td ltx_align_center">
<div class="ltx_inline-block ltx_transformed_outer" style="width:31.7pt;height:6.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:31.7pt;transform:translate(0pt,0pt) rotate(-0deg) ;">
<p class="ltx_p"><span class="ltx_text" style="font-size:90%;">Learned</span></p>
</span></div>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt"><math id="S5.T3.m3" class="ltx_Math" alttext="10" display="inline"><mn mathsize="90%">10</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><math id="S5.T3.m4" class="ltx_Math" alttext="L\to 1" display="inline"><mrow><mi mathsize="90%">L</mi><mo mathsize="90%" stretchy="false">→</mo><mn mathsize="90%">1</mn></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">22.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">22.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">27.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">22.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">26.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">12.70</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math id="S5.T3.m5" class="ltx_Math" alttext="10" display="inline"><mn mathsize="90%">10</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr">
<math id="S5.T3.m6" class="ltx_Math" alttext="L\to 1" display="inline"><mrow><mi mathsize="90%">L</mi><mo mathsize="90%" stretchy="false">→</mo><mn mathsize="90%">1</mn></mrow></math><span class="ltx_text" style="font-size:90%;">*</span>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">23.64</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">23.64</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">28.63</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">23.79</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">28.46</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">13.18</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math id="S5.T3.m7" class="ltx_Math" alttext="10" display="inline"><mn mathsize="90%">10</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr"><math id="S5.T3.m8" class="ltx_Math" alttext="\lceil L/T\rceil" display="inline"><mrow><mo maxsize="90%" minsize="90%">⌈</mo><mrow><mi mathsize="90%">L</mi><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">T</mi></mrow><mo maxsize="90%" minsize="90%">⌉</mo></mrow></math></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">22.43</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">21.92</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.69</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">25.16</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">23.46</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">26.47</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S5.T3.m9" class="ltx_Math" alttext="20" display="inline"><mn mathsize="90%">20</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><math id="S5.T3.m10" class="ltx_Math" alttext="L\to 1" display="inline"><mrow><mi mathsize="90%">L</mi><mo mathsize="90%" stretchy="false">→</mo><mn mathsize="90%">1</mn></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">26.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">26.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">28.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">22.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">28.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">12.85</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math id="S5.T3.m11" class="ltx_Math" alttext="20" display="inline"><mn mathsize="90%">20</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr">
<math id="S5.T3.m12" class="ltx_Math" alttext="L\to 1" display="inline"><mrow><mi mathsize="90%">L</mi><mo mathsize="90%" stretchy="false">→</mo><mn mathsize="90%">1</mn></mrow></math><span class="ltx_text" style="font-size:90%;">*</span>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">27.28</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">27.28</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">30.13</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.55</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">29.82</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">13.19</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math id="S5.T3.m13" class="ltx_Math" alttext="20" display="inline"><mn mathsize="90%">20</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr"><math id="S5.T3.m14" class="ltx_Math" alttext="\lceil L/T\rceil" display="inline"><mrow><mo maxsize="90%" minsize="90%">⌈</mo><mrow><mi mathsize="90%">L</mi><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">T</mi></mrow><mo maxsize="90%" minsize="90%">⌉</mo></mrow></math></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.69</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">25.94</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">27.01</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">27.49</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">25.56</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">27.82</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Constant-time machine translation on WMT’14 De<math id="S5.T3.m18" class="ltx_Math" alttext="\rightarrow" display="inline"><mo stretchy="false">→</mo></math>En with different settings of the budget (<math id="S5.T3.m19" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math>) and number of tokens predicted each iteration (<math id="S5.T3.m20" class="ltx_Math" alttext="o_{t}" display="inline"><msub><mi>o</mi><mi>t</mi></msub></math>). * denotes rescoring generated hypotheses using autoregressive model instead of proposed model.
</figcaption>
</figure>
<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">least2most</span>, <span class="ltx_text ltx_font_bold">easy-first</span>, and <span class="ltx_text ltx_font_bold">learned</span> generation strategies automatically adapt the generation order based on the intermediate sequences generated.
We investigate the resulting generation orders on the development set by presenting each as a 10-dim vector (downsampling as necessary), where each element corresponds to the selected position in the target sequence normalized by sequence length. We cluster these sequences with <math id="S5.SS0.SSS0.Px2.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-means clustering and visualize the clusters centers as curves with thickness proportional to the number of sequences in the cluster in Fig. <a href="#S5.F1" title="Figure 1 ‣ 5 Linear-Time Decoding: Result and Analysis ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S5.SS0.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p">The visualization reveals that many sequences are generated monotonically, either left-to-right or right-to-left (see, e.g., <span class="ltx_text" style="color:#00FF00;"> green</span>, <span class="ltx_text" style="color:#800080;"> purple</span> and <span class="ltx_text" style="color:#FF8000;"> orange</span> clusters in <span class="ltx_text ltx_font_bold">easy-first</span>, De<math id="S5.SS0.SSS0.Px2.p2.m1" class="ltx_Math" alttext="\to" display="inline"><mo stretchy="false">→</mo></math>En, and <span class="ltx_text" style="color:#FF8000;"> orange</span>, <span class="ltx_text" style="color:#0000FF;"> blue</span>, and <span class="ltx_text" style="color:#FF0000;"> red</span> clusters in <span class="ltx_text ltx_font_bold">learned</span>, En<math id="S5.SS0.SSS0.Px2.p2.m2" class="ltx_Math" alttext="\to" display="inline"><mo stretchy="false">→</mo></math>De).
For the <span class="ltx_text ltx_font_bold">easy-first</span> and <span class="ltx_text ltx_font_bold">least2most</span> strategies, we additionally identify clusters of sequences that are generated from outside in (e.g., <span class="ltx_text" style="color:#0000FF;"> blue</span> and <span class="ltx_text" style="color:#FF0000;"> red</span> clusters in <span class="ltx_text ltx_font_bold">easy-first</span>, De<math id="S5.SS0.SSS0.Px2.p2.m3" class="ltx_Math" alttext="\to" display="inline"><mo stretchy="false">→</mo></math>En, and <span class="ltx_text" style="color:#FF0000;"> red</span> and <span class="ltx_text" style="color:#800080;"> purple</span> clusters in <span class="ltx_text ltx_font_bold">least2most</span>, En<math id="S5.SS0.SSS0.Px2.p2.m4" class="ltx_Math" alttext="\to" display="inline"><mo stretchy="false">→</mo></math>De).</p>
</div>
<div id="S5.SS0.SSS0.Px2.p3" class="ltx_para">
<p class="ltx_p">On De<math id="S5.SS0.SSS0.Px2.p3.m1" class="ltx_Math" alttext="\to" display="inline"><mo stretchy="false">→</mo></math>En, in roughly 75% of the generations, the <span class="ltx_text ltx_font_bold">learned</span> policy either generated from left-to-right (<span class="ltx_text" style="color:#FF8000;">orange</span>) or generated the final token, typically punctuation, followed by left-to-right generation (<span class="ltx_text" style="color:#00FF00;">green</span>).
In the remaining 25% of generations, the <span class="ltx_text ltx_font_bold">learned</span> policy generates with variations of an outside-in strategy (<span class="ltx_text" style="color:#FF0000;">red</span>, <span class="ltx_text" style="color:#0000FF;">blue</span>, <span class="ltx_text" style="color:#BF0040;">purple</span>).
See Appendix Figures 7-9 for examples.
On En<math id="S5.SS0.SSS0.Px2.p3.m2" class="ltx_Math" alttext="\to" display="inline"><mo stretchy="false">→</mo></math>De, the <span class="ltx_text ltx_font_bold">learned</span> policy has a higher rate of left-to-right generation, with roughly 85% of generations using a left-to-right variation (<span class="ltx_text" style="color:#0000FF;">blue</span>, <span class="ltx_text" style="color:#FF8000;">orange</span>).
These variations are however typically not strictly monotonic; the learned policy usually starts with the final token, and often skips tokens in the left-to-right order before generating them at a later time.
We hypothesize that the learned policy tends towards variations of left-to-right since (a) left-to-right may be an easy strategy to learn, yet (b) left-to-right achieves reasonable performance.</p>
</div>
<div id="S5.SS0.SSS0.Px2.p4" class="ltx_para">
<p class="ltx_p">In general, we explain the tendency towards either monotonic or outside-in generation
by the availability of contextual evidence, or lack thereof. At the beginning of generation, the only two non-mask symbols are the beginning and end of sentence symbols, making it easier to predict a symbol at the beginning or end of the sentence. As more symbols are filled near the boundaries, more evidence is accumulated for the decoding strategy to accurately predict symbols near the center. This process manifests either as monotonic or outside-in generation.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Constant-Time Decoding: Result and Analysis</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">The trends in constant-time decoding noticeably differ from those in linear-time decoding. First, the <span class="ltx_text ltx_font_bold">left2right</span> strategy performs comparably worse compared to the best performing strategies in constant-time decoding. The performance gap is wider (up to 4.8 BLEU) with a tighter budget (<math id="S6.p1.m1" class="ltx_Math" alttext="T=10" display="inline"><mrow><mi>T</mi><mo>=</mo><mn>10</mn></mrow></math>). Second, the <span class="ltx_text ltx_font_bold">learned</span> coordinate selection strategy performs best when generating <math id="S6.p1.m2" class="ltx_Math" alttext="\lceil L/T\rceil" display="inline"><mrow><mo stretchy="false">⌈</mo><mrow><mi>L</mi><mo>/</mo><mi>T</mi></mrow><mo stretchy="false">⌉</mo></mrow></math> symbols every iteration, despite only being trained with linear-time decoding, but performs significantly worse when annealing the number of generated symbols from <math id="S6.p1.m3" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> to <math id="S6.p1.m4" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math>. This could be explained by the fact that the learned policy was never trained to refine predicted symbols, which is the case in <math id="S6.p1.m5" class="ltx_Math" alttext="L\to 1" display="inline"><mrow><mi>L</mi><mo stretchy="false">→</mo><mn>1</mn></mrow></math> constant-time decoding. Third, <span class="ltx_text ltx_font_bold">easy-first</span> is the second-best performing strategy in the <math id="S6.p1.m6" class="ltx_Math" alttext="\lceil L/T\rceil" display="inline"><mrow><mo stretchy="false">⌈</mo><mrow><mi>L</mi><mo>/</mo><mi>T</mi></mrow><mo stretchy="false">⌉</mo></mrow></math> setting, but similarly to the <span class="ltx_text ltx_font_bold">learned</span> strategy it performs worse in the <math id="S6.p1.m7" class="ltx_Math" alttext="L\to 1" display="inline"><mrow><mi>L</mi><mo stretchy="false">→</mo><mn>1</mn></mrow></math> setting. This may be because in the <math id="S6.p1.m8" class="ltx_Math" alttext="L\to 1" display="inline"><mrow><mi>L</mi><mo stretchy="false">→</mo><mn>1</mn></mrow></math> setting it is preferable to first generate hard-to-predict symbols and have multiple attempts at refining them, rather than predicting hard tokens at the end of generation process and not getting an opportunity to refine them, as is done in <span class="ltx_text ltx_font_bold">easy-first</span> scenario. To verify this hypothesis, we test a <span class="ltx_text ltx_font_bold">hard-first</span> strategy where we flip the signs of the coefficients of <span class="ltx_text ltx_font_bold">easy-first</span> in the log-linear model. This new <span class="ltx_text ltx_font_bold">hard-first</span> strategy works on par with <span class="ltx_text ltx_font_bold">least2most</span>, again confirming that decoding strategies must be selected based on the target tasks and decoding setting.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p">With a fixed budget of <math id="S6.p2.m1" class="ltx_Math" alttext="T=20" display="inline"><mrow><mi>T</mi><mo>=</mo><mn>20</mn></mrow></math>, linearly annealing <math id="S6.p2.m2" class="ltx_Math" alttext="o_{t}" display="inline"><msub><mi>o</mi><mi>t</mi></msub></math> from <math id="S6.p2.m3" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> to <math id="S6.p2.m4" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math>, and <span class="ltx_text ltx_font_bold">least2most</span> decoding, constant-time translation can achieve translation quality comparable to linear-time translation with the same model (30.13 vs. 30.58), and to beam-search translations using the strong neural autoregressive model (30.13 vs 30.92).
Even with a tighter budget of <math id="S6.p2.m5" class="ltx_Math" alttext="10" display="inline"><mn>10</mn></math> iterations (less than half the average sentence length), constant-time translation loses only 1.8 BLEU points (28.63 vs. 30.58), which confirms the finding by <cite class="ltx_cite ltx_citemacro_citet">Ghazvininejad<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib13" title="Mask-predict: parallel decoding of conditional masked language models" class="ltx_ref">2019</a>)</cite> and offers new opportunities in advancing constant-time machine translation systems. Compared to other constant-time machine translation approaches, our model outperforms many recent approaches by <cite class="ltx_cite ltx_citemacro_citet">Gu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib33" title="Non-autoregressive neural machine translation" class="ltx_ref">2018</a>); Lee<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib26" title="Deterministic non-autoregressive neural sequence modeling by iterative refinement" class="ltx_ref">2018</a>); Wang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib5" title="Non-autoregressive machine translation with auxiliary regularization" class="ltx_ref">2019</a>); Ma<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib7" title="FlowSeq: non-autoregressive conditional sequence generation with generative flow" class="ltx_ref">2019</a>)</cite>, while being comparable to <cite class="ltx_cite ltx_citemacro_citet">Ghazvininejad<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib13" title="Mask-predict: parallel decoding of conditional masked language models" class="ltx_ref">2019</a>); Shu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib6" title="Latent-variable non-autoregressive neural machine translation with deterministic inference using a delta posterior" class="ltx_ref">2019</a>)</cite>. We present full table comparing performance of various constant-time decoding approaches in Table 1 in Appendix.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">We present a generalized framework of neural sequence generation that unifies decoding in directed and undirected neural sequence models. Under this framework, we separate position selection and symbol replacement, allowing us to apply a diverse set of generation algorithms, inspired by those for directed neural sequence models, to undirected models such as BERT and its translation variant.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p">We evaluate these generation strategies on WMT’14 En-De machine translation using a recently proposed masked translation model.
Our experiments reveal that undirected neural sequence models achieve performance comparable to conventional, state-of-the-art autoregressive models, given an appropriate choice of decoding strategy.
We further show that constant-time translation in these models performs similar to linear-time translation by using one of the proposed generation strategies. Analysis of the generation order automatically determined by these adaptive decoding strategies reveals that most sequences are generated either monotonically or outside-in.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p class="ltx_p">We only apply our framework to the problem of sequence generation. As one extension of our work, we could also apply it to other structured data such as grids (for e.g. images) and arbitrary graphs. Overall, we hope that our generalized framework opens new avenues in developing and understanding generation algorithms for a variety of settings.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="bib.L1" class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Berglund, T. Raiko, M. Honkala, L. Kärkkäinen, A. Vetek, and J. T. Karhunen (2015)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bidirectional recurrent neural networks as generative models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 856–864</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p1" title="3.1 BERT as an undirected sequence model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>.
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Besag (1977)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Efficiency of pseudolikelihood estimation for simple gaussian fields</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p1" title="3.1 BERT as an undirected sequence model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>.
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Devlin, M. Chang, and K. T. Kenton Lee (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">BERT: pre-training of deep bidirectional transformers for language understanding</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">NAACL</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models</span></span>,
<a href="#S1.p1" title="1 Introduction ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S3.SS1.p1" title="3.1 BERT as an undirected sequence model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>.
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Edunov, A. Baevski, and M. Auli (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Pre-trained language model representations for language generation</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1903.09722</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p3" title="3.1 BERT as an undirected sequence model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>.
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Ghazvininejad, O. Levy, Y. Liu, and L. Zettlemoyer (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mask-predict: parallel decoding of conditional masked language models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1904.09324</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A1.p1" title="Appendix A Comparison with other non-autoregressive neural machine translation approaches ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix A</span></a>,
<a href="#A4.T4" title="In Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a href="#S1.p5" title="1 Introduction ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S3.SS1.p2" title="3.1 BERT as an undirected sequence model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>,
<a href="#S4.I1.i3.p1" title="In Handcrafted decoding strategies ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">item 3</span></a>,
<a href="#S4.SS0.SSS0.Px2.p2" title="Sequence models ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>,
<a href="#S4.SS0.SSS0.Px7.p1" title="Decoding scenarios ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>,
<a href="#S6.p2" title="6 Constant-Time Decoding: Result and Analysis ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6</span></a>.
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Gu, J. Bradbury, C. Xiong, V. O. K. Li, and R. Socher (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Non-autoregressive neural machine translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1711.02281</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A4.T4" title="In Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a href="#S6.p2" title="6 Constant-Time Decoding: Result and Analysis ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6</span></a>.
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Gu, Q. Liu, and K. Cho (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Insertion-based decoding with automatically inferred generation order</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1902.01370</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A2.p1" title="Appendix B Non-monotonic neural sequence models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix B</span></a>.
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. Hendrycks and K. Gimpel (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1606.08415,</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS0.SSS0.Px2.p1" title="Sequence models ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. P. Kingma and J. Ba (2014)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Adam: a method for stochastic optimization</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint 1412.6980</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS0.SSS0.Px4.p1" title="Training details ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, <span class="ltx_text ltx_bib_etal">et al.</span> (2007)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Moses: open source toolkit for statistical machine translation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ACL</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS0.SSS0.Px1.p1" title="Data and preprocessing ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">G. Lample and A. Conneau (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cross-lingual language model pretraining</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1901.07291</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models</span></span>,
<a href="#S1.p4" title="1 Introduction ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S3.SS1.p3" title="3.1 BERT as an undirected sequence model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>,
<a href="#S4.SS0.SSS0.Px2.p1" title="Sequence models ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>,
<a href="#S4.SS0.SSS0.Px2.p2" title="Sequence models ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Lee, E. Mansimov, and K. Cho (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deterministic non-autoregressive neural sequence modeling by iterative refinement</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1802.06901</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A4.T4" title="In Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a href="#S2.SS1.SSS0.Px2.p1" title="Non-autoregressive neural sequence modeling by iterative refinement ‣ 2.1 Special Cases ‣ 2 A Generalized Framework of Sequence Generation ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.1</span></a>,
<a href="#S6.p2" title="6 Constant-Time Decoding: Result and Analysis ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6</span></a>.
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. A. Levine and G. Casella (2006)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Optimizing random scan gibbs samplers</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Multivariate Analysis</span> <span class="ltx_text ltx_bib_volume">97</span> (<span class="ltx_text ltx_bib_number">10</span>), <span class="ltx_text ltx_bib_pages"> pp. 2071–2100</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS0.Px2.p1" title="Adaptive Gibbs sampling: non-uniform coordinate selection ‣ 3.2 Gibbs sampling in the generalized sequence generation model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2</span></a>.
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">X. Ma, C. Zhou, X. Li, G. Neubig, and E. Hovy (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">FlowSeq: non-autoregressive conditional sequence generation with generative flow</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint 1909.02480</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A4.T4" title="In Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a href="#S6.p2" title="6 Constant-Time Decoding: Result and Analysis ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6</span></a>.
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Papineni, S. Roukos, T. Ward, and W. Zhu (2002)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">BLEU: a method for automatic evaluation of machine translation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 40th annual meeting on association for computational linguistics</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 311–318</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS0.SSS0.Px1.p1" title="Main findings ‣ 5 Linear-Time Decoding: Result and Analysis ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">P. Ramachandran, P. Liu, and Q. Le (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised pretraining for sequence to sequence learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Proceedings of the 2017 Conference on Empirical Methods in Natural
Language Processing</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.18653/v1/d17-1039" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="https://dx.doi.org/10.18653/v1/d17-1039" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p3" title="3.1 BERT as an undirected sequence model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>.
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Proximal Policy Optimization Algorithms</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1707.06347</span>,
<a href="http://arxiv.org/abs/1707.06347" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS0.SSS0.Px6.p2" title="Learned decoding strategies ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Sennrich, B. Haddow, and A. Birch (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Neural machine translation of rare words with subword units</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ACL</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS0.SSS0.Px1.p1" title="Data and preprocessing ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Shu, J. Lee, H. Nakayama, and K. Cho (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Latent-variable non-autoregressive neural machine translation with deterministic inference using a delta posterior</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint 1908.07181</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A4.T4" title="In Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a href="#S6.p2" title="6 Constant-Time Decoding: Result and Analysis ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6</span></a>.
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Song, X. Tan, T. Qin, J. Lu, and T. Liu (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MASS: masked sequence to sequence pre-training for language generation</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1905.02450</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p3" title="3.1 BERT as an undirected sequence model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>.
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov (2014)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dropout: a simple way to prevent neural networks from overfitting</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS0.SSS0.Px4.p1" title="Training details ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Stern, W. Chan, J. Kiros, and J. Uszkoreit (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Insertion transformer: flexible sequence generation via insertion operations</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1902.03249</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A2.p1" title="Appendix B Non-monotonic neural sequence models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix B</span></a>.
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Q. Sun, S. Lee, and D. Batra (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bidirectional beam search: forward-backward inference in neural sequence models for fill-in-the-blank image captioning</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p1" title="3.1 BERT as an undirected sequence model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>.
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Attention is all you need</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">NIPS</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A4.T4" title="In Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a href="#S1.p4" title="1 Introduction ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S4.SS0.SSS0.Px2.p1" title="Sequence models ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>,
<a href="#S4.SS0.SSS0.Px3.p1" title="Baseline model ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Wang and K. Cho (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">BERT has a mouth, and it must speak: bert as a markov random field language model</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1902.04094</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p1" title="3.1 BERT as an undirected sequence model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>,
<a href="#S3.SS1.p2" title="3.1 BERT as an undirected sequence model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>,
<a href="#S4.SS0.SSS0.Px5.p2" title="Handcrafted decoding strategies ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Wang, J. Zhang, and H. Chen (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Semi-autoregressive neural machine translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1808.08583</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS1.SSS0.Px3.p1" title="Semi-autoregressive neural sequence models ‣ 2.1 Special Cases ‣ 2 A Generalized Framework of Sequence Generation ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Wang, F. Tian, D. He, T. Qin, C. Zhai, and T. Liu (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Non-autoregressive machine translation with auxiliary regularization</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint 1902.10245</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A4.T4" title="In Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a href="#S6.p2" title="6 Constant-Time Decoding: Result and Analysis ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6</span></a>.
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Welleck, K. Brantley, H. D. III, and K. Cho (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Non-monotonic sequential text generation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 36th International Conference on Machine Learning</span>,  <span class="ltx_text ltx_bib_editor">K. Chaudhuri and R. Salakhutdinov (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Proceedings of Machine Learning Research</span>, Vol. <span class="ltx_text ltx_bib_volume">97</span>, <span class="ltx_text ltx_bib_place">Long Beach, California, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 6716–6726</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://proceedings.mlr.press/v97/welleck19a.html" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A2.p1" title="Appendix B Non-monotonic neural sequence models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix B</span></a>.
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. J. Williams (1992)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Simple statistical gradient-following algorithms for connectionist reinforcement learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Machine Learning</span> <span class="ltx_text ltx_bib_volume">8</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 229–256</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 1573-0565</span>,
<a href="https://dx.doi.org/10.1007/BF00992696" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="https://doi.org/10.1007/BF00992696" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS0.Px3.p1" title="Adaptive Gibbs sampling: learned coordinate selection ‣ 3.2 Gibbs sampling in the generalized sequence generation model ‣ 3 Decoding from Masked Language Models ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a href="#S4.SS0.SSS0.Px6.p2" title="Learned decoding strategies ‣ 4 Experimental Settings ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Comparison with other non-autoregressive neural machine translation approaches</h2>

<div id="A1.p1" class="ltx_para">
<p class="ltx_p">We present the comparison of results of our approach with other constant-time machine translation approaches in Table <a href="#A4.T4" title="Table 4 ‣ Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Our model is most similar to conditional model by  <cite class="ltx_cite ltx_citemacro_citep">(Ghazvininejad<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib13" title="Mask-predict: parallel decoding of conditional masked language models" class="ltx_ref">2019</a>)</cite>. However, there are differences in both model and training hyperparameters between our work and work by <cite class="ltx_cite ltx_citemacro_citep">(Ghazvininejad<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib13" title="Mask-predict: parallel decoding of conditional masked language models" class="ltx_ref">2019</a>)</cite>. We use smaller Transformer model with 1024 hidden units vs 2048 units in <cite class="ltx_cite ltx_citemacro_citep">(Ghazvininejad<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib13" title="Mask-predict: parallel decoding of conditional masked language models" class="ltx_ref">2019</a>)</cite>. We also train the model with more than twice smaller batch size since we use 8 GPUs on DGX-1 machine and <cite class="ltx_cite ltx_citemacro_citep">(Ghazvininejad<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib13" title="Mask-predict: parallel decoding of conditional masked language models" class="ltx_ref">2019</a>)</cite> use 16 GPUs on two DGX-1 machine with float16 precision. Finally we don’t average best 5 checkpoints and don’t use label smoothing for our model.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Non-monotonic neural sequence models</h2>

<div id="A2.p1" class="ltx_para">
<p class="ltx_p">The proposed generalized framework subsumes recently proposed variants of non-monotonic generation <cite class="ltx_cite ltx_citemacro_citep">(Welleck<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib27" title="Non-monotonic sequential text generation" class="ltx_ref">2019</a>; Stern<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib28" title="Insertion transformer: flexible sequence generation via insertion operations" class="ltx_ref">2019</a>; Gu<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib29" title="Insertion-based decoding with automatically inferred generation order" class="ltx_ref">2019</a>)</cite>. Unlike the other special cases described above, these non-monotonic generation approaches learn not only the symbol replacement distribution but also the coordinate selection distribution, and implicitly the length distribution, from data. Because the length of a sequence is often not decided in advance, the intermediate coordinate sequence <math id="A2.p1.m1" class="ltx_Math" alttext="Z^{t}" display="inline"><msup><mi>Z</mi><mi>t</mi></msup></math> and the coordinate selection distribution are reparameterized to work with relative coordinates rather than absolute coordinates. We do not go into details of these recent algorithms, but we emphasize that all these approaches are special cases of the proposed framework, which further suggests other variants of non-monotonic generation.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Energy evolution over generation steps</h2>

<div id="A3.p1" class="ltx_para">
<p class="ltx_p">While the results in Table 1 in paper indicate that our decoding algorithms find better generations in terms of BLEU relative to uniform decoding, we verify that the algorithms produce generations that are more likely according to the model.
We do so by computing the energy (negative logit) of the sequence of intermediate sentences generated while using an algorithm, and comparing to the average energy of intermediate sentences generated by picking positions uniformly at random.
We plot this energy difference over decoding in Figure <a href="#A4.F2" title="Figure 2 ‣ Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We additionally plot the evolution of energy of the sequence by different position selection algorithms throughout generation process in Figure <a href="#A4.F3" title="Figure 3 ‣ Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Overall, we find that left-to-right, least-to-most, and easy-first do find sentences that are lower energy than the uniform baseline over the entire decoding process.
Easy-first produces sentences with the lowest energy, followed by least-to-most, and then left-to-right.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Sample sequences and their generation orders</h2>

<div id="A4.p1" class="ltx_para">
<p class="ltx_p">We present sample decoding processes on De<math id="A4.p1.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo stretchy="false">→</mo></math>En with <math id="A4.p1.m2" class="ltx_Math" alttext="b=1,T=L" display="inline"><mrow><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>T</mi><mo>=</mo><mi>L</mi></mrow></mrow></math> using the <span class="ltx_text ltx_font_bold">easy-first</span> decoding algorithm in Figures <a href="#A4.F4" title="Figure 4 ‣ Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, <a href="#A4.F5" title="Figure 5 ‣ Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, <a href="#A4.F6" title="Figure 6 ‣ Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, and <a href="#A4.F7" title="Figure 7 ‣ Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, and the <span class="ltx_text ltx_font_bold">learned</span> decoding algorithm in Figures <a href="#A4.F8" title="Figure 8 ‣ Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, <a href="#A4.F9" title="Figure 9 ‣ Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, and <a href="#A4.F10" title="Figure 10 ‣ Appendix D Sample sequences and their generation orders ‣ A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.
For easy-first decoding, we highlight examples decoding in right-to-left-to-right-to-left order, outside-in, left-to-right, and right-to-left orders, which respectively correspond to the <span class="ltx_text" style="color:#FF8000;"> orange</span>, <span class="ltx_text" style="color:#BF0040;"> purple</span>, <span class="ltx_text" style="color:#FF0000;"> red</span>, and <span class="ltx_text" style="color:#0000FF;"> blue</span> clusters from Figure 1 in the main paper.
For learned decoding, we highlight examples with right-to-left-to-right, outside-in, and left-to-right orders, corresponding to the <span class="ltx_text" style="color:#0000FF;"> blue</span>, <span class="ltx_text" style="color:#FF0000;"> red</span>, and <span class="ltx_text" style="color:#00FF00;"> green</span> clusters.
The examples demonstrate the ability of the coordinate selection strategies to adapt the generation order based on the intermediate sequences generated. Even in the cases of largely monotonic generation order (left-to-right and right-to-left), each algorithm has the capacity to make small changes to the generation order as needed.</p>
</div>
<figure id="A4.T4" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_tt"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span class="ltx_text ltx_font_bold" style="font-size:90%;">WMT2014</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Models</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">EN-DE</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">DE-EN</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt">
<span class="ltx_text" style="font-size:90%;">AR Transformer-base </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span>Vaswani<span class="ltx_text ltx_bib_etal"> et al.</span><span class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib43" title="Attention is all you need" class="ltx_ref">2017</a><span class="ltx_text" style="font-size:90%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">27.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">–</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt">
<span class="ltx_text" style="font-size:90%;">AR </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span>Gu<span class="ltx_text ltx_bib_etal"> et al.</span><span class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib33" title="Non-autoregressive neural machine translation" class="ltx_ref">2018</a><span class="ltx_text" style="font-size:90%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">23.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">–</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">NAR (+Distill +FT +NPD S=100)</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">21.61</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">–</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt">
<span class="ltx_text" style="font-size:90%;">AR </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span>Lee<span class="ltx_text ltx_bib_etal"> et al.</span><span class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib26" title="Deterministic non-autoregressive neural sequence modeling by iterative refinement" class="ltx_ref">2018</a><span class="ltx_text" style="font-size:90%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">24.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">28.47</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Adaptive NAR Model</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">16.56</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">–</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Adaptive NAR Model (+Distill)</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">21.54</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">25.43</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt">
<span class="ltx_text" style="font-size:90%;">AR </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span>Wang<span class="ltx_text ltx_bib_etal"> et al.</span><span class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib5" title="Non-autoregressive machine translation with auxiliary regularization" class="ltx_ref">2019</a><span class="ltx_text" style="font-size:90%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">27.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">31.29</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">NAT-REG (+Distill)</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">20.65</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.77</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">NAT-REG (+Distill +AR rescoring)</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.61</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">28.90</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt">
<span class="ltx_text" style="font-size:90%;">AR </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span>Ghazvininejad<span class="ltx_text ltx_bib_etal"> et al.</span><span class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib13" title="Mask-predict: parallel decoding of conditional masked language models" class="ltx_ref">2019</a><span class="ltx_text" style="font-size:90%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">27.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">31.09</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">CMLM with 4 iterations</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">22.25</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">–</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">CMLM with 4 iterations (+Distill)</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">25.94</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">29.90</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">CMLM with 10 iterations</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.61</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">–</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">CMLM with 10 iterations (+Distill)</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">27.03</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">30.53</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt">
<span class="ltx_text" style="font-size:90%;">AR </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span>Shu<span class="ltx_text ltx_bib_etal"> et al.</span><span class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib6" title="Latent-variable non-autoregressive neural machine translation with deterministic inference using a delta posterior" class="ltx_ref">2019</a><span class="ltx_text" style="font-size:90%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">26.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">–</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Latent-Variable NAR</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">11.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">–</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Latent-Variable NAR (+Distill)</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">22.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">–</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Latent-Variable NAR (+Distill +AR Rescoring)</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">25.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">–</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt">
<span class="ltx_text" style="font-size:90%;">AR </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:90%;">(</span>Ma<span class="ltx_text ltx_bib_etal"> et al.</span><span class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib7" title="FlowSeq: non-autoregressive conditional sequence generation with generative flow" class="ltx_ref">2019</a><span class="ltx_text" style="font-size:90%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">27.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">31.44</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">FlowSeq-base (+NPD n = 30)</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">21.15</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">26.04</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">FlowSeq-base (+Distill +NPD n = 30)</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">23.48</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">28.40</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt"><span class="ltx_text" style="font-size:90%;">AR (ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">26.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">30.92</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Contant-time 10 iterations</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">21.98</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">27.14</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Contant-time 10 iterations (+AR Rescoring)</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">24.53</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">28.63</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:90%;">Contant-time 20 iterations</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">23.92</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">28.54</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text" style="font-size:90%;">Contant-time 20 iterations (+AR Rescoring)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">25.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">30.13</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>BLEU scores on WMT’14 En<math id="A4.T4.m3" class="ltx_Math" alttext="\rightarrow" display="inline"><mo stretchy="false">→</mo></math>De and De<math id="A4.T4.m4" class="ltx_Math" alttext="\rightarrow" display="inline"><mo stretchy="false">→</mo></math>En datasets showing performance of various constant-time machine translation approaches. Each block shows the performance of autoregressive model baseline with their proposed approach. AR denotes autoregressive model. Distill denotes distillation. AR rescoring denotes rescoring of samples with autoregressive model. FT denotes fertility. NPD denotes noisy parallel decoding followed by rescoring with autoregressive model. </figcaption>
</figure>
<figure id="A4.F2" class="ltx_figure"><img src="x2.png" id="A4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="275" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" style="font-size:90%;">Average difference in energy <math id="A4.F2.m2" class="ltx_Math" alttext="\uparrow" display="inline"><mo stretchy="false">↑</mo></math> between sequences generated by selecting positions uniformly at random versus by different algorithms, over the course of decoding.</span></figcaption>
</figure>
<figure id="A4.F3" class="ltx_figure"><img src="x3.png" id="A4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="224" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" style="font-size:90%;">Evolution of the energy of the sequence <math id="A4.F3.m2" class="ltx_Math" alttext="\downarrow" display="inline"><mo stretchy="false">↓</mo></math> over the course of decoding by different position selection algorithms.</span></figcaption>
</figure>
<figure id="A4.F4" class="ltx_figure">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;">Iteration</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;color:#FF8000;">Right-to-Left-to-Right-to-Left</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">(source)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">Würde es mir je gelingen , an der Universität Oxford ein normales Leben zu führen ?</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">Oxford</span><span class="ltx_text" style="font-size:80%;"> ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">ever</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ _ _ _ _ _ Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">I</span><span class="ltx_text" style="font-size:80%;"> ever _ _ _ _ _ _ _ _ _ _ _ Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">5</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ I ever _ _ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">of</span><span class="ltx_text" style="font-size:80%;"> Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">6</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold" style="font-size:80%;">Would</span><span class="ltx_text" style="font-size:80%;"> I ever _ _ _ _ _ _ _ _ _ _ of Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">7</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Would I ever _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">normal</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ of Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">8</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Would I ever _ _ _ _ _ normal _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">at</span><span class="ltx_text" style="font-size:80%;"> _ _ of Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">9</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Would I ever _ _ _ _ _ normal _ at </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">the</span><span class="ltx_text" style="font-size:80%;"> _ of Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Would I ever _ _ _ _ _ normal _ at the </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">University</span><span class="ltx_text" style="font-size:80%;"> of Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">11</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Would I ever _ _ _ _ _ normal </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">life</span><span class="ltx_text" style="font-size:80%;"> at the University of Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">12</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Would I ever _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">live</span><span class="ltx_text" style="font-size:80%;"> _ normal life at the University of Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">13</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Would I ever _ _ _ live </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">a</span><span class="ltx_text" style="font-size:80%;"> normal life at the University of Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">14</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Would I ever _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">able</span><span class="ltx_text" style="font-size:80%;"> _ live a normal life at the University of Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">15</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Would I ever </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">be</span><span class="ltx_text" style="font-size:80%;"> able _ live a normal life at the University of Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">16</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Would I ever be able </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">to</span><span class="ltx_text" style="font-size:80%;"> live a normal life at the University of Oxford ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">(target)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">Would I ever be able to lead a normal life at Oxford ?</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:113%;">Figure 4</span>: </span><span class="ltx_text" style="font-size:113%;">Example sentences generated following an <span class="ltx_text" style="color:#FF8000;"> right-to-left-to-right-to-left</span> generation order, using the <span class="ltx_text ltx_font_bold">easy-first</span> decoding algorithm on De<math id="A4.F4.m2" class="ltx_Math" alttext="\rightarrow" display="inline"><mo stretchy="false">→</mo></math>En.</span></figcaption>
</figure>
<figure id="A4.F5" class="ltx_figure">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;">Iteration</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;color:#800080;">Outside-In</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">(source)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">Doch ohne zivilgesellschaftliche Organisationen könne eine Demokratie nicht funktionieren .</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">.</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">cannot</span><span class="ltx_text" style="font-size:80%;"> _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">democracy</span><span class="ltx_text" style="font-size:80%;"> cannot _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">without</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ democracy cannot _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">5</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ without _ _ _ _ _ democracy cannot </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">work</span><span class="ltx_text" style="font-size:80%;"> .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">6</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold" style="font-size:80%;">But</span><span class="ltx_text" style="font-size:80%;"> without _ _ _ _ _ democracy cannot work .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">7</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">But without _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">a</span><span class="ltx_text" style="font-size:80%;"> democracy cannot work .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">8</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">But without _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">society</span><span class="ltx_text" style="font-size:80%;"> _ _ a democracy cannot work .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">9</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">But without _ society _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">,</span><span class="ltx_text" style="font-size:80%;"> a democracy cannot work .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">But without </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">civil</span><span class="ltx_text" style="font-size:80%;"> society _ , a democracy cannot work .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">11</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">But without civil society </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">organisations</span><span class="ltx_text" style="font-size:80%;"> , a democracy cannot work .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">(target)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">Yet without civil society organisations , a democracy cannot function .</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:113%;">Figure 5</span>: </span><span class="ltx_text" style="font-size:113%;">Example sentences generated following an <span class="ltx_text" style="color:#800080;"> outside-in</span> generation order, using the <span class="ltx_text ltx_font_bold">easy-first</span> decoding algorithm on De<math id="A4.F5.m2" class="ltx_Math" alttext="\rightarrow" display="inline"><mo stretchy="false">→</mo></math>En.</span></figcaption>
</figure>
<figure id="A4.F6" class="ltx_figure">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;">Iteration</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;color:#FF0000;">Left-to-Right</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">(source)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">Denken Sie , dass die Medien zu viel vom PSG erwarten ?</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold" style="font-size:80%;">Do</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ _ _ _ _ ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Do </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">you</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ _ _ _ ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Do you </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">think</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ _ _ ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">5</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Do you think _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">PS</span><span class="ltx_text" style="font-size:80%;"> _ ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">6</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Do you think _ _ _ _ _ _ PS </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">@G</span><span class="ltx_text" style="font-size:80%;"> ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">7</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Do you think _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">media</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ PS @G ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">8</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Do you think </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">the</span><span class="ltx_text" style="font-size:80%;"> media _ _ _ _ PS @G ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">9</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Do you think the media </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">expect</span><span class="ltx_text" style="font-size:80%;"> _ _ _ PS @G ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Do you think the media expect _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">much</span><span class="ltx_text" style="font-size:80%;"> _ PS @G ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">11</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Do you think the media expect </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">too</span><span class="ltx_text" style="font-size:80%;"> much _ PS @G ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">12</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Do you think the media expect too much </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">of</span><span class="ltx_text" style="font-size:80%;"> PS @G ?</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">(target)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">Do you think the media expect too much of PS @G ?</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:113%;">Figure 6</span>: </span><span class="ltx_text" style="font-size:113%;">Example sentences generated following an <span class="ltx_text" style="color:#FF0000;"> left-to-right</span> generation order, using the <span class="ltx_text ltx_font_bold">easy-first</span> decoding algorithm on De<math id="A4.F6.m2" class="ltx_Math" alttext="\rightarrow" display="inline"><mo stretchy="false">→</mo></math>En.</span></figcaption>
</figure>
<figure id="A4.F7" class="ltx_figure">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;">Iteration</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;color:#0000FF;">Right-to-Left</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">(source)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">Ein weiterer Streitpunkt : die Befugnisse der Armee .</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">.</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">army</span><span class="ltx_text" style="font-size:80%;"> .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">of</span><span class="ltx_text" style="font-size:80%;"> _ army .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ of </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">the</span><span class="ltx_text" style="font-size:80%;"> army .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">5</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">powers</span><span class="ltx_text" style="font-size:80%;"> of the army .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">6</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">the</span><span class="ltx_text" style="font-size:80%;"> powers of the army .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">7</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">:</span><span class="ltx_text" style="font-size:80%;"> the powers of the army .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">8</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">point</span><span class="ltx_text" style="font-size:80%;"> : the powers of the army .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">9</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">contentious</span><span class="ltx_text" style="font-size:80%;"> point : the powers of the army .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold" style="font-size:80%;">Another</span><span class="ltx_text" style="font-size:80%;"> contentious point : the powers of the army .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">(target)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">Another issue : the powers conferred on the army .</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:113%;">Figure 7</span>: </span><span class="ltx_text" style="font-size:113%;">Example sentences generated following an <span class="ltx_text" style="color:#0000FF;"> right-to-left</span> generation order, using the <span class="ltx_text ltx_font_bold">easy-first</span> decoding algorithm on De<math id="A4.F7.m2" class="ltx_Math" alttext="\rightarrow" display="inline"><mo stretchy="false">→</mo></math>En.</span></figcaption>
</figure>
<figure id="A4.F8" class="ltx_figure">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;">Iteration</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;color:#0000FF;">Right-to-Left-to-Right</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">(source)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">Die Aktien von Flight Centre stiegen gestern um 3 Cent auf 38,20 Dollar .</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">.</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">20</span><span class="ltx_text" style="font-size:80%;"> .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">38.</span><span class="ltx_text" style="font-size:80%;"> 20 .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">$</span><span class="ltx_text" style="font-size:80%;"> 38. 20 .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">5</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">to</span><span class="ltx_text" style="font-size:80%;"> $ 38. 20 .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">6</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold" style="font-size:80%;">Flight</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ _ _ to $ 38. 20 .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">7</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Flight </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">Centre</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ _ to $ 38. 20 .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">8</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Flight Centre </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">’s</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ to $ 38. 20 .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">9</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Flight Centre ’s </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">shares</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ to $ 38. 20 .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Flight Centre ’s shares </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">rose</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ to $ 38. 20 .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">11</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Flight Centre ’s shares rose </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">by</span><span class="ltx_text" style="font-size:80%;"> _ _ _ to $ 38. 20 .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">12</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Flight Centre ’s shares rose by _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">yesterday</span><span class="ltx_text" style="font-size:80%;"> to $ 38. 20 .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">13</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Flight Centre ’s shares rose by </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">3</span><span class="ltx_text" style="font-size:80%;"> _ yesterday to $ 38. 20 .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">14</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Flight Centre ’s shares rose by 3 </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">cents</span><span class="ltx_text" style="font-size:80%;"> yesterday to $ 38. 20 .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">(target)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">Flight Centre shares were up 3c at $ 38.20 yesterday .</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:113%;">Figure 8</span>: </span><span class="ltx_text" style="font-size:113%;">Example sentences generated following an <span class="ltx_text" style="color:#0000FF;"> Right-to-Left-to-Right</span> generation order, using the <span class="ltx_text ltx_font_bold">learned</span> decoding algorithm on De<math id="A4.F8.m2" class="ltx_Math" alttext="\rightarrow" display="inline"><mo stretchy="false">→</mo></math>En.</span></figcaption>
</figure>
<figure id="A4.F9" class="ltx_figure">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;">Iteration</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;color:#FF0000;">Outside-In</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">(source)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">Terminal 3 wird vor allem von kleineren US-Fluggesellschaften bedient .</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">.</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold" style="font-size:80%;">Terminal</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ _ _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Terminal </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">3</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Terminal 3 _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">airlines</span><span class="ltx_text" style="font-size:80%;"> .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">5</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Terminal 3 _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">US</span><span class="ltx_text" style="font-size:80%;"> airlines .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">6</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Terminal 3 _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">smaller</span><span class="ltx_text" style="font-size:80%;"> US airlines .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">7</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Terminal 3 _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">by</span><span class="ltx_text" style="font-size:80%;"> smaller US airlines .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">8</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Terminal 3 </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">is</span><span class="ltx_text" style="font-size:80%;"> _ _ by smaller US airlines .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">9</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Terminal 3 is </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">mainly</span><span class="ltx_text" style="font-size:80%;"> _ by smaller US airlines .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">Terminal 3 is mainly </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">served</span><span class="ltx_text" style="font-size:80%;"> by smaller US airlines .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">(target)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">Terminal 3 serves mainly small US airlines .</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:113%;">Figure 9</span>: </span><span class="ltx_text" style="font-size:113%;">Example sentences generated following an <span class="ltx_text" style="color:#FF0000;"> Outside-In</span> generation order, using the <span class="ltx_text ltx_font_bold">learned</span> decoding algorithm on De<math id="A4.F9.m2" class="ltx_Math" alttext="\rightarrow" display="inline"><mo stretchy="false">→</mo></math>En.</span></figcaption>
</figure>
<figure id="A4.F10" class="ltx_figure">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;">Iteration</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:80%;color:#00FF00;">Left-to-Right</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">(source)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:80%;">Die Gewinner des Team- und Einzelwettkampfs erhalten Preise .</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">_ _ _ _ _ _ _ _ _ _ _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">.</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold" style="font-size:80%;">The</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ _ _ _ _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">The </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">winners</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ _ _ _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">The winners </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">of</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ _ _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">5</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">The winners of </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">the</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">6</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">The winners of the </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">team</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">7</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">The winners of the team </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">and</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">8</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">The winners of the team and </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">individual</span><span class="ltx_text" style="font-size:80%;"> _ _ _ _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">9</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">The winners of the team and individual </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">competitions</span><span class="ltx_text" style="font-size:80%;"> _ _ _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">The winners of the team and individual competitions </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">will</span><span class="ltx_text" style="font-size:80%;"> _ _ .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">11</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">The winners of the team and individual competitions will _ </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">prizes</span><span class="ltx_text" style="font-size:80%;"> .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:80%;">12</span></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text" style="font-size:80%;">The winners of the team and individual competitions will </span><span class="ltx_text ltx_font_bold" style="font-size:80%;">receive</span><span class="ltx_text" style="font-size:80%;"> prizes .</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">(target)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:80%;">The winners of the team and individual contests receive prizes .</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:113%;">Figure 10</span>: </span><span class="ltx_text" style="font-size:113%;">Example sentences generated following an <span class="ltx_text" style="color:#00FF00;"> left-to-right</span> generation order, using the <span class="ltx_text ltx_font_bold">learned</span> decoding algorithm on De<math id="A4.F10.m2" class="ltx_Math" alttext="\rightarrow" display="inline"><mo stretchy="false">→</mo></math>En.</span></figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Dec  5 00:19:52 2024 by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>
</body>
</html>
